[["index.html", "Introducción a Ciencia de Datos y Machine Learning con Python BIENVENIDA Objetivo Instructores Alcances del curso Duración y evaluación del curso Recursos y dinámica de clase Asesorías", " Introducción a Ciencia de Datos y Machine Learning con Python BIENVENIDA Objetivo Brindar al participante los elementos teóricos y prácticos básicos alrededor de la programación para el análisis de datos. Aprenderá a distinguir las diferentes soluciones a problemas que pueden resolverse con algoritmos de machine learning y aprenderá a usar el conjunto de librerías en Python más novedoso, estructuradas y ampliamente usadas para la manipulación, transformación y visualización de datos. Instructores ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario egresado de la Facultad de Ciencias con maestría en Ciencia de Datos por el ITAM. Se especializa en modelos predictivos y de clasificación de machine learning aplicado a seguros, banca, marketing, deportes, e-commerce y movilidad. Ha sido consultor Senior Data Scientist para empresas y organizaciones como GNP, El Universal, UNAM, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), Comisión Nacional de los Derechos Humanos (CNDH), Sinia, Geek-end, Invesmark, entre otros. Ha contribuido en más de 30 proyectos de impacto nacional con diferentes institutos de investigación de la UNAM como el Instituto de Investigaciones Sociales, Instituto de Geografía, Instituto de Investigaciones Jurídicas, Programa Universitario de Estudios sobre la Ciudad, Fundación UNAM y Rectoría. Actualmente es Data Scientist Expert en la fábrica de inteligencia artifical en BBVA (AI Factory), es profesor de Ciencia de datos y Machine Learning en AMAT, y consultor estadístico de encuestas nacionales de investigación social realizadas por la UNAM. Adicionalmente, participa en el Laboratorio Nacional de Observación de la Tierra (LANOT) en la detección en tiempo real de contaminación del mar por sargazo a través de algoritmos de IA y percepción remota aplicados a los datos proveidos por el satélite Landsat9. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboap@gmail.com Actuaria egresada de la Facultad de Ciencias y candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Manager y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Es experta en entendimiento de negocio para la correcta implementación de algoritmos de análisis de datos, desde la ingenieria y arquitectura de datos hasta BI y modelos de Machine Learning. Actualmente se desarrolla como Data Analytics Manager para México, Chile y Colombia en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Adicionalmente participa en proyectos como Senior Data Scientist en CLOSTER, como profesora del diplomado de Metodología de la Investigación Social por la UNAM y como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Actinver Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. Alcances del curso Al finalizar este curso, el participante será capaz de consumir, manipular y visualizar información para resolver problemas de propósito general asociados a los datos. Apenderá a implementar diferentes algoritmos de machine learning y mejorar su desempeño predictivo en problemas de clasificación, regresión y segmentación. Requisitos: Computadora con al menos 8Gb Ram Instalar Python con versión 3.8 o superior Instalar un IDE preferido. Jupyter, RStudio, Spyder, VSCode Temario: 1. Introducción a Ciencia de Datos Machine Learning, Bigdata, BI, AI y CD Objetivo de ciencia de datos Requisitos y aplicaciones Tipos de algoritmos Ciclo de vida de un proyecto 2. Manipulación de datos Importación de tablas Manipulación de tablas Transformación de estructuras 3. Concepto de Machine Learning Machine learning Análisis supervisado Sesgo y varianza Partición de datos Preprocesamiento e ingeniería de datos 4. Algoritmos de Machine Learning Regresión Lineal Métricas de error Regresión logística Métricas de error KNN Árbol de decisión Random Forest Comparación de modelos Duración y evaluación del curso El programa tiene una duración de 40 hrs. Las clases serán impartidas los días sábado, de 9:00 am a 1:00 pm Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. Recursos y dinámica de clase En esta clase estaremos usando: Python da click aquí si aún no lo descargas RStudio da click aquí también VSCode da click aquí si quieres descargar Anaconda da click aquí si quieres descargar R da click para descargar Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar ó Ya estoy listo para iniciar One Drive Notas de clase Revisame si quieres aprender Asesorías Los profesores se encuentran en la mejor disposición de asistir las dudas de clase de todos los alumnos. El grupo de whatsapp ha sido creado para compartir información relevante al curso y exponer dudas y soluciones que puedan ser de interés de todo el grupo. Los alumnos podrán hacer uso del canal de comunicación para externar sus dudas de clase durante el tiempo que dure el curso. Los profesores se comprometen a responder en el transcurso del día las preguntas realizadas que sean relevantes con la clase. Las respuestas se realizarán de lunes a viernes en un horario de 10:00am a 8:00pm. ¡¡ AVISO !! No se atenderán dudas que tengan que ver con otros proyectos o asignaciones laborales de los estudiantes en sus respectivos ambientes de trabajo. Se invita a los estudiantes a que las dudas realizadas en clase sean relevantes a la clase y los ejemplos a resolver sean de interés para todo el alumnado. Nota: En caso de requerir consultoría especializada o particular a un tema de interés, se deberá contactar al área administrativa para solicitar la cotización correspondiente al servicio correspondiente. "],["conceptos-de-ciencia-de-datos.html", "Capítulo 1 Conceptos de Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? 1.2 Objetivos 1.3 Requisitos 1.4 Aplicaciones 1.5 Tipos de algoritmos 1.6 Ciclo de un proyecto", " Capítulo 1 Conceptos de Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? Definiendo conceptos: Estadística Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. La estadística descriptiva, como su nombre lo indica, se encarga de describir datos y obtener conclusiones. Se utilizan números (media, mediana, moda, mínimo, máximo, etc) para analizar datos y llegar a conclusiones de acuerdo a ellos. La estadística inferencial argumenta o infiere sus resultados a partir de las muestras de una población. Se intenta conseguir información al utilizar un procedimiento ordenado en el manejo de los datos de la muestra. La estadística predictiva busca estimar valores y escenarios futuros más probables de ocurrir a partir de referencias históricas previas. Se suelen ocupar como apoyo características y factores áltamente asociados al fenómeno que se desea predecir. Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPI’s y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. BI esta enfocado en analizar la historia pasada para tomar decisiones hacia el futuro. ¿Qué características tiene un KPI? Específicos Continuos y periódicos Objetivos Cuantificables Medibles Realistas Concisos Coherentes Relevantes Machine Learning: Machine learning –aprendizaje de máquina– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imágenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es común que se confunda los conceptos de Big Data y Big Compute, como se mencionó, Big Data se refiere al procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada para resolver problemas que usan algoritmos muy complejos. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 1.2 Objetivos Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway Más sobre Conway: Forbes 2016 1.3 Requisitos Background científico: Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría analítica, programación, conocimientos computacionales… etc Datos relevantes y suficientes: Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Etiquetas: Se necesita la intervención humana para etiquetar, clasificar e introducir los datos en el algoritmo. Software: Existen distintos lenguajes de programación para realizar ciencia de datos 1.4 Aplicaciones Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: Podemos pensar en una infinidad de aplicaciones comerciales basadas en el análisis de datos. Con la intención de estructurar las posibles aplicaciones, se ofrece a continuación una categorización que, aunque no es suficiente para englobar todos los posibles casos de uso, sí es sorprendente la cantidad de aplicaciones que abarca. 1. Aplicaciones centradas en los clientes Incrementar beneficio al mejorar recomendaciones de productos Up-selling Cross-selling Reducir tasas de cancelación y mejorar tasas de retención Personalizar experiencia de usuario Mejorar el marketing dirigido Análisis de sentimientos Personalización de productos o servicios 2. Optimización de problemas Optimización de precios Ubicación de nuevas sucursales Maximización de ganancias mediante producción de materias primas Construcción de portafolios de inversión 3. Predicción de demanda Número futuro de clientes Número esperado de viajes en avión / camión / bicis Número de contagios por un virus (demanda médica / medicamentos / etc) Predicción de uso de recursos (luz / agua / gas) 4. Análisis de detección de fraudes Detección de robo de identidad Detección de transacciones ilícitas Detección de servicios fraudulentos Detección de zonas geográficas con actividades ilícitas 1.5 Tipos de algoritmos Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 1.5.1 Aprendizaje supervisado En el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. Estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien”. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 1.5.2 Aprendizaje no supervisado En el aprendizaje no supervisado, carecemos de etiquetas. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir ¿qué es qué? por nosotros mismos. Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad 1.5.3 Aprendizaje por refuerzo Su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Optimización de campañas de marketing - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN Ejemplo: Mario Bros 1.6 Ciclo de un proyecto Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes “experimentos”. Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. "],["introducción-a-python.html", "Capítulo 2 Introducción a Python 2.1 ¿Cómo obtener Python? 2.2 ¿Qué es RStudio? 2.3 Uso de python en Rstudio 2.4 Lectura de datos 2.5 Consultas de datos 2.6 Orden y estructura", " Capítulo 2 Introducción a Python Python es un lenguaje de programación de alto nivel, interpretado y generalmente considerado como un lenguaje fácil de aprender y utilizar. Fue creado por Guido van Rossum y lanzado por primera vez en 1991. Python se destaca por su sintaxis clara y legible, lo que facilita la escritura y comprensión del código. Python se ha vuelto extremadamente popular en el campo de la ciencia de datos por varias razones: Facilidad de uso: Python se destaca por su sintaxis simple y legible, lo que facilita a los científicos de datos escribir, leer y mantener el código. Además, cuenta con una gran cantidad de bibliotecas y paquetes que facilitan tareas comunes en el análisis de datos. Amplia comunidad y ecosistema: Cuenta con una gran comunidad de desarrolladores y científicos de datos que contribuyen con bibliotecas y paquetes de código abierto para realizar diversas tareas en ciencia de datos. Algunas de las bibliotecas más populares incluyen NumPy (para cálculos numéricos), pandas (para procesamiento de dataframes), matplotlib (para gráficos), scikit-learn (para machine learning) y TensorFlow (para deep learning). Integración con otras tecnologías: Python se puede integrar fácilmente con otras tecnologías y lenguajes, lo que lo convierte en una opción versátil para la ciencia de datos. Por ejemplo, es común utilizar Python junto con bases de datos, herramientas de big data como Apache Spark, y lenguajes como R y SQL. Flexibilidad: Python es un lenguaje flexible que se adapta a diferentes necesidades en ciencia de datos. Puede utilizarse tanto para realizar tareas simples como el procesamiento y limpieza de datos, como para desarrollar modelos de aprendizaje automático complejos y aplicaciones de inteligencia artificial. Accesibilidad: Python es un lenguaje de programación open-source, lo que significa que es de uso gratuito y todos pueden contribuir a su desarrollo. Esto permite que toda la comunidad tiene acceso a los desarrollos y contribuciones de código publicado. En el ámbito empresarial y académico disminuye los costos de software. Estas características hacen de Python una herramienta poderosa y popular para realizar análisis de datos, desarrollar modelos de aprendizaje automático y abordar una amplia gama de problemas en la ciencia de datos. 2.1 ¿Cómo obtener Python? Python puede ser fácilmente descargado de forma gratuita desde el sitio oficial https://www.python.org/downloads/. Python está disponible para las plataformas Windows, Mac y Linux. 2.2 ¿Qué es RStudio? RStudio es un Entorno de Desarrollo Integrado (IDE, por sus siglas en inglés) creado por Posit (antes Rstudio) para R y Python. Este permite y facilita el desarrollo y ejecución de sintaxis para código en R y python, incluye una consola y proporciona herramientas para la gestión del espacio de trabajo. RStudio está disponible para Windows, Mac y Linux o para navegadores conectados a RStudio Server o RStudio Server Pro. Algunas de las principales características de Rstudio que lo hacen una gran herramienta para trabajar, son: Auto completado de código Sangría inteligente Resaltado de sintaxis Facilidad para definir funciones Soporte integrado Documentación integrada Administración de directorios y proyectos Visor de datos Depurador interactivo para corregir errores Conección con Rmarkwon y Sweave La siguiente imagen muestra la forma en la que está estructurado RStudio. El orden de los páneles puede ser elegido por el usuario, así como las características de tipo de letra, tamaño y color de fondo, entre otras características. Figure 2.1: Páneles de trabajo de Rstudio 2.3 Uso de python en Rstudio library(reticulate) install_python(list = TRUE) use_python(&quot;C:/Program Files/Python310/python.exe&quot;) #### Conda #### # Creación de ambiente virtual de python 3.10 con conda conda_create(envname = &quot;conda_dsml_py3_10&quot;) # Consultar la lista de ambientes creados conda_list() use_miniconda(&quot;conda_dsml_py3_10&quot;) # instalar pandas conda_install(&quot;conda_dsml_py3_10&quot;, &quot;pandas&quot;) pandas &lt;- import(&quot;pandas&quot;) #### VirtualEnv #### # Creación de ambiente virtual de python 3.10 con virtualenv virtualenv_create(envname = &quot;venv_dsml_py3_10&quot;) virtualenv_list() use_virtualenv(&quot;venv_dsml_py3_10&quot;) # install pandas virtualenv_install(&quot;venv_dsml_py3_10&quot;, &quot;pandas&quot;) 2.4 Lectura de datos El primer paso para analizar datos es incorporarlos a la sesión de python para que puedan ser manipulados y observados. Existen múltiples librerías y funciones que permiten leer la información proveniente de un archivo externo, el cual puede tener una de muchas posibles extensiones. Usualmente, no creamos los datos desde la sesión, sino que a través de un archivo externo se realiza la lectura de datos escritos en un archivo. Los más comúnes son: La paquetería pandas fue desarrollada recientemente para lidiar con la lectura de archivos rápidamente. Esta paquetería proporciona funciones que suelen ser mucho más rápidas que las funciones base que proporciona python. 2.4.1 Archivos csv A la hora de importar conjuntos de datos en python, uno de los formatos más habituales en los que hallamos información es en archivos separados por comas (comma separated values), cuya extensión suele ser .csv. En ellos encontramos múltiples líneas que recogen la tabla de interés, y en las cuales los valores aparecen, de manera consecutiva, separados por el carácter ,. Para importar este tipo de archivos en nuestra sesión de python, se utiliza la función read_csv(). El único argumento que debemos de pasar a esta función de manera obligatoria, es file, el nombre o la ruta completa del archivo que pretendemos importar. La paquetería pandas fue desarrollada para lidiar con la lectura de archivos grandes rápidamente. Veamos un ejemplo: import pandas as pd data_csv = pd.read_csv(&quot;data/ames.csv&quot;) data_csv.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2930 entries, 0 to 2929 ## Data columns (total 74 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 MS_SubClass 2930 non-null object ## 1 MS_Zoning 2930 non-null object ## 2 Lot_Frontage 2930 non-null int64 ## 3 Lot_Area 2930 non-null int64 ## 4 Street 2930 non-null object ## 5 Alley 2930 non-null object ## 6 Lot_Shape 2930 non-null object ## 7 Land_Contour 2930 non-null object ## 8 Utilities 2930 non-null object ## 9 Lot_Config 2930 non-null object ## 10 Land_Slope 2930 non-null object ## 11 Neighborhood 2930 non-null object ## 12 Condition_1 2930 non-null object ## 13 Condition_2 2930 non-null object ## 14 Bldg_Type 2930 non-null object ## 15 House_Style 2930 non-null object ## 16 Overall_Cond 2930 non-null object ## 17 Year_Built 2930 non-null int64 ## 18 Year_Remod_Add 2930 non-null int64 ## 19 Roof_Style 2930 non-null object ## 20 Roof_Matl 2930 non-null object ## 21 Exterior_1st 2930 non-null object ## 22 Exterior_2nd 2930 non-null object ## 23 Mas_Vnr_Type 1155 non-null object ## 24 Mas_Vnr_Area 2930 non-null int64 ## 25 Exter_Cond 2930 non-null object ## 26 Foundation 2930 non-null object ## 27 Bsmt_Cond 2930 non-null object ## 28 Bsmt_Exposure 2930 non-null object ## 29 BsmtFin_Type_1 2930 non-null object ## 30 BsmtFin_SF_1 2930 non-null int64 ## 31 BsmtFin_Type_2 2930 non-null object ## 32 BsmtFin_SF_2 2930 non-null int64 ## 33 Bsmt_Unf_SF 2930 non-null int64 ## 34 Total_Bsmt_SF 2930 non-null int64 ## 35 Heating 2930 non-null object ## 36 Heating_QC 2930 non-null object ## 37 Central_Air 2930 non-null object ## 38 Electrical 2930 non-null object ## 39 First_Flr_SF 2930 non-null int64 ## 40 Second_Flr_SF 2930 non-null int64 ## 41 Gr_Liv_Area 2930 non-null int64 ## 42 Bsmt_Full_Bath 2930 non-null int64 ## 43 Bsmt_Half_Bath 2930 non-null int64 ## 44 Full_Bath 2930 non-null int64 ## 45 Half_Bath 2930 non-null int64 ## 46 Bedroom_AbvGr 2930 non-null int64 ## 47 Kitchen_AbvGr 2930 non-null int64 ## 48 TotRms_AbvGrd 2930 non-null int64 ## 49 Functional 2930 non-null object ## 50 Fireplaces 2930 non-null int64 ## 51 Garage_Type 2930 non-null object ## 52 Garage_Finish 2930 non-null object ## 53 Garage_Cars 2930 non-null int64 ## 54 Garage_Area 2930 non-null int64 ## 55 Garage_Cond 2930 non-null object ## 56 Paved_Drive 2930 non-null object ## 57 Wood_Deck_SF 2930 non-null int64 ## 58 Open_Porch_SF 2930 non-null int64 ## 59 Enclosed_Porch 2930 non-null int64 ## 60 Three_season_porch 2930 non-null int64 ## 61 Screen_Porch 2930 non-null int64 ## 62 Pool_Area 2930 non-null int64 ## 63 Pool_QC 2930 non-null object ## 64 Fence 2930 non-null object ## 65 Misc_Feature 106 non-null object ## 66 Misc_Val 2930 non-null int64 ## 67 Mo_Sold 2930 non-null int64 ## 68 Year_Sold 2930 non-null int64 ## 69 Sale_Type 2930 non-null object ## 70 Sale_Condition 2930 non-null object ## 71 Sale_Price 2930 non-null int64 ## 72 Longitude 2930 non-null float64 ## 73 Latitude 2930 non-null float64 ## dtypes: float64(2), int64(32), object(40) ## memory usage: 1.7+ MB pd.set_option(&#39;display.max_columns&#39;, 6) data_csv.head(5) ## MS_SubClass MS_Zoning \\ ## 0 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 1 One_Story_1946_and_Newer_All_Styles Residential_High_Density ## 2 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 3 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 4 Two_Story_1946_and_Newer Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 0 141 ... 215000 -93.619754 42.054035 ## 1 80 ... 105000 -93.619756 42.053014 ## 2 81 ... 172000 -93.619387 42.052659 ## 3 93 ... 244000 -93.617320 42.051245 ## 4 74 ... 189900 -93.638933 42.060899 ## ## [5 rows x 74 columns] data_csv.describe() ## Lot_Frontage Lot_Area Year_Built ... Sale_Price \\ ## count 2930.000000 2930.000000 2930.000000 ... 2930.000000 ## mean 57.647782 10147.921843 1971.356314 ... 180796.060068 ## std 33.499441 7880.017759 30.245361 ... 79886.692357 ## min 0.000000 1300.000000 1872.000000 ... 12789.000000 ## 25% 43.000000 7440.250000 1954.000000 ... 129500.000000 ## 50% 63.000000 9436.500000 1973.000000 ... 160000.000000 ## 75% 78.000000 11555.250000 2001.000000 ... 213500.000000 ## max 313.000000 215245.000000 2010.000000 ... 755000.000000 ## ## Longitude Latitude ## count 2930.000000 2930.000000 ## mean -93.642897 42.034482 ## std 0.025700 0.018410 ## min -93.693153 41.986498 ## 25% -93.660217 42.022088 ## 50% -93.641806 42.034662 ## 75% -93.622113 42.049853 ## max -93.577427 42.063388 ## ## [8 rows x 34 columns] La base de datos llamada AmesHousing contiene un conjunto de datos con información de la Oficina del Tasador de Ames utilizada para calcular los valores tasados para las propiedades residenciales individuales vendidas en Ames, Iowa, de 2006 a 2010. FUENTES: Ames, Oficina del Tasador de Iowa. Pueden descargar los datos para la clase aquí ¿Y si el archivo que necesitamos leer está en excel? 2.4.2 Archivos txt Uno de los archivos más comunes es el .txt. La librería pandas también cuenta con parámetros en la función read_csv que permiten leer fácilmente los datos contenidos en formato tabular. ames_txt = pd.read_csv(&quot;data/ames.txt&quot;, delimiter = &quot;;&quot;) ames_txt.head(3) ## MS_SubClass MS_Zoning \\ ## 0 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 1 One_Story_1946_and_Newer_All_Styles Residential_High_Density ## 2 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 0 141 ... 215000 -93.619754 42.054035 ## 1 80 ... 105000 -93.619756 42.053014 ## 2 81 ... 172000 -93.619387 42.052659 ## ## [3 rows x 74 columns] La función read_csv() funciona para leer archivos con diferentes delimitadores posibles, es decir, es posible especificar si las columnas están separadas por espacios, comas, punto y coma, tabulador o algún otro delimitador (““,”,“,”;“,”, “@”). Adicionalmente, se puede especificar si el archivo contiene encabezado, si existen renglones a saltar, codificación, tipo de variable y muchas más opciones. Todos estos detalles pueden consultarse en la documentación oficial. 2.4.3 Archivos xls y xlsx La paquetería pandas facilita la obtención de datos tabulares de archivos de Excel. Admite tanto el formato .xls heredado como el formato .xlsx moderno basado en XML. Es importante mencionar que es necesario instalar una dependencia para que funcione. Se requiere de la librería openpyxl ames_xlsx = pd.read_excel(&quot;data/ames.xlsx&quot;) ames_xlsx.head(3) ## MS_SubClass MS_Zoning \\ ## 0 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 1 One_Story_1946_and_Newer_All_Styles Residential_High_Density ## 2 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 0 141 ... 215000 -93.619754 42.054035 ## 1 80 ... 105000 -93.619756 42.053014 ## 2 81 ... 172000 -93.619387 42.052659 ## ## [3 rows x 74 columns] 2.4.4 Archivos pickle Un tipo de archivo que resulta de particular interés, es el .pkl. Este archivo comprime cualquier objeto o resultado que sea usado o producido en python. Uno puede almacenar el objeto de interés de la siguiente manera: import pickle with open(&#39;data/ames.pkl&#39;, &#39;wb&#39;) as f: pickle.dump(ames_xlsx, f, pickle.HIGHEST_PROTOCOL) Puede observarse que en el explorador de archivos se encuentra ahora el nuevo archivo con extensión .pkl, el cual puede ser posteriormente incorporado a una sesión de python para seguir trabajando con él. with open(&#39;data/ames.pkl&#39;, &#39;rb&#39;) as f: ames_pkl = pickle.load(f) # alternativa # ames_pkl = pd.read_pickle(&#39;data/ames.pkl&#39;) ames_pkl.head(3) ## MS_SubClass MS_Zoning \\ ## 0 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 1 One_Story_1946_and_Newer_All_Styles Residential_High_Density ## 2 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 0 141 ... 215000 -93.619754 42.054035 ## 1 80 ... 105000 -93.619756 42.053014 ## 2 81 ... 172000 -93.619387 42.052659 ## ## [3 rows x 74 columns] Algunas de las grandes ventajas que tiene almacenar los archivos en formato pickle, son las siguientes: No es necesario volver a ejecutar procesos largos cuando ya se ha logrado realizar una vez. El tiempo de lectura de la información es considerablemente más rápido. 2.5 Consultas de datos Python hoy en día es el lenguaje de programación más popular, sin embargo otros lenguajes aún mantienen ventajas al haber desarrollado librerías muy especializadas y limpias para trabajar. Esto no significa que todo esté perdido para python, pues algunos desarrolladores están tratando de emular las funciones que existen en otros lenguajes y que han sido ampliamente adoptados por la comunidad. En R, existe un conjunto de librerías llamado TIDYVERSE que sirve extraordinariamente para transformar y manipular datos. Aunque en python también se puede hacer con pandas, encontramos muy atractivo aprender funciones que se usan de igual manera en varios lenguajes de programación. La librería en python que simula a dplyr de tidyverse se conoce como siuba El paquete siuba proporciona un conjunto de funciones muy útiles para manipular dataframes y así reducir el número de repeticiones, la probabilidad de cometer errores y el número de caracteres que hay que escribir. Como valor extra, podemos encontrar que la gramática de siuba es más fácil de entender. Revisaremos algunas de sus funciones más usadas (verbos), así como el uso de pipes (&gt;&gt;) para combinarlas. select() filter() arrange() mutate() summarise() join() group_by() Primero tenemos que instalar y cargar la paquetería: #R virtualenv_install(&quot;venv_dsml_py3_10&quot;, &quot;siuba&quot;) # python from siuba import * Usaremos el dataset AmesHousing que se proporcionó en el capítulo anterior (el alumno puede hacer el ejercicio con datos propios, si así lo desea) ames_housing = pd.read_csv(&quot;data/ames.csv&quot;) ames_housing.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2930 entries, 0 to 2929 ## Data columns (total 74 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 MS_SubClass 2930 non-null object ## 1 MS_Zoning 2930 non-null object ## 2 Lot_Frontage 2930 non-null int64 ## 3 Lot_Area 2930 non-null int64 ## 4 Street 2930 non-null object ## 5 Alley 2930 non-null object ## 6 Lot_Shape 2930 non-null object ## 7 Land_Contour 2930 non-null object ## 8 Utilities 2930 non-null object ## 9 Lot_Config 2930 non-null object ## 10 Land_Slope 2930 non-null object ## 11 Neighborhood 2930 non-null object ## 12 Condition_1 2930 non-null object ## 13 Condition_2 2930 non-null object ## 14 Bldg_Type 2930 non-null object ## 15 House_Style 2930 non-null object ## 16 Overall_Cond 2930 non-null object ## 17 Year_Built 2930 non-null int64 ## 18 Year_Remod_Add 2930 non-null int64 ## 19 Roof_Style 2930 non-null object ## 20 Roof_Matl 2930 non-null object ## 21 Exterior_1st 2930 non-null object ## 22 Exterior_2nd 2930 non-null object ## 23 Mas_Vnr_Type 1155 non-null object ## 24 Mas_Vnr_Area 2930 non-null int64 ## 25 Exter_Cond 2930 non-null object ## 26 Foundation 2930 non-null object ## 27 Bsmt_Cond 2930 non-null object ## 28 Bsmt_Exposure 2930 non-null object ## 29 BsmtFin_Type_1 2930 non-null object ## 30 BsmtFin_SF_1 2930 non-null int64 ## 31 BsmtFin_Type_2 2930 non-null object ## 32 BsmtFin_SF_2 2930 non-null int64 ## 33 Bsmt_Unf_SF 2930 non-null int64 ## 34 Total_Bsmt_SF 2930 non-null int64 ## 35 Heating 2930 non-null object ## 36 Heating_QC 2930 non-null object ## 37 Central_Air 2930 non-null object ## 38 Electrical 2930 non-null object ## 39 First_Flr_SF 2930 non-null int64 ## 40 Second_Flr_SF 2930 non-null int64 ## 41 Gr_Liv_Area 2930 non-null int64 ## 42 Bsmt_Full_Bath 2930 non-null int64 ## 43 Bsmt_Half_Bath 2930 non-null int64 ## 44 Full_Bath 2930 non-null int64 ## 45 Half_Bath 2930 non-null int64 ## 46 Bedroom_AbvGr 2930 non-null int64 ## 47 Kitchen_AbvGr 2930 non-null int64 ## 48 TotRms_AbvGrd 2930 non-null int64 ## 49 Functional 2930 non-null object ## 50 Fireplaces 2930 non-null int64 ## 51 Garage_Type 2930 non-null object ## 52 Garage_Finish 2930 non-null object ## 53 Garage_Cars 2930 non-null int64 ## 54 Garage_Area 2930 non-null int64 ## 55 Garage_Cond 2930 non-null object ## 56 Paved_Drive 2930 non-null object ## 57 Wood_Deck_SF 2930 non-null int64 ## 58 Open_Porch_SF 2930 non-null int64 ## 59 Enclosed_Porch 2930 non-null int64 ## 60 Three_season_porch 2930 non-null int64 ## 61 Screen_Porch 2930 non-null int64 ## 62 Pool_Area 2930 non-null int64 ## 63 Pool_QC 2930 non-null object ## 64 Fence 2930 non-null object ## 65 Misc_Feature 106 non-null object ## 66 Misc_Val 2930 non-null int64 ## 67 Mo_Sold 2930 non-null int64 ## 68 Year_Sold 2930 non-null int64 ## 69 Sale_Type 2930 non-null object ## 70 Sale_Condition 2930 non-null object ## 71 Sale_Price 2930 non-null int64 ## 72 Longitude 2930 non-null float64 ## 73 Latitude 2930 non-null float64 ## dtypes: float64(2), int64(32), object(40) ## memory usage: 1.7+ MB ames_housing.describe() ## Lot_Frontage Lot_Area Year_Built ... Sale_Price \\ ## count 2930.000000 2930.000000 2930.000000 ... 2930.000000 ## mean 57.647782 10147.921843 1971.356314 ... 180796.060068 ## std 33.499441 7880.017759 30.245361 ... 79886.692357 ## min 0.000000 1300.000000 1872.000000 ... 12789.000000 ## 25% 43.000000 7440.250000 1954.000000 ... 129500.000000 ## 50% 63.000000 9436.500000 1973.000000 ... 160000.000000 ## 75% 78.000000 11555.250000 2001.000000 ... 213500.000000 ## max 313.000000 215245.000000 2010.000000 ... 755000.000000 ## ## Longitude Latitude ## count 2930.000000 2930.000000 ## mean -93.642897 42.034482 ## std 0.025700 0.018410 ## min -93.693153 41.986498 ## 25% -93.660217 42.022088 ## 50% -93.641806 42.034662 ## 75% -93.622113 42.049853 ## max -93.577427 42.063388 ## ## [8 rows x 34 columns] 2.5.1 Seleccionar columnas Observamos que nuestros datos tienen 2,930 observaciones y 74 variables, con select() podemos seleccionar las variables que se indiquen. ( ames_housing &gt;&gt; select(_.Lot_Area, _.Neighborhood, _.Year_Sold, _.Sale_Price) ) ## Lot_Area Neighborhood Year_Sold Sale_Price ## 0 31770 North_Ames 2010 215000 ## 1 11622 North_Ames 2010 105000 ## 2 14267 North_Ames 2010 172000 ## 3 11160 North_Ames 2010 244000 ## 4 13830 Gilbert 2010 189900 ## ... ... ... ... ... ## 2925 7937 Mitchell 2006 142500 ## 2926 8885 Mitchell 2006 131000 ## 2927 10441 Mitchell 2006 132000 ## 2928 10010 Mitchell 2006 170000 ## 2929 9627 Mitchell 2006 188000 ## ## [2930 rows x 4 columns] ¡¡ RECORDAR !! El operador pipe (&gt;&gt;) se usa para conectar un elemento con una función o acción a realizar. En este caso solo se indica que en los datos de ames se seleccionan 4 variables. Al operador ’_.’ se le conoce como siu-expresion. Es muy útil para hacer referencia a una columna que se encuentra dentro del conjunto de datos que se encuentra operando. Para que el operador &gt;&gt; ejecute todo el pipeline es importante envolverlo entre paréntesis. Con select() y contains() podemos seleccionar variables con alguna cadena de texto. ( ames_housing &gt;&gt; select(_.contains(&quot;Area&quot;)) ) ## Lot_Area Mas_Vnr_Area Gr_Liv_Area Garage_Area Pool_Area ## 0 31770 112 1656 528 0 ## 1 11622 0 896 730 0 ## 2 14267 108 1329 312 0 ## 3 11160 0 2110 522 0 ## 4 13830 0 1629 482 0 ## ... ... ... ... ... ... ## 2925 7937 0 1003 588 0 ## 2926 8885 0 902 484 0 ## 2927 10441 0 970 0 0 ## 2928 10010 0 1389 418 0 ## 2929 9627 94 2000 650 0 ## ## [2930 rows x 5 columns] De igual manera, con select(), contains() podemos seleccionar que inicien o terminen con alguna cadena de texto. ( ames_housing &gt;&gt; select(_.contains(&quot;^Garage&quot;)) ) ## Garage_Type Garage_Finish Garage_Cars Garage_Area Garage_Cond ## 0 Attchd Fin 2 528 Typical ## 1 Attchd Unf 1 730 Typical ## 2 Attchd Unf 1 312 Typical ## 3 Attchd Fin 2 522 Typical ## 4 Attchd Fin 2 482 Typical ## ... ... ... ... ... ... ## 2925 Detchd Unf 2 588 Typical ## 2926 Attchd Unf 2 484 Typical ## 2927 No_Garage No_Garage 0 0 No_Garage ## 2928 Attchd RFn 2 418 Typical ## 2929 Attchd Fin 3 650 Typical ## ## [2930 rows x 5 columns] ( ames_housing &gt;&gt; select(_.contains(&quot;Area$&quot;)) ) ## Lot_Area Mas_Vnr_Area Gr_Liv_Area Garage_Area Pool_Area ## 0 31770 112 1656 528 0 ## 1 11622 0 896 730 0 ## 2 14267 108 1329 312 0 ## 3 11160 0 2110 522 0 ## 4 13830 0 1629 482 0 ## ... ... ... ... ... ... ## 2925 7937 0 1003 588 0 ## 2926 8885 0 902 484 0 ## 2927 10441 0 970 0 0 ## 2928 10010 0 1389 418 0 ## 2929 9627 94 2000 650 0 ## ## [2930 rows x 5 columns] 2.5.2 Filtrar observaciones La función filter() nos permite filtrar filas según una condición, primero notemos que la variable Sale_Condition tiene distintas categorías. ames_housing[&#39;Sale_Condition&#39;].value_counts() ## Sale_Condition ## Normal 2413 ## Partial 245 ## Abnorml 190 ## Family 46 ## Alloca 24 ## AdjLand 12 ## Name: count, dtype: int64 ¡¡ SPOILER !! En un modelo predictivo de Machine Learning, no es correcto agregar columnas cuyo valor es conocido hasta el momento de la observación. Es decir, no deben agregarse variables que no se conozca su valor al momento de la predicción, como es el caso de condición de venta. Ahora usaremos la función filter para quedarnos solo con las observaciones con condición de venta “normal”. ( ames_housing &gt;&gt; filter(_.Sale_Condition == &quot;Normal&quot;) ) ## MS_SubClass MS_Zoning \\ ## 0 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 1 One_Story_1946_and_Newer_All_Styles Residential_High_Density ## 2 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 3 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 4 Two_Story_1946_and_Newer Residential_Low_Density ## ... ... ... ## 2925 Split_or_Multilevel Residential_Low_Density ## 2926 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2927 Split_Foyer Residential_Low_Density ## 2928 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2929 Two_Story_1946_and_Newer Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 0 141 ... 215000 -93.619754 42.054035 ## 1 80 ... 105000 -93.619756 42.053014 ## 2 81 ... 172000 -93.619387 42.052659 ## 3 93 ... 244000 -93.617320 42.051245 ## 4 74 ... 189900 -93.638933 42.060899 ## ... ... ... ... ... ... ## 2925 37 ... 142500 -93.604776 41.988964 ## 2926 0 ... 131000 -93.602680 41.988314 ## 2927 62 ... 132000 -93.606847 41.986510 ## 2928 77 ... 170000 -93.600190 41.990921 ## 2929 74 ... 188000 -93.599996 41.989265 ## ## [2413 rows x 74 columns] También se puede usar para filtrar variables numéricas: ( ames_housing &gt;&gt; filter( (_.Lot_Area &gt; 1000) &amp; (_.Sale_Price &gt;= 150000) ) ) ## MS_SubClass MS_Zoning \\ ## 0 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 3 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 4 Two_Story_1946_and_Newer Residential_Low_Density ## 5 Two_Story_1946_and_Newer Residential_Low_Density ## ... ... ... ## 2921 Duplex_All_Styles_and_Ages Residential_Low_Density ## 2922 Duplex_All_Styles_and_Ages Residential_Low_Density ## 2923 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2928 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2929 Two_Story_1946_and_Newer Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 0 141 ... 215000 -93.619754 42.054035 ## 2 81 ... 172000 -93.619387 42.052659 ## 3 93 ... 244000 -93.617320 42.051245 ## 4 74 ... 189900 -93.638933 42.060899 ## 5 78 ... 195500 -93.638925 42.060779 ## ... ... ... ... ... ... ## 2921 55 ... 150900 -93.604475 41.990043 ## 2922 63 ... 188000 -93.603534 41.990134 ## 2923 80 ... 160000 -93.608688 41.988737 ## 2928 77 ... 170000 -93.600190 41.990921 ## 2929 74 ... 188000 -93.599996 41.989265 ## ## [1677 rows x 74 columns] Notemos que en el ejemplo anterior se usa &amp;, que ayuda a filtrar por dos condiciones. También puede usarse | para filtrar por alguna de las dos condiciones. ( ames_housing &gt;&gt; filter((_.Lot_Area &lt; 1000) | (_.Sale_Price &lt;= 150000)) ) ## MS_SubClass MS_Zoning \\ ## 1 One_Story_1946_and_Newer_All_Styles Residential_High_Density ## 18 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 23 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 24 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 25 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## ... ... ... ## 2920 Two_Story_PUD_1946_and_Newer Residential_Medium_Density ## 2924 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2925 Split_or_Multilevel Residential_Low_Density ## 2926 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2927 Split_Foyer Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 1 80 ... 105000 -93.619756 42.053014 ## 18 140 ... 141000 -93.622971 42.056673 ## 23 0 ... 149000 -93.626231 42.055147 ## 24 0 ... 149900 -93.626537 42.054592 ## 25 65 ... 142000 -93.628806 42.055227 ## ... ... ... ... ... ... ## 2920 21 ... 71000 -93.602345 41.991532 ## 2924 160 ... 131000 -93.606842 41.987686 ## 2925 37 ... 142500 -93.604776 41.988964 ## 2926 0 ... 131000 -93.602680 41.988314 ## 2927 62 ... 132000 -93.606847 41.986510 ## ## [1271 rows x 74 columns] Las condiciones pueden ser expresiones lógicas construidas mediante los operadores relacionales y lógicos: &lt; : Menor que &gt; : Mayor que == : Igual que &lt;= : Menor o igual que &gt;= : Mayor o igual que != : Diferente que isin : Pertenece al conjunto isnull : Es NA EJERCICIO: Practicar la función de filtro de observaciones usando los operadores auxiliares. Concatenar el resultado de seleccionar columnas y posteriormente filtrar columnas. 2.5.3 Ordenar registros La función arrange() se utiliza para ordenar las filas de un data frame de acuerdo a una o varias variables. Este ordenamiento puede ser ascendente o descendente. Por defecto arrange() ordena las filas por orden ascendente: ( ames_housing &gt;&gt; arrange(_.Sale_Price) ) ## MS_SubClass MS_Zoning \\ ## 181 One_Story_1945_and_Older Residential_Medium_Density ## 1553 One_Story_1946_and_Newer_All_Styles A_agr ## 726 One_Story_1945_and_Older C_all ## 2843 One_Story_1945_and_Older Residential_Low_Density ## 2880 One_Story_1946_and_Newer_All_Styles C_all ## ... ... ... ## 44 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 1063 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 2445 Two_Story_1946_and_Newer Residential_Low_Density ## 1760 Two_Story_1946_and_Newer Residential_Low_Density ## 1767 Two_Story_1946_and_Newer Residential_Low_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 181 68 ... 12789 -93.606789 42.030388 ## 1553 80 ... 13100 -93.625217 42.018806 ## 726 60 ... 34900 -93.605207 42.023218 ## 2843 60 ... 35000 -93.659043 42.021559 ## 2880 50 ... 35311 -93.615012 42.019099 ## ... ... ... ... ... ... ## 44 100 ... 611657 -93.655051 42.059617 ## 1063 106 ... 615000 -93.656958 42.058484 ## 2445 118 ... 625000 -93.657851 42.053314 ## 1760 160 ... 745000 -93.657592 42.053321 ## 1767 104 ... 755000 -93.657271 42.051980 ## ## [2930 rows x 74 columns] Si las queremos ordenar de forma ascendente, lo haremos del siguiente modo: ( ames_housing &gt;&gt; arrange(-_.Sale_Price) ) ## MS_SubClass MS_Zoning \\ ## 1767 Two_Story_1946_and_Newer Residential_Low_Density ## 1760 Two_Story_1946_and_Newer Residential_Low_Density ## 2445 Two_Story_1946_and_Newer Residential_Low_Density ## 1063 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## 44 One_Story_1946_and_Newer_All_Styles Residential_Low_Density ## ... ... ... ## 2880 One_Story_1946_and_Newer_All_Styles C_all ## 2843 One_Story_1945_and_Older Residential_Low_Density ## 726 One_Story_1945_and_Older C_all ## 1553 One_Story_1946_and_Newer_All_Styles A_agr ## 181 One_Story_1945_and_Older Residential_Medium_Density ## ## Lot_Frontage ... Sale_Price Longitude Latitude ## 1767 104 ... 755000 -93.657271 42.051980 ## 1760 160 ... 745000 -93.657592 42.053321 ## 2445 118 ... 625000 -93.657851 42.053314 ## 1063 106 ... 615000 -93.656958 42.058484 ## 44 100 ... 611657 -93.655051 42.059617 ## ... ... ... ... ... ... ## 2880 50 ... 35311 -93.615012 42.019099 ## 2843 60 ... 35000 -93.659043 42.021559 ## 726 60 ... 34900 -93.605207 42.023218 ## 1553 80 ... 13100 -93.625217 42.018806 ## 181 68 ... 12789 -93.606789 42.030388 ## ## [2930 rows x 74 columns] Si se desea usar dos o más columnas para realizar el ordenamiento, deben separarse por comas cada una de las características ( ames_housing &gt;&gt; arrange(_.Sale_Condition, -_.Sale_Price, _.Lot_Area) &gt;&gt; select(_.Sale_Condition, _.Sale_Price, _.Lot_Area) ) ## Sale_Condition Sale_Price Lot_Area ## 1760 Abnorml 745000 15623 ## 456 Abnorml 552000 14836 ## 1425 Abnorml 475000 11778 ## 1058 Abnorml 390000 13418 ## 1171 Abnorml 328900 5119 ## ... ... ... ... ## 2826 Partial 140000 3675 ## 2472 Partial 132000 13072 ## 1996 Partial 124500 3600 ## 1299 Partial 115000 6000 ## 712 Partial 113000 4456 ## ## [2930 rows x 3 columns] Notemos que en el ejemplo anterior usamos dos pipes (&gt;&gt;), como habíamos mencionado se pueden usar los necesarios para combinar funciones. 2.5.4 Agregar / Modificar Con la función mutate() podemos computar transformaciones de variables en un data frame. A menudo, tendremos la necesidad de crear nuevas variables que se calculan a partir de variables existentes. La función mutate() proporciona una interfaz clara para realizar este tipo de operaciones. Por ejemplo, haremos el cálculo de la antigüedad del inmueble a partir de las variables Year_Sold y Year_Remod_Add: ejemplo_mutate = ( ames_housing &gt;&gt; select(_.Year_Sold, _.Year_Remod_Add) &gt;&gt; mutate(Antique = _.Year_Sold - _.Year_Remod_Add) ) ejemplo_mutate ## Year_Sold Year_Remod_Add Antique ## 0 2010 1960 50 ## 1 2010 1961 49 ## 2 2010 1958 52 ## 3 2010 1968 42 ## 4 2010 1998 12 ## ... ... ... ... ## 2925 2006 1984 22 ## 2926 2006 1983 23 ## 2927 2006 1992 14 ## 2928 2006 1975 31 ## 2929 2006 1994 12 ## ## [2930 rows x 3 columns] El ejemplo anterior crea una nueva variable. Ahora se presenta otro ejemplo en donde se modifica una variable ya creada. ( ejemplo_mutate &gt;&gt; mutate(Antique = _.Antique * 12) ) ## Year_Sold Year_Remod_Add Antique ## 0 2010 1960 600 ## 1 2010 1961 588 ## 2 2010 1958 624 ## 3 2010 1968 504 ## 4 2010 1998 144 ## ... ... ... ... ## 2925 2006 1984 264 ## 2926 2006 1983 276 ## 2927 2006 1992 168 ## 2928 2006 1975 372 ## 2929 2006 1994 144 ## ## [2930 rows x 3 columns] En este segundo ejemplo, se modifica el número de años de antigüedad y se multiplica por un factor de 12 para modificar el tiempo en una escala de meses. 2.5.5 Resumen estadístico La función summarize() se comporta de forma análoga a la función mutate(), excepto que en lugar de añadir nuevas columnas crea un nuevo data frame que resumen el valor de los renglones a través de un estadístico. Podemos usar el ejemplo anterior y calcular la media de la variable creada Antique: ( ames_housing &gt;&gt; select(_.Year_Sold, _.Year_Remod_Add) &gt;&gt; mutate(Antique = _.Year_Sold - _.Year_Remod_Add) &gt;&gt; summarize( Mean_Antique = _.Antique.mean(), Median_Antique = _.Antique.median(), First_Antique = _.Antique.iloc[0], Last_Antique = _.Antique.iloc[-1], ) ) ## Mean_Antique Median_Antique First_Antique Last_Antique ## 0 23.523891 15.0 50 12 Solo fue necesario agregar un pipe, especificar el nombre de la variable creada y la operación a realizar. A continuación se muestran funciones que trabajando conjuntamente con la función summarize() facilitarán nuestro trabajo diario. Todas ellas toman como argumento un vector y devuelven un único resultado: min(), max() : Valores max y min. mean() : Media. median() : Mediana. sum() : Suma de los valores. var(), std() : Varianza y desviación estándar. count() : El número de valores en un vector. iloc() : Especificar el índice del elemento a extraer Mas adelante veremos como combinar esta función con la función group_by() para calcular estadísticos agrupados por alguna característica de interés. EJERCICIO: Realizar una consulta usando summarize() y cada una de las funciones estadísticas listadas anteriormente. 2.5.6 Agrupamiento La función group_by() agrupa un conjunto de filas de acuerdo con los valores de una o más columnas o expresiones. Usaremos el ejemplo anterior. Primero creamos nuestra nueva variable Antique, después agrupamos por vecindario y al final calculamos la media de la variable Antique. Gracias al agrupamiento, nos regresara una media por cada grupo creado, es decir, nos regresara el promedio de la antigüedad por vecindario. ( ames_housing &gt;&gt; mutate(Antique = _.Year_Sold - _.Year_Remod_Add) &gt;&gt; group_by(_.Neighborhood) &gt;&gt; summarize(Mean_Antique = _.Antique.mean().round(0) ) ) ## Neighborhood Mean_Antique ## 0 Bloomington_Heights 2.0 ## 1 Blueste 25.0 ## 2 Briardale 35.0 ## 3 Brookside 39.0 ## 4 Clear_Creek 28.0 ## 5 College_Creek 8.0 ## 6 Crawford 29.0 ## 7 Edwards 33.0 ## 8 Gilbert 9.0 ## 9 Green_Hills 14.0 ## 10 Greens 26.0 ## 11 Iowa_DOT_and_Rail_Road 43.0 ## 12 Landmark 12.0 ## 13 Meadow_Village 32.0 ## 14 Mitchell 22.0 ## 15 North_Ames 37.0 ## 16 Northpark_Villa 32.0 ## 17 Northridge 11.0 ## 18 Northridge_Heights 2.0 ## 19 Northwest_Ames 28.0 ## 20 Old_Town 34.0 ## 21 Sawyer 32.0 ## 22 Sawyer_West 15.0 ## 23 Somerset 3.0 ## 24 South_and_West_of_Iowa_State_University 44.0 ## 25 Stone_Brook 8.0 ## 26 Timberland 11.0 ## 27 Veenker 20.0 ¡¡ REVISAR !! En este link se encuentra un buen resumen de las funciones complementarias a dplyr y tidyr de R. La librería se llama plydata. 2.6 Orden y estructura Un conjunto de datos puede ser representado de muchas maneras distintas y contener en todos los casos la misma información. Sin embargo, no todos los modos en que se presenta la información resulta óptimo para su procesamiento y análisis. Los conjuntos de datos ordenados serán más fáciles de trabajar y analizar. Algunas de las características principales que presentan los conjuntos de datos ordenados son las siguientes: Cada variable debe tener su propia columna. Cada observación debe tener su propio renglón. Cada valor debe tener su propia celda. La figura anterior muestra la estructura de orden que debe tener un conjunto de datos. A pesar de que pueda parecer intuitivo y sencillo, en la práctica es considerable el número de conjuntos de datos desordenados. La limpieza y ordenamiento debe ser trabajado de forma impecable a fin de que puedan realizarse buenas prácticas. El tiempo de limpieza y ordenamiento varía mucho dependiendo de la dimensión del conjunto de datos. Algunos de los principales problemas que pueden tener los conjuntos de datos no ordenados son: Una variable puede estar dispersa en múltiples columnas Una observación puede estar esparcida en múltiples renglones La paquetería plydata cuenta con funciones para resolver dichos problemas. Entre las principales funciones que tiene la paquetería, se encuentran pivot_longer(), pivot_wider(), mismas que se analizarán a continuación. Primero tenemos que instalar y cargar la paquetería: #R virtualenv_install(&quot;venv_dsml_py3_10&quot;, &quot;plydata&quot;) # python import plydata as pr ( ames_housing &gt;&gt; mutate(Antique = _.Year_Sold - _.Year_Remod_Add) &gt;&gt; mutate(Antique_status = if_else(_.Antique &lt; 10, &quot;new&quot;, &quot;old&quot;) ) &gt;&gt; pr.group_by(&quot;Antique_status&quot;) &gt;&gt; pr.tally() ) ## Antique_status n ## 0 old 1779 ## 1 new 1151 2.6.1 Pivote horizontal La función pivot_wider() resulta muy útil a la hora de organizar los datos. Su función consiste en dispersar una variable clave en múltiples columnas. Lo primero que se debe hacer para poder hacer uso de dicha función es instalar y cargar la librería. El siguiente conjunto de datos contiene el número de localidades rurales y urbanas por municipio de la Ciudad de México. Como es posible observar, algunos municipios aparecen más de una vez en el marco de datos, esto se debe a que cada municipio puede tener ambos ámbitos, rural y urbano. Para hacer que el conjunto de datos sea ordenado, es necesario que cada observación aparezca una sola vez por renglón y cada una de las categorías (rural y urbano) de la variable “Ámbito” deberá ocupar el lugar de una columna. El siguiente código muestra cómo convertir los datos no ordenados en un conjunto ordenado. import pickle with open(&#39;data/loc_mun_cdmx.pkl&#39;, &#39;rb&#39;) as f: loc_mun_cdmx = pickle.load(f) loc_mun_cdmx ## Ambito NOM_MUN Total_localidades ## 0 Rural Álvaro Obregón 3 ## 1 Rural La Magdalena Contreras 8 ## 2 Rural Cuajimalpa de Morelos 14 ## 3 Rural Tláhuac 31 ## 4 Rural Xochimilco 78 ## 5 Rural Tlalpan 95 ## 6 Rural Milpa Alta 187 ## 7 Urbano Álvaro Obregón 1 ## 8 Urbano Azcapotzalco 1 ## 9 Urbano Benito Juárez 1 ## 10 Urbano Coyoacán 1 ## 11 Urbano Cuauhtémoc 1 ## 12 Urbano Gustavo A. Madero 1 ## 13 Urbano Iztacalco 1 ## 14 Urbano Iztapalapa 1 ## 15 Urbano La Magdalena Contreras 1 ## 16 Urbano Miguel Hidalgo 1 ## 17 Urbano Venustiano Carranza 1 ## 18 Urbano Xochimilco 1 ## 19 Urbano Cuajimalpa de Morelos 2 ## 20 Urbano Tlalpan 4 ## 21 Urbano Tláhuac 5 ## 22 Urbano Milpa Alta 10 En la tabla actual existe ahora un y solo un registro por cada observación (nombre de municipio en este caso). El valor que le corresponde a cada una de las columnas creadas es la frecuencia absoluta de localidades que tienen la característica “Rural” y “Urbano” respectivamente. Pero… ¿qué pasa cuando no existen todos los valores en ambas columnas? Si no se especifica la manera de llenar los datos faltantes, estos contendrán NAs. Siempre se puede elegir el caracter o número con el cual se imputan los datos faltantes. from plydata.tidy import pivot_wider ( loc_mun_cdmx &gt;&gt; pivot_wider(names_from = &quot;Ambito&quot;, values_from = &quot;Total_localidades&quot;) ) ## NOM_MUN Rural Urbano ## 0 Azcapotzalco NaN 1.0 ## 1 Benito Juárez NaN 1.0 ## 2 Coyoacán NaN 1.0 ## 3 Cuajimalpa de Morelos 14.0 2.0 ## 4 Cuauhtémoc NaN 1.0 ## 5 Gustavo A. Madero NaN 1.0 ## 6 Iztacalco NaN 1.0 ## 7 Iztapalapa NaN 1.0 ## 8 La Magdalena Contreras 8.0 1.0 ## 9 Miguel Hidalgo NaN 1.0 ## 10 Milpa Alta 187.0 10.0 ## 11 Tlalpan 95.0 4.0 ## 12 Tláhuac 31.0 5.0 ## 13 Venustiano Carranza NaN 1.0 ## 14 Xochimilco 78.0 1.0 ## 15 Álvaro Obregón 3.0 1.0 ( loc_mun_cdmx &gt;&gt; pivot_wider( names_from = &quot;Ambito&quot;, values_from = &quot;Total_localidades&quot;, values_fill = 0 ) ) ## NOM_MUN Rural Urbano ## 0 Azcapotzalco 0 1 ## 1 Benito Juárez 0 1 ## 2 Coyoacán 0 1 ## 3 Cuajimalpa de Morelos 14 2 ## 4 Cuauhtémoc 0 1 ## 5 Gustavo A. Madero 0 1 ## 6 Iztacalco 0 1 ## 7 Iztapalapa 0 1 ## 8 La Magdalena Contreras 8 1 ## 9 Miguel Hidalgo 0 1 ## 10 Milpa Alta 187 10 ## 11 Tlalpan 95 4 ## 12 Tláhuac 31 5 ## 13 Venustiano Carranza 0 1 ## 14 Xochimilco 78 1 ## 15 Álvaro Obregón 3 1 En caso de que existan múltiples columnas que se desean dispersar mediante el pivote de una columna con múltiples categorías, es posible especificar tal re estructuración a través del siguiente código. with open(&#39;data/us_rent_income.pkl&#39;, &#39;rb&#39;) as f: us_rent_income = pickle.load(f) us_rent_income ## GEOID NAME variable estimate moe ## 0 01 Alabama income 24476.0 136.0 ## 1 01 Alabama rent 747.0 3.0 ## 2 02 Alaska income 32940.0 508.0 ## 3 02 Alaska rent 1200.0 13.0 ## 4 04 Arizona income 27517.0 148.0 ## .. ... ... ... ... ... ## 99 55 Wisconsin rent 813.0 3.0 ## 100 56 Wyoming income 30854.0 342.0 ## 101 56 Wyoming rent 828.0 11.0 ## 102 72 Puerto Rico income NaN NaN ## 103 72 Puerto Rico rent 464.0 6.0 ## ## [104 rows x 5 columns] ( us_rent_income &gt;&gt; select(-_.GEOID) &gt;&gt; pivot_wider( names_from = &quot;variable&quot;, values_from = [&quot;estimate&quot;, &quot;moe&quot;] ) ) ## NAME estimate_income estimate_rent moe_income moe_rent ## 0 Alabama 24476.0 747.0 136.0 3.0 ## 1 Alaska 32940.0 1200.0 508.0 13.0 ## 2 Arizona 27517.0 972.0 148.0 4.0 ## 3 Arkansas 23789.0 709.0 165.0 5.0 ## 4 California 29454.0 1358.0 109.0 3.0 ## 5 Colorado 32401.0 1125.0 109.0 5.0 ## 6 Connecticut 35326.0 1123.0 195.0 5.0 ## 7 Delaware 31560.0 1076.0 247.0 10.0 ## 8 District of Columbia 43198.0 1424.0 681.0 17.0 ## 9 Florida 25952.0 1077.0 70.0 3.0 ## 10 Georgia 27024.0 927.0 106.0 3.0 ## 11 Hawaii 32453.0 1507.0 218.0 18.0 ## 12 Idaho 25298.0 792.0 208.0 7.0 ## 13 Illinois 30684.0 952.0 83.0 3.0 ## 14 Indiana 27247.0 782.0 117.0 3.0 ## 15 Iowa 30002.0 740.0 143.0 4.0 ## 16 Kansas 29126.0 801.0 208.0 5.0 ## 17 Kentucky 24702.0 713.0 159.0 4.0 ## 18 Louisiana 25086.0 825.0 155.0 4.0 ## 19 Maine 26841.0 808.0 187.0 7.0 ## 20 Maryland 37147.0 1311.0 152.0 5.0 ## 21 Massachusetts 34498.0 1173.0 199.0 5.0 ## 22 Michigan 26987.0 824.0 82.0 3.0 ## 23 Minnesota 32734.0 906.0 189.0 4.0 ## 24 Mississippi 22766.0 740.0 194.0 5.0 ## 25 Missouri 26999.0 784.0 113.0 4.0 ## 26 Montana 26249.0 751.0 206.0 9.0 ## 27 Nebraska 30020.0 773.0 146.0 4.0 ## 28 Nevada 29019.0 1017.0 213.0 6.0 ## 29 New Hampshire 33172.0 1052.0 387.0 9.0 ## 30 New Jersey 35075.0 1249.0 148.0 4.0 ## 31 New Mexico 24457.0 809.0 214.0 6.0 ## 32 New York 31057.0 1194.0 69.0 3.0 ## 33 North Carolina 26482.0 844.0 111.0 3.0 ## 34 North Dakota 32336.0 775.0 245.0 9.0 ## 35 Ohio 27435.0 764.0 94.0 2.0 ## 36 Oklahoma 26207.0 766.0 101.0 3.0 ## 37 Oregon 27389.0 988.0 146.0 4.0 ## 38 Pennsylvania 28923.0 885.0 119.0 3.0 ## 39 Puerto Rico NaN 464.0 NaN 6.0 ## 40 Rhode Island 30210.0 957.0 259.0 6.0 ## 41 South Carolina 25454.0 836.0 123.0 4.0 ## 42 South Dakota 28821.0 696.0 276.0 7.0 ## 43 Tennessee 25453.0 808.0 102.0 4.0 ## 44 Texas 28063.0 952.0 110.0 2.0 ## 45 Utah 27928.0 948.0 239.0 6.0 ## 46 Vermont 29351.0 945.0 361.0 11.0 ## 47 Virginia 32545.0 1166.0 202.0 5.0 ## 48 Washington 32318.0 1120.0 113.0 4.0 ## 49 West Virginia 23707.0 681.0 203.0 6.0 ## 50 Wisconsin 29868.0 813.0 135.0 3.0 ## 51 Wyoming 30854.0 828.0 342.0 11.0 Ejercicio: Agrupa los datos de localidades por ámbito Agrega una columna con el porcentaje de localidades por alcaldía Realiza un pivote horizontal sobre el ámbito y las variables numéricas de total de localidades y su respectivo porcentaje de población (crear). Ordena los registros de forma descendente de acuerdo con el total de localidades rural y urbano. 2.6.2 Pivote vertical pivot_longer() es podría ser la función inversa de la anterior, se necesita comúnmente para ordenar los conjuntos de datos capturados en crudo, ya que a menudo no son capturados acorde a las mejores estructuras para facilitar el análisis. El conjunto de datos relig_income almacena recuentos basados en una encuesta que (entre otras cosas) preguntó a las personas sobre su religión e ingresos anuales: with open(&#39;data/relig_income.pkl&#39;, &#39;rb&#39;) as f: relig_income = pickle.load(f) relig_income ## religion &lt;$10k $10-20k ... $100-150k &gt;150k \\ ## 0 Agnostic 27.0 34.0 ... 109.0 84.0 ## 1 Atheist 12.0 27.0 ... 59.0 74.0 ## 2 Buddhist 27.0 21.0 ... 39.0 53.0 ## 3 Catholic 418.0 617.0 ... 792.0 633.0 ## 4 Don’t know/refused 15.0 14.0 ... 17.0 18.0 ## 5 Evangelical Prot 575.0 869.0 ... 723.0 414.0 ## 6 Hindu 1.0 9.0 ... 48.0 54.0 ## 7 Historically Black Prot 228.0 244.0 ... 81.0 78.0 ## 8 Jehovah&#39;s Witness 20.0 27.0 ... 11.0 6.0 ## 9 Jewish 19.0 19.0 ... 87.0 151.0 ## 10 Mainline Prot 289.0 495.0 ... 753.0 634.0 ## 11 Mormon 29.0 40.0 ... 49.0 42.0 ## 12 Muslim 6.0 7.0 ... 8.0 6.0 ## 13 Orthodox 13.0 17.0 ... 42.0 46.0 ## 14 Other Christian 9.0 7.0 ... 14.0 12.0 ## 15 Other Faiths 20.0 33.0 ... 40.0 41.0 ## 16 Other World Religions 5.0 2.0 ... 4.0 4.0 ## 17 Unaffiliated 217.0 299.0 ... 321.0 258.0 ## ## Don&#39;t know/refused ## 0 96.0 ## 1 76.0 ## 2 54.0 ## 3 1489.0 ## 4 116.0 ## 5 1529.0 ## 6 37.0 ## 7 339.0 ## 8 37.0 ## 9 162.0 ## 10 1328.0 ## 11 69.0 ## 12 22.0 ## 13 73.0 ## 14 18.0 ## 15 71.0 ## 16 8.0 ## 17 597.0 ## ## [18 rows x 11 columns] ¿Crees que ésta es la mejor estructura para la tabla? ¿Cómo imaginas que podría modificarse? Este conjunto de datos contiene tres variables: religión, almacenada en las filas income repartidos entre los nombres de columna count almacenado en los valores de las celdas. Para ordenarlo usamos pivot_longer(): from plydata.tidy import pivot_longer ( relig_income &gt;&gt; pivot_longer( cols = [&#39;&lt;$10k&#39;, &#39;$10-20k&#39;, &#39;$20-30k&#39;, &#39;$30-40k&#39;, &#39;$40-50k&#39;, &#39;$50-75k&#39;, &#39;$75-100k&#39;, &#39;$100-150k&#39;, &#39;&gt;150k&#39;, &quot;Don&#39;t know/refused&quot;], names_to = &quot;income&quot;, values_to = &quot;count&quot;) ) ## religion income count ## 0 Agnostic &lt;$10k 27.0 ## 1 Atheist &lt;$10k 12.0 ## 2 Buddhist &lt;$10k 27.0 ## 3 Catholic &lt;$10k 418.0 ## 4 Don’t know/refused &lt;$10k 15.0 ## .. ... ... ... ## 175 Orthodox Don&#39;t know/refused 73.0 ## 176 Other Christian Don&#39;t know/refused 18.0 ## 177 Other Faiths Don&#39;t know/refused 71.0 ## 178 Other World Religions Don&#39;t know/refused 8.0 ## 179 Unaffiliated Don&#39;t know/refused 597.0 ## ## [180 rows x 3 columns] El primer argumento es el conjunto de datos para remodelar, relig_income. El segundo argumento describe qué columnas necesitan ser reformadas. En este caso, son todas las columnas excepto por religion. El names_to da el nombre de la variable que se creará a partir de los datos almacenados en los nombres de columna, es decir, ingresos. Los values_to dan el nombre de la variable que se creará a partir de los datos almacenados en el valor de la celda, es decir, count. Ni la columna names_to ni la values_to existen en relig_income, por lo que las proporcionamos como cadenas de caracteres entre comillas. "],["visualización.html", "Capítulo 3 Visualización 3.1 EDA: Análisis Exploratorio de Datos 3.2 GEDA: Análisis Exploratorio de Datos Gráficos 3.3 Ggplot / plotnine 3.4 Análisis univariado 3.5 Análisis multivariado 3.6 Reporte interactivos", " Capítulo 3 Visualización “El análisis exploratorio de datos se refiere al proceso de realizar investigaciones iniciales sobre los datos para descubrir patrones, detectar anomalías, probar hipótesis y verificar suposiciones con la ayuda de estadísticas resumidas y representaciones gráficas.” Towards 3.1 EDA: Análisis Exploratorio de Datos Un análisis exploratorio de datos tiene principalmente 5 objetivos: Maximizar el conocimiento de un conjunto de datos Descubrir la estructura subyacente de los datos Extraer variables importantes Detectar valores atípicos y anomalías Probar los supuestos subyacentes EDA no es idéntico a los gráficos estadísticos aunque los dos términos se utilizan casi indistintamente. Los gráficos estadísticos son una colección de técnicas, todas basadas en gráficos y todas centradas en un aspecto de caracterización de datos. EDA abarca un lugar más grande. EDA es una filosofía sobre cómo diseccionar un conjunto de datos; lo que buscamos; cómo nos vemos; y cómo interpretamos. Los científicos de datos pueden utilizar el análisis exploratorio para garantizar que los resultados que producen sean válidos y aplicables a los resultados y objetivos comerciales deseados. EDA se utiliza principalmente para ver qué datos pueden revelar más allá del modelado formal o la tarea de prueba de hipótesis y proporciona una mejor comprensión de las variables del conjunto de datos y las relaciones entre ellas. También puede ayudar a determinar si las técnicas estadísticas que está considerando para el análisis de datos son apropiadas. Dependiendo del tipo de variable queremos obtener la siguiente información: Variables numéricas: Tipo de dato: float, integer Número de observaciones Mean Desviación estándar Cuartiles: 25%, 50%, 75% Valor máximo Valor mínimo Número de observaciones únicos Top 5 observaciones repetidas Número de observaciones con valores faltantes ¿Hay redondeos? Variables categóricas Número de categorías Valor de las categorías Moda Valores faltantes Número de observaciones con valores faltantes Proporción de observaciones por categoría Top 1, top 2, top 3 (moda 1, moda 2, moda 3) Faltas de ortografía ? Fechas Fecha inicio Fecha fin Huecos en las fechas: sólo tenemos datos entre semana, etc. Formatos de fecha (YYYY-MM-DD) Tipo de dato: date, time, timestamp Número de faltantes (NA) Número de observaciones Texto Longitud promedio de cada observación Identificar el lenguaje, si es posible Longitud mínima de cada observación Longitud máxima de cada observación Cuartiles de longitud: 25%, 50%, 75% Coordenadas geoespaciales Primero se pone la latitud y luego la longitud Primer decimal: 111 kms Segundo decimal: 11.1 kms Tercer decimal: 1.1 kms Cuarto decimal: 11 mts Quinto decimal: 1.1 mt Sexto decimal: 0.11 mts Valores que están cercanos al 100 representan la longitud El símbolo en cada coordenada representa si estamos al norte (positivo) o sur (negativo) -en la latitud-, al este (positivo) o al - oeste (negativo) -en la longitud-. 3.2 GEDA: Análisis Exploratorio de Datos Gráficos Como complemento al EDA podemos realizar un GEDA, que es un análisis exploratorio de los datos apoyándonos de visualizaciones, la visualización de datos no trata de hacer gráficas “bonitas” o “divertidas”, ni de simplificar lo complejo. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. 3.2.1 Lo que no se debe hacer… Fuentes: WTF Visualizations Flowingdata 3.2.2 Principios de visualización El objetivo de una visualización es sintetizar información relevante al análisis presentada de manera sencilla y sin ambigüedad. Lo usamos de apoyo para explicar a una audiencia más amplia que puede no ser tan técnica. Una gráfica debe reportar el resultado de un análisis detallado, nunca lo reemplaza. No hacer gráficas porque se vean “cool” Antes de hacer una gráfica, debe pensarse en lo que se quiere expresar o representar Existen “reglas” o mejores gráficas para representar cierto tipo de información de acuerdo a los tipos de datos que se tienen o al objetivo se quiere lograr con la visualización. From Data to Viz No utilizar pie charts 3.3 Ggplot / plotnine En R se creó una de las mejores librerías para visualizar datos. Su nombre es ggplot. Aunque python tiene sus propias librerías como matplotlib y seaborn, ninguna ha tenido tanto reconocimiento como ggplot en R. Por esta razón, en python se ha desarrollado una librería que simule las ya creadas y aprobadas funciones de gglpot. El nombre de la librería es: plotnine. En Ggplot todo funciona a través de capas. Las capas se añaden una sobre otra eligiendo y personalizando las estéticas visuales. En todo momento es posible especificar los colores, grosor, transparencia, formas, etc que cada uno de los gráficos va tomando para formar la imagen general. 3.3.1 Capas Estéticas En ggplot2, aestetics significa “algo que puedes ver”. Algunos ejemplos son: Posición (por ejemplo, los ejes x e y) Color (color “externo”) Fill (color de relleno) Shape (forma de puntos) Linetype (tipo de linea) Size (tamaño) Alpha (para la transparencia: los valores más altos tendrían formas opacas y los más bajos, casi transparentes). Hay que advertir que no todas las estéticas tienen la misma potencia en un gráfico. El ojo humano percibe fácilmente longitudes distintas. Pero tiene problemas para comparar áreas (que es lo que regula la estética size) o intensidades de color. Se recomienda usar las estéticas más potentes para representar las variables más importantes. Cada tipo de objeto geométrico (geom) solo acepta un subconjunto de todos los aestéticos. Puedes consultar la pagina de ayuda de geom() para ver que aestéticos acepta. El mapeo aestético se hace con la función aes(). 3.3.2 Capas geométricas Los objetos geométricos son las formas que puede tomar un gráfico. Algunos ejemplos son: Barras (geom_bar(), para las variables univariados discretos o nominales) Histogramas (geom_hist() para aquellas variables univariadas continuas) Puntos (geom_point() para scatter plots, gráficos de puntos, etc…) Lineas (geom_line() para series temporales, lineas de tendencia, etc…) Cajas (geom_boxplot() para gráficos de cajas) Un gráfico debe tener al menos un geom, pero no hay limite. Puedes añadir más geom usando el signo +. Una vez añadida una capa al gráfico a este pueden agregarse nuevas capas ## sepal_length sepal_width petal_length petal_width species species_cat ## 0 5.1 3.5 1.4 0.2 0.0 setosa ## 1 4.9 3.0 1.4 0.2 0.0 setosa ## 2 4.7 3.2 1.3 0.2 0.0 setosa ## 3 4.6 3.1 1.5 0.2 0.0 setosa ## 4 5.0 3.6 1.4 0.2 0.0 setosa ## &lt;Figure Size: (1280 x 960)&gt; ( iris &gt;&gt; ggplot(aes(x = &quot;sepal_length&quot;, y = &quot;sepal_width&quot;, color = &quot;species_cat&quot;)) + geom_point() + geom_smooth() ) ## &lt;Figure Size: (1280 x 960)&gt; 3.3.3 Facetas Muchos de los gráficos que pueden generarse con los elementos anteriores pueden reproducirse usando los gráficos tradicionales de R, pero no los que usan facetas, que pueden permitirnos explorar las variables de diferente forma, por ejemplo: ( ggplot(iris, aes(x = &quot;sepal_length&quot;, y = &quot;sepal_width&quot;, color = &quot;species_cat&quot;)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + facet_wrap(&quot;species_cat&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; crea tres gráficos dispuestos horizontalmente que comparan la relación entre la anchura y la longitud del pétalo de las tres especies de iris. Una característica de estos gráficos, que es crítica para poder hacer comparaciones adecuadas, es que comparten ejes. 3.3.4 Más sobre estéticas Para los ejercicios en clase utilizaremos el set de datos: Diamonds: from plotnine import data as p9d diamonds = p9d.diamonds diamonds ## carat cut color ... x y z ## 0 0.23 Ideal E ... 3.95 3.98 2.43 ## 1 0.21 Premium E ... 3.89 3.84 2.31 ## 2 0.23 Good E ... 4.05 4.07 2.31 ## 3 0.29 Premium I ... 4.20 4.23 2.63 ## 4 0.31 Good J ... 4.34 4.35 2.75 ## ... ... ... ... ... ... ... ... ## 53935 0.72 Ideal D ... 5.75 5.76 3.50 ## 53936 0.72 Good D ... 5.69 5.75 3.61 ## 53937 0.70 Very Good D ... 5.66 5.68 3.56 ## 53938 0.86 Premium H ... 6.15 6.12 3.74 ## 53939 0.75 Ideal D ... 5.83 5.87 3.64 ## ## [53940 rows x 10 columns] Descripción Un conjunto de datos que contiene los precios y otros atributos de casi 54.000 diamantes. Las variables son las siguientes: price: precio en dólares estadounidenses ( $ 326 -  $ 18,823) carat: peso del diamante (0.2–5.01) cut: calidad del corte (Regular, Bueno, Muy Bueno, Premium, Ideal) color: color del diamante, de D (mejor) a J (peor) clarity: una medida de la claridad del diamante (I1 (peor), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (mejor)) x: longitud en mm (0-10,74) y: ancho en mm (0–58,9) width in mm (0–58.9) z: profundidad en mm (0–31,8) depth porcentaje de profundidad total = z / media (x, y) = 2 * z / (x + y) (43–79) table: ancho de la parte superior del diamante en relación con el punto más ancho (43–95) Ejemplo práctico: diamonds >> ggplot() + aes(x = cut_number('carat', 5), y = 'price') + geom_boxplot() + aes(color = 'cut') + labs(title = 'Distribución de precio por categoría de corte') + labs(caption = 'Data source:Diamont set') + labs(x = 'Peso del diamante') + labs(y = 'Precio') + guides(color = guide_legend(title = 'Calidad del corte')) + ylim(0, 20000) 3.4 Análisis univariado El análisis univariado tiene como objetivo conocer la calidad y distribución de los datos. Se busca conocer medidas de tendencia central, variación promedio, cantidad de valores perdidos, etc. Es vital conocer los datos y su calidad antes de usarlos. 3.4.1 Variables numéricas Los histogramas son gráficas de barras que se obtienen a partir de tablas de frecuencias, donde cada barra se escala según la frecuencia relativa entre el ancho del intervalo de clase correspondiente. Un histograma muestra la acumulación ó tendencia, la variabilidad o dispersión y la forma de la distribución. El Diagrama de Caja y bigotes un tipo de gráfico que muestra un resumen de una gran cantidad de datos en cinco medidas descriptivas, además de intuir su morfología y simetría. Este tipo de gráficos nos permite identificar valores atípicos y comparar distribuciones. Además de conocer de una forma cómoda y rápida como el 50% de los valores centrales se distribuyen. Se puede detectar rápidamente los siguientes valores: Primer cuartil: el 25% de los valores son menores o igual a este valor (punto 2 en el gráfico anterior). Mediana o Segundo Cuartil: Divide en dos partes iguales la distribución. De forma que el 50% de los valores son menores o igual a este valor (punto 3 en el gráfico siguiente). Tercer cuartil: el 75% de los valores son menores o igual a este valor (punto 4 en el gráfico siguiente). Rango Intercuartílico (RIC): Diferencia entre el valor del tercer cuartil y el primer cuartil. Tip: El segmento que divide la caja en dos partes es la mediana (punto 3 del gráfico), que facilitará la comprensión de si la distribución es simétrica o asimétrica, si la mediana se sitúa en el centro de la caja entonces la distribución es simétrica y tanto la media, mediana y moda coinciden. Precio from mizani.formatters import comma_format, dollar_format ( diamonds &gt;&gt; ggplot(aes(x = &quot;price&quot;)) + geom_histogram(color = &quot;pink&quot;, fill = &quot;purple&quot;, bins=30) + scale_x_continuous(labels=dollar_format()) + scale_y_continuous(labels=comma_format()) + ggtitle(&quot;Distribución d eprecio&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot(aes(x = &quot;price&quot;)) + geom_histogram(aes(y=&#39;stat(density)&#39;), bins = 30, fill = &#39;blue&#39;, color = &quot;white&quot;) + geom_density(colour = &quot;black&quot;, size = 1) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot() + geom_boxplot(aes(x = 0, y = &quot;price&quot;), color= &quot;blue&quot;, fill= &quot;lightblue&quot;) + scale_y_continuous(labels = dollar_format(prefix=&#39;$&#39;, digits=0, big_mark=&#39;,&#39;)) + theme(axis_text_x=element_blank()) + ggtitle(&quot;Distribución de precio&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; Peso del diamante ( diamonds &gt;&gt; ggplot(aes(x = 0, y = &quot;carat&quot;)) + geom_boxplot(color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha = 0.3) + scale_y_continuous(labels = comma_format(digits=1)) + theme(axis_text_y=element_blank()) + coord_flip() + ggtitle(&quot;Distribución del peso de los diamantes&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; 3.4.2 Variables nominales/categóricas Calidad de corte ( diamonds &gt;&gt; ggplot( aes( x = &quot;cut&quot;)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;cyan&quot;, alpha= 0.7) + scale_y_continuous(labels = comma_format()) + ggtitle(&quot;Distribución de calidad de corte&quot;) + theme_dark() ) ## &lt;Figure Size: (1280 x 960)&gt; Claridad ( diamonds &gt;&gt; ggplot( aes( x = &quot;clarity&quot;)) + geom_bar( fill= &quot;darkblue&quot;, color= &quot;black&quot;, alpha= 0.7) + geom_text( aes(label=after_stat(&#39;count&#39;), group=1), stat=&#39;count&#39;, nudge_x=0, nudge_y=0.125, va=&#39;bottom&#39;, format_string=&#39;{:,.0f}&#39;) + scale_y_continuous(labels = comma_format()) + ggtitle(&quot;Distribución claridad&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot( aes( x = &quot;clarity&quot;)) + geom_bar( fill= &quot;darkblue&quot;, color= &quot;black&quot;, alpha= 0.9) + geom_text( aes(label=after_stat(&#39;count / sum(count) * 100&#39;), group=1), color = &quot;white&quot;, stat=&#39;count&#39;, va=&#39;bottom&#39;, ha=&#39;right&#39;, format_string=&#39;{:.1f}%&#39;) + scale_y_continuous(labels = comma_format()) + ggtitle(&quot;Distribución claridad&quot;) + coord_flip() ) ## &lt;Figure Size: (1280 x 960)&gt; 3.5 Análisis multivariado Un buen análisis de datos, requiere del análisis conjunto de variables. Una sola variable es importante de analizar en cuanto a su distribución y calidad, no obstante, no dice mucho al analizarse por sí sola. Es por ello, que es indispensable analizar la covariabilidad y dependencia entre los distintos atributos de la información. En el análisis multivariado, se busca comparar la información haciendo contrastes de colores, formas, tamaños, paneles, etc. Precio vs Calidad del corte ( diamonds &gt;&gt; ggplot(aes(y = &quot;price&quot;, x = &quot;cut&quot;, color = &quot;cut&quot;)) + geom_jitter(size = 0.3, alpha = 0.3) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot(aes(y = &quot;price&quot;, x = &quot;cut&quot;, color = &quot;cut&quot;)) + geom_boxplot(size=1, alpha= 0.3) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot(aes(x = &quot;price&quot; ,fill = &quot;cut&quot;)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot(aes(x= &quot;price&quot; ,fill = &quot;cut&quot;)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) + facet_wrap(&quot;cut&quot;, ncol = 1) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot( aes(x = &quot;carat&quot;, y = &quot;price&quot;)) + geom_point(aes(color = &quot;clarity&quot;), size = 0.5, alpha = 0.3 ) + geom_smooth() + ylim(0, 20000) ) ## &lt;Figure Size: (1280 x 960)&gt; ( diamonds &gt;&gt; ggplot( aes(x = &quot;carat&quot;, y = &quot;price&quot;)) + geom_point(aes(color = &quot;clarity&quot;), size = 0.3, alpha = 0.3 ) + facet_wrap(&quot;clarity&quot;)+ geom_smooth() + ylim(0, 20000) ) ## &lt;Figure Size: (1280 x 960)&gt; Ejercicios Realiza 3 gráficas miltivariadas (involucra 4 o 5 variables). Interpreta resultados ¡ Warning ! Nunca se debe olvidar que debemos de analizar los datos de manera objetiva, nuestro criterio sobre un problema o negocio no debe de tener sesgos sobre lo que “nos gustaría encontrar en los datos” o lo “que creemos que debe pasar”…. 3.6 Reporte interactivos Es posible automatizar reportes de análisis de datos. Los reportes pueden realizarse tanto en formato estático (.docx y .pdf) como en formato interactivo (.html). Existen diversos manuales sumamente amplios que permiten conocer las múltiples funcionalidades de las librerías que hacen posible la creación de documentos. Para el caso de documentos, existe la librería Quarto, la cual crea un documento estático o interactivo, mientras que para reportes en presentaciones existe la librería Xaringan, la cual sustituye a las presentaciones de power point. Es importante tomar en cuenta el balance entre complejidad y funcionalidad. Si bien es cierto que a través de esta herramienta pueden automatizarse reportes que consideren los resultados salientes de python, es importante considerar que las personas que puedan editar tal presentación serán limitadas. A continuación se enlistan los links de tutoriales para crear los documentos mencionados: Quarto Xaringan Otros lenguajes Tablas estáticas Tablas interactivas "],["introducción-a-machine-learning.html", "Capítulo 4 Introducción a Machine Learning 4.1 Análisis Supervisado vs No supervisado 4.2 Sesgo vs varianza 4.3 Partición de datos 4.4 Feature engineering 4.5 Pipeline", " Capítulo 4 Introducción a Machine Learning Como se había mencionado, el Machine Learning es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos para hacer predicciones. Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma. El término se utilizó por primera vez en 1959. Sin embargo, ha ganado relevancia en los últimos años debido al aumento de la capacidad de computación y al BOOM de los datos. Un algoritmo para computadoras puede ser pensado como una receta. Describe exactamente qué pasos se realizan uno tras otro. Los ordenadores no entienden las recetas de cocina, sino los lenguajes de programación: En ellos, el algoritmo se descompone en pasos formales (comandos) que el ordenador puede entender. La cuestión no es solo saber para qué sirve el Machine Learning, sino que saber cómo funciona y cómo poder implementarlo en la industria para aprovecharse de sus beneficios. Hay ciertos pasos que usualmente se siguen para crear un modelo de Machine Learning. Estos son típicamente realizados por científicos de los datos que trabajan en estrecha colaboración con los profesionales de los negocios para los que se está desarrollando el modelo. Seleccionar y preparar un conjunto de datos de entrenamiento Los datos de entrenamiento son un conjunto de datos representativos de los datos que el modelo de Machine Learning ingerirá para resolver el problema que está diseñado para resolver. Los datos de entrenamiento deben prepararse adecuadamente: aleatorizados y comprobados en busca de desequilibrios o sesgos que puedan afectar al entrenamiento. También deben dividirse en dos subconjuntos: el subconjunto de entrenamiento, que se utilizará para entrenar el algoritmo, y el subconjunto de validación, que se utilizará para probarlo y perfeccionarlo. Elegir un algoritmo para ejecutarlo en el conjunto de datos de entrenamiento Este es uno de los pasos más importantes, ya que se debe elegir qué algoritmo utilizar, siendo este un conjunto de pasos de procesamiento estadístico. El tipo de algoritmo depende del tipo (supervisado o no supervisado), la cantidad de datos del conjunto de datos de entrenamiento y del tipo de problema que se debe resolver. Entrenamiento del algoritmo para crear el modelo El entrenamiento del algoritmo es un proceso iterativo: implica ejecutar las variables a través del algoritmo, comparar el resultado con los resultados que debería haber producido, ajustar los pesos y los sesgos dentro del algoritmo que podrían dar un resultado más exacto, y ejecutar las variables de nuevo hasta que el algoritmo devuelva el resultado correcto la mayoría de las veces. El algoritmo resultante, entrenado y preciso, es el modelo de Machine Learning. Usar y mejorar el modelo El paso final es utilizar el modelo con nuevos datos y, en el mejor de los casos, para que mejore en precisión y eficacia con el tiempo. De dónde procedan los nuevos datos dependerá del problema que se resuelva. Por ejemplo, un modelo de Machine Learning diseñado para identificar el spam ingerirá mensajes de correo electrónico, mientras que un modelo de Machine Learning que maneja una aspiradora robot ingerirá datos que resulten de la interacción en el mundo real con muebles movidos o nuevos objetos en la habitación. 4.1 Análisis Supervisado vs No supervisado Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: Aprendizaje supervisado: estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Aprendizaje no supervisado: en el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. En el aprendizaje no supervisado, carecemos de este tipo de etiqueta. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir qué es qué por nosotros mismos. Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad Aprendizaje por refuerzo: su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN 4.1.1 Regresión vs clasificación Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 4.2 Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobre-ajuste y falta de ajuste. 4.2.1 Balance entre sesgo y varianza o Trade-off El objetivo de cualquier algoritmo supervisado de Machine Learning es lograr un sesgo bajo, una baja varianza y a su vez el algoritmo debe lograr un buen rendimiento de predicción. El sesgo frente a la varianza se refiere a la precisión frente a la consistencia de los modelos entrenados por su algoritmo. Podemos diagnosticarlos de la siguiente manera: Los algoritmos de baja varianza (alto sesgo) tienden a ser menos complejos, con una estructura subyacente simple o rígida. Los algoritmos de bajo sesgo (alta varianza) tienden a ser más complejos, con una estructura subyacente flexible. No hay escapatoria a la relación entre el sesgo y la varianza en Machine Learning, aumentar el sesgo disminuirá la varianza, aumentar la varianza disminuirá el sesgo. 4.2.2 Error total Comprender el sesgo y la varianza es fundamental para comprender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el error general, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el sesgo es equivalente a la reducción en la varianza. Para construir un buen modelo, necesitamos encontrar un buen equilibrio entre el sesgo y la varianza de manera que minimice el error total. 4.2.3 Overfitting El modelo es muy particular. Error debido a la varianza Durante el entrenamiento tiene un desempeño muy bueno, pero al pasar nuevos datos su desempeño es malo. 4.2.4 Underfitting El modelo es demasiado general. Error debido al sesgo. Durante el entrenamiento no tiene un buen desempeño. 4.2.5 Error irreducible El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 4.3 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 4.3.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. Supongamos que asignamos el \\(80\\%\\) de los datos al conjunto de entrenamiento y el \\(20\\%\\) restante a las pruebas. El método más común es utilizar un muestreo aleatorio simple. En python existe un módulo de sklearn que permite hacer tal separación de datos: import pandas as pd from siuba import select, _ from plydata.one_table_verbs import pull from sklearn.model_selection import train_test_split ames = pd.read_csv(&quot;data/ames.csv&quot;) y = ames &gt;&gt; pull(&quot;Sale_Price&quot;) X = select(ames, -_.Sale_Price) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.20, random_state = 12345 ) print(&quot;Tamaño de conjunto de entrenamiento: &quot;, X_train.shape) ## Tamaño de conjunto de entrenamiento: (2344, 73) print(&quot;Tamaño de conjunto de prueba: &quot;, X_test.shape) ## Tamaño de conjunto de prueba: (586, 73) El muestreo aleatorio simple es apropiado en muchos casos, pero hay excepciones. Cuando hay un desbalance de clases en los problemas de clasificación, el uso de una muestra aleatoria simple puede asignar al azar estas muestras poco frecuentes de manera desproporcionada al conjunto de entrenamiento o prueba. Para evitar esto, se puede utilizar un muestreo estratificado. La división de entrenamiento/prueba se lleva a cabo por separado dentro de cada clase y luego estas submuestras se combinan en el conjunto general de entrenamiento y prueba. Para los problemas de regresión, los datos de los resultados se pueden agrupar artificialmente en cuartiles y luego realizar un muestreo estratificado cuatro veces por separado. Este es un método eficaz para mantener similares las distribuciones del resultado entre el conjunto de entrenamiento y prueba. Observamos que la distribución del precio de venta está sesgada a la derecha. Las casas más caras no estarían bien representadas en el conjunto de entrenamiento con una simple partición; esto aumentaría el riesgo de que nuestro modelo sea ineficaz para predecir el precio de dichas propiedades. Las líneas verticales punteadas indican los cuatro cuartiles para estos datos. Una muestra aleatoria estratificada llevaría a cabo la división 80/20 dentro de cada uno de estos subconjuntos de datos y luego combinaría los resultados. En sklearn, esto se logra usando el argumento de estratos: import numpy as np numeric_column = ames &gt;&gt; pull(&quot;Sale_Price&quot;) quartiles = np.percentile(numeric_column, [25, 50, 75]) # Crea una nueva variable categórica basada en los cuartiles stratify_variable = pd.cut( numeric_column, bins=[float(&#39;-inf&#39;), quartiles[0], quartiles[1], quartiles[2], float(&#39;inf&#39;)], labels=[&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;] ) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.20, random_state = 12345, stratify = stratify_variable ) ¿Qué proporción debería ser usada? No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Muy pocos datos en el conjunto de entrenamiento obstaculizan la capacidad del modelo para encontrar estimaciones de parámetros adecuadas y muy pocos datos en el conjunto de prueba reducen la calidad de las estimaciones de rendimiento. Se debe elegir un porcentaje que cumpla con los objetivos de nuestro proyecto con consideraciones que incluyen: Costo computacional en el entrenamiento del modelo. Costo computacional en la evaluación del modelo. Representatividad del conjunto de formación. Representatividad del conjunto de pruebas. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 4.3.2 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobre-ajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Los conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una sola partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo. Con sklearn, un conjunto de validación es como cualquier otro objeto de remuestreo; este tipo es diferente solo en que tiene una sola iteración # Dividir los datos en entrenamiento (60%) y el resto (40%) X_train, X_temp, y_train, y_temp = train_test_split( X, y, test_size = 0.4, random_state = 12345 ) # Dividir el resto en conjuntos de prueba (15%) y validación (25%) X_test, X_val, y_test, y_val = train_test_split( X_temp, y_temp, test_size = 0.625, random_state = 42 ) # Training (60%), testing (15%), validation (25%) # Imprimir los tamaños de los conjuntos resultantes print(&quot;Tamaño de conjunto de entrenamiento: &quot;, X_train.shape) ## Tamaño de conjunto de entrenamiento: (1758, 73) print(&quot;Tamaño de conjunto de prueba: &quot;, X_test.shape) ## Tamaño de conjunto de prueba: (439, 73) print(&quot;Tamaño de conjunto de validación: &quot;, X_val.shape) ## Tamaño de conjunto de validación: (733, 73) Esta función regresa una columna para los objetos de división de datos y una columna llamada id que tiene una cadena de caracteres con el identificador de remuestreo. El argumento de estratos hace que el muestreo aleatorio se lleve a cabo dentro de la variable de estratificación. Esto puede ayudar a garantizar que el número de datos en los datos del análisis sea equivalente a las proporciones del conjunto de datos original. (Los estratos inferiores al 10% del total se agrupan). Otra opción de muestreo bastante común es la realizada mediante múltiples submuestras de los datos originales. Diversos métodos se revisarán a lo largo del curso. 4.3.3 Leave-one-out cross-validation La validación cruzada es una manera de predecir el ajuste de un modelo a un hipotético conjunto de datos de prueba cuando no disponemos del conjunto explícito de datos de prueba. El método LOOCV en un método iterativo que se inicia empleando como conjunto de entrenamiento todas las observaciones disponibles excepto una, que se excluye para emplearla como validación. Si se emplea una única observación para calcular el error, este varía mucho dependiendo de qué observación se haya seleccionado. Para evitarlo, el proceso se repite tantas veces como observaciones disponibles se tengan, excluyendo en cada iteración una observación distinta, ajustando el modelo con el resto y calculando el error con dicha observación. Finalmente, el error estimado por el es el promedio de todos lo \\(i\\) errores calculados. La principal desventaja de este método es su costo computacional. El proceso requiere que el modelo sea reajustado y validado tantas veces como observaciones disponibles se tengan lo que en algunos casos puede ser muy complicado. sklearn contiene la función LeaveOneOut(). from sklearn.model_selection import LeaveOneOut, cross_val_score from sklearn.linear_model import LinearRegression y = ames &gt;&gt; pull(&quot;Sale_Price&quot;) ## Otra forma: ames[&quot;Sale_Price&quot;] X = select(ames, _.Gr_Liv_Area) # Crea el regresor lineal que deseas evaluar regressor = LinearRegression() # Crea el objeto Leave-One-Out Cross-Validation loo = LeaveOneOut() # Realiza la validación cruzada LOOCV y obtén los scores de cada iteración scores = cross_val_score( regressor, X, y, cv = loo, scoring=&#39;neg_mean_squared_error&#39;, error_score = &#39;raise&#39;) # Calcula el promedio y la desviación estándar de los scores mean_score = -scores.mean() std_score = scores.std() # Imprime los resultados print(&quot;Scores de cada iteración:&quot;, scores) ## Scores de cada iteración: [-2.80608203e+08 -7.01304898e+07 -1.05533389e+08 ... -1.07632629e+08 ## -2.45849621e+06 -2.37271777e+09] print(&quot;Promedio del score:&quot;, mean_score) ## Promedio del score: 3206707385.6871295 print(&quot;Desviación estándar del score:&quot;, std_score) ## Desviación estándar del score: 9431124835.27696 4.3.4 K Fold Cross Validation En la validación cruzada de K iteraciones (K Fold Cross Validation) los datos de muestra se dividen en K subconjuntos. Uno de los subconjuntos se utiliza como datos de prueba y el resto como datos de entrenamiento. El proceso de validación cruzada es repetido durante \\(k\\) iteraciones, con cada uno de los posibles subconjuntos de datos de prueba. Finalmente se obtiene el promedio de los rendimientos de cada iteración para obtener un único resultado. Lo más común es utilizar la validación cruzada de 10 iteraciones. Este método de validación cruzada se utiliza principalmente para: Estimar el error cuando nuestro conjunto de prueba es muy pequeño. Es decir, se tiene la misma configuración de parámetros y solamente cambia el conjunto de prueba y validación. Encontrar lo mejores hiperparámetros que ajusten mejor el modelo. Es decir, en cada bloque se tiene una configuración de hiperparámetros distinto y se seleccionará aquellos hiperparámetros que hayan producido el error más pequeño. En python, esto se logra mediante las siguiente función: from sklearn.model_selection import KFold # Crea el objeto K-Fold Cross-Validation con K=5 (puedes cambiar el valor de K según tus necesidades) kf = KFold(n_splits = 10, shuffle = True, random_state = 42) # Realiza la validación cruzada KFCV y obtén los scores de cada iteración scores = cross_val_score( regressor, X, y, cv = kf, scoring=&#39;neg_mean_squared_error&#39; ) # Calcula el promedio y la desviación estándar de los scores mean_score = -scores.mean() std_score = scores.std() # Imprime los resultados print(&quot;Scores de cada iteración:&quot;, scores) ## Scores de cada iteración: [-3.94519600e+09 -3.61280406e+09 -2.60703956e+09 -3.56455494e+09 ## -3.10455928e+09 -3.08042248e+09 -2.79329524e+09 -3.61684676e+09 ## -3.00764032e+09 -2.75338281e+09] print(&quot;Promedio del score:&quot;, mean_score) ## Promedio del score: 3208574145.632535 print(&quot;Desviación estándar del score:&quot;, std_score) ## Desviación estándar del score: 425269133.9629184 Éste método nos permite detectar el nivel de error para diferentes conjuntos de entrenamiento y validación. El modelo es el mismo, pero existen pequeñas perturbaciones en los datos que ayudan a estimar el promedio y desviación estándar del nivel de precisión del modelo. 4.3.5 Validación cruzada para series de tiempo En este procedimiento, hay una serie de conjuntos de prueba, cada uno de los cuales consta de una única observación. El conjunto de entrenamiento correspondiente consta solo de observaciones que ocurrieron antes de la observación que forma el conjunto de prueba. Por lo tanto, no se pueden utilizar observaciones futuras para construir el pronóstico. El siguiente diagrama ilustra la serie de conjuntos de entrenamiento y prueba, donde las observaciones azules forman los conjuntos de entrenamiento y las observaciones rojas forman los conjuntos de prueba. La precisión del pronóstico se calcula promediando los conjuntos de prueba. Este procedimiento a veces se conoce como “evaluación en un origen de pronóstico continuo” porque el “origen” en el que se basa el pronóstico avanza en el tiempo. Con los pronósticos de series de tiempo, los pronósticos de un paso pueden no ser tan relevantes como los pronósticos de varios pasos. En este caso, el procedimiento de validación cruzada basado en un origen de pronóstico continuo se puede modificar para permitir el uso de errores de varios pasos. Suponga que estamos interesados en modelos que producen buenos pronósticos de 4 pasos por delante. Entonces el diagrama correspondiente se muestra a continuación. ¡¡ I M P O R T A N T E !! El modo de entrenar un modelo debe hacerse imitando la manera en que se realizarán las predicciones posteriormente. Es decir: Si se pretenden hacer predicciones de eventos futuros (dentro de un mes), al momento de entrenar el modelo los datos deben corresponder al estatus en que se encontraban 1 mes antes de observar la variable de respuesta y la variable de respuesta debe ser el valor real que se deseaba predecir. 4.4 Feature engineering Hay varios pasos que se deben de seguir para crear un modelo útil: Recopilación de datos. Limpieza de datos. Creación de nuevas variables. Estimación de parámetros. Selección y ajuste del modelo. Evaluación del rendimiento. Al comienzo de un proyecto, generalmente hay un conjunto finito de datos disponibles para todas estas tareas. OJO: A medida que los datos se reutilizan para múltiples tareas, aumentan los riesgos de agregar sesgos o grandes efectos de errores metodológicos. Como punto de partida para nuestro flujo de trabajo de aprendizaje automático, necesitaremos datos de entrada. En la mayoría de los casos, estos datos se cargarán y almacenarán en forma de data frames. Incluirán una o varias variables predictivas y, en caso de aprendizaje supervisado, también incluirán un resultado conocido. Sin embargo, no todos los modelos pueden lidiar con diferentes problemas de datos y, a menudo, necesitamos transformar los datos para obtener el mejor rendimiento posible del modelo. Este proceso se denomina pre-procesamiento y puede incluir una amplia gama de pasos, como: Dicotomización de variables: Variables cualitativas que solo pueden tomar el valor \\(0\\) o \\(1\\) para indicar la ausencia o presencia de una condición específica. Estas variables se utilizan para clasificar los datos en categorías mutuamente excluyentes o para activar comandos de encendido / apagado Near Zero Value (nzv) o Varianza Cero: En algunas situaciones, el mecanismo de generación de datos puede crear predictores que solo tienen un valor único (es decir, un “predictor de varianza cercando a cero”). Para muchos modelos (excluidos los modelos basados en árboles), esto puede hacer que el modelo se bloquee o que el ajuste sea inestable. De manera similar, los predictores pueden tener solo una pequeña cantidad de valores únicos que ocurren con frecuencias muy bajas. Imputaciones: Si faltan algunos predictores, ¿deberían estimarse mediante imputación? Des-correlacionar: Si hay predictores correlacionados, ¿debería mitigarse esta correlación? Esto podría significar filtrar predictores, usar análisis de componentes principales o una técnica basada en modelos (por ejemplo, regularización). Normalizar: ¿Deben centrarse y escalar los predictores? Transformar: ¿Es útil transformar los predictores para que sean más simétricos? (por ejemplo, escala logarítmica). Dependiendo del caso de uso, algunos pasos de pre-procesamiento pueden ser indispensables para pasos posteriores, mientras que otros solo son opcionales. Sin embargo, dependiendo de los pasos de pre-procesamiento elegidos, el rendimiento del modelo puede cambiar significativamente en pasos posteriores. Por lo tanto, es muy común probar varias configuraciones. La ingeniería de datos abarca actividades que dan formato a los valores de los predictores para que se puedan utilizar de manera eficaz para nuestro modelo. Esto incluye transformaciones y codificaciones de los datos para representar mejor sus características importantes. Por ejemplo: 1.- Supongamos que un conjunto de datos tiene dos predictores que se pueden representar de manera más eficaz en nuestro modelo como una proporción, así, tendríamos un nuevo predictor a partir de la proporción de los dos predictores originales. X Proporción (X) 691 0.1836789 639 0.1698565 969 0.2575758 955 0.2538543 508 0.1350346 2.- Al elegir cómo codificar nuestros datos en el modelado, podríamos elegir una opción que creemos que está más asociada con el resultado. El formato original de los datos, por ejemplo numérico (edad) versus categórico (grupo). Edad Grupo 7 Niños 78 Adultos mayores 17 Adolescentes 25 Adultos 90 Adultos mayores La ingeniería y el pre-procesamiento de datos también pueden implicar el cambio de formato requerido por el modelo. Algunos modelos utilizan métricas de distancia geométrica y, en consecuencia, los predictores numéricos deben centrarse y escalar para que estén todos en las mismas unidades. De lo contrario, los valores de distancia estarían sesgados por la escala de cada columna. 4.5 Pipeline Se le conoce como pipeline, workflow o recipe a una serie de instrucciones para el procesamiento de datos. El pipeline define los pasos sin ejecutarlos inmediatamente; es sólo una especificación de lo que se debe hacer cada vez que se entrene el modelo. Este procesamiento de datos se realiza en cada conjunto de entrenamiento, ya sea 1 solo o varios como el caso de LOOCV o KFCV. Ventajas de usar un pipeline: Los cálculos se pueden reciclar entre modelos ya que no están estrechamente acoplados a la función de modelado. Una receta permite un conjunto más amplio de opciones de procesamiento de datos que las que pueden ofrecer las fórmulas. La sintaxis puede ser muy compacta. Todo el procesamiento de datos se puede capturar en un solo objeto en lugar de tener scripts que se repiten o incluso se distribuyen en diferentes archivos. La siguiente sección explica la estructura y flujo de transformaciones: from sklearn.pipeline import Pipeline # Crear el pipeline con las mismas transformaciones pipeline = Pipeline([ (&#39;transformer_1&#39;, transformer1), (&#39;transformer_2&#39;, transformer2), (&#39;transformer_3&#39;, transformer3), (&#39;transformer_4&#39;, transformer4), . . . (&#39;transformer_n_1&#39;, transformer_n_1), (&#39;transformer_n&#39;, transformer_n) ]) # Ajustar el pipeline al conjunto de entrenamiento pipeline.fit(X_train) # Aplicar el pipeline al conjunto de prueba X_test_transformed = pipeline.transform(X_test) Existe otra función que ayuda a la creación de un pipeline, su nombre es: make_pipeline. Tanto la función Pipeline como la función make_pipeline en scikit-learn son utilizadas para construir pipelines de procesamiento y modelado. Sin embargo, tienen algunas diferencias clave en cuanto a cómo se definen y gestionan los nombres de los pasos en el pipeline: Pipeline: Con la función Pipeline, debes proporcionar explícitamente un nombre para cada paso en el pipeline. Estos nombres son necesarios para acceder a los atributos específicos de cada paso y para referirse a ellos al realizar tareas como la búsqueda de hiperparámetros. Sintaxis: La sintaxis de la función Pipeline requiere que proporciones una lista de tuplas donde cada tupla consiste en un nombre de paso y una instancia del estimador o transformador correspondiente. from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC pipeline = Pipeline([ (&#39;scaler&#39;, StandardScaler()), (&#39;pca&#39;, PCA(n_components=2)), (&#39;svm&#39;, SVC()) ]) make_pipeline: Con la función make_pipeline, los nombres de los pasos se generan automáticamente utilizando los nombres de las clases en minúsculas. Esto significa que no tienes que proporcionar nombres explícitos para cada paso, lo que puede ser conveniente para flujos de trabajo simples donde los nombres no son críticos. Sintaxis: La función make_pipeline toma una serie de argumentos que son simplemente las instancias de los estimadores o transformadores que deseas encadenar en el pipeline. Los nombres de los pasos se generan automáticamente. from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC pipeline = make_pipeline( StandardScaler(), PCA(n_components=2), SVC() ) A continuación se muestran distintos ejemplos de transformaciones realizadas comúnmente en el pre-procesamiento de modelos predictivos. Como ejemplo, utilizaremos el subconjunto de predictores disponibles en los datos de vivienda: Ames En cuanto a las transformaciones posibles, existe una gran cantidad de funciones que soportan este proceso. En esta sección se muestran algunas de las transformación más comunes, entre ellas: Normalización Dicotomización Creación de nuevas columnas Datos faltantes Imputaciones Interacciones Etc. 4.5.1 Normalizar columnas numéricas Quizá la transformación numérica más usada en todos los modelos es la estandarización o normalización de variables numéricas. Este proceso se realiza para homologar la escala de las variables numéricas, de modo que no predomine una sobre otra debido a la diferencia de magnitudes o escalas. Este proceso se tiene de fondo el siguiente proceso estadístico: \\[Z=\\frac{X-\\hat{\\mu}_x}{\\hat{\\sigma}_x}\\] Donde: X = Es una variable o columna numérica \\(\\hat{\\mu}_x\\) = Es la estimación de la media de la variable X \\(\\hat{\\sigma}_x\\) = Es la estimación de la desviación estándar de la variable X from sklearn import preprocessing import numpy as np X_train = np.array([[ 1., -1., 5.], [ 0., 0., 10.], [ 1., -1., 5.], [ 0., 0., 10.], [ 1., -1., 5.], [ 0., 0., 10.], [ 1., -1., 5.], [ 0., 0., 10]]) scaler = preprocessing.StandardScaler().fit(X_train) scaler.mean_ ## array([ 0.5, -0.5, 7.5]) scaler.scale_ ## array([0.5, 0.5, 2.5]) X_scaled = scaler.transform(X_train) X_scaled ## array([[ 1., -1., -1.], ## [-1., 1., 1.], ## [ 1., -1., -1.], ## [-1., 1., 1.], ## [ 1., -1., -1.], ## [-1., 1., 1.], ## [ 1., -1., -1.], ## [-1., 1., 1.]]) Si deseamos ahora calcular la media y desviación de los datos ya escalados, obtenemos lo siguiente: X_scaled.mean(axis=0) ## array([0., 0., 0.]) X_scaled.std(axis=0) ## array([1., 1., 1.]) ¡¡ R E C O R D A R !! Los transformadores que se incluyen en el pipeline se ajustan al conjunto de entrenamiento utilizando el método .fit(), lo que permite que las transformaciones posteriores se calculen basándose en los datos de entrenamiento. Luego, aplicamos el mismo pipeline ajustado al conjunto de prueba utilizando el método .transform() para obtener los datos transformados de manera coherente. X_test = np.array([[ 0., 1., 1.], [ 2., 0., 1.], [ 0., 1., 1.]]) X_test_scaled = scaler.transform(X_test) X_test_scaled ## array([[-1. , 3. , -2.6], ## [ 3. , 1. , -2.6], ## [-1. , 3. , -2.6]]) 4.5.2 Dicotomización de categorías Otra transformación necesaria en la mayoría de los modelos predictivos es la creación de las variables dummy. Se mencionó anteriormente que los modelos requieren de una matriz numérica de características explicativas que permita calcular patrones estadísticos para predecir la variable de respuesta. El proceso de dicotomización consiste en crear una variable dicotómica por cada categoría de una columna con valores nominales. from sklearn.preprocessing import OneHotEncoder # Datos de entrenamiento X_train_cat = pd.DataFrame({ &quot;Cat&quot;: [&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;C&#39;, &#39;B&#39;, &#39;B&#39;] }) X_train_cat ## Cat ## 0 A ## 1 B ## 2 A ## 3 C ## 4 B ## 5 B # Crear una instancia de OneHotEncoder encoder = OneHotEncoder(drop=None, handle_unknown=&#39;ignore&#39;, sparse_output=False) # Ajustar y transformar el encoder en los datos de entrenamiento X_train_encoded = encoder.fit_transform(X_train_cat) X_train_encoded = pd.DataFrame(X_train_encoded, columns = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]) X_train_encoded ## A B C ## 0 1.0 0.0 0.0 ## 1 0.0 1.0 0.0 ## 2 1.0 0.0 0.0 ## 3 0.0 0.0 1.0 ## 4 0.0 1.0 0.0 ## 5 0.0 1.0 0.0 # Datos de prueba X_test_cat = pd.DataFrame({ &quot;Cat&quot;: [&#39;A&#39;, &#39;C&#39;, &#39;B&#39;] }) # Transformar los datos de prueba utilizando el encoder ajustado X_test_encoded = pd.DataFrame(encoder.transform(X_test_cat), columns = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]) X_test_encoded ## A B C ## 0 1.0 0.0 0.0 ## 1 0.0 0.0 1.0 ## 2 0.0 1.0 0.0 4.5.3 Imputación de datos faltantes La función SimpleImputer crea una imputación sobre los datos faltantes. Las imputaciones o sustituciones más comunes son realizadas a través de medidas de tendencia central tales como la media y mediana. Para poder usar una transformación distinta a cada una de las columnas vamos a hacer uso de una función auxiliar muuuuuuy importante. Su nombre es: ColumnTransformer. El ColumnTransformer es una clase en scikit-learn que permite aplicar diferentes transformaciones a diferentes columnas o conjuntos de columnas en los datos de manera independiente y luego combinar los resultados en un solo conjunto de datos. Es útil cuando tienes un conjunto de datos que contiene diferentes tipos de características (numéricas, categóricas, texto, etc.) y deseas aplicar diferentes transformaciones a cada tipo de característica de manera eficiente. Caso numérico: from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer # Datos de ejemplo con valores faltantes data = np.array([[1, 1, 3, np.nan], [2, 2, np.nan, 1], [1, np.nan, 6, 5], [2, 3, 6, 5], [np.nan, 4, 9, 0]]) data ## array([[ 1., 1., 3., nan], ## [ 2., 2., nan, 1.], ## [ 1., nan, 6., 5.], ## [ 2., 3., 6., 5.], ## [nan, 4., 9., 0.]]) # Definir las estrategias de imputación imputer_mean = SimpleImputer(strategy=&#39;mean&#39;) # Imputación con la media imputer_median = SimpleImputer(strategy=&#39;median&#39;) # Imputación con la mediana imputer_mode = SimpleImputer(strategy=&#39;most_frequent&#39;) # Imputación con la moda imputer_arbitrary = SimpleImputer(strategy=&#39;constant&#39;, fill_value=0) # Imputación constante # Crear un ColumnTransformer para aplicar diferentes estrategias a diferentes columnas column_transformer = ColumnTransformer(transformers=[ (&#39;mean_imputer&#39;, imputer_mean, [0]), (&#39;median_imputer&#39;, imputer_median, [1]), (&#39;mode_imputer&#39;, imputer_mode, [2]), (&#39;arbitrary_imputer&#39;, imputer_arbitrary, [3]) ]) # Crear el pipeline con el ColumnTransformer pipeline = Pipeline(steps=[ (&#39;column_transformer&#39;, column_transformer) ]) # Aplicar el pipeline a los datos imputed_data = pipeline.fit_transform(data) imputed_data ## array([[1. , 1. , 3. , 0. ], ## [2. , 2. , 6. , 1. ], ## [1. , 2.5, 6. , 5. ], ## [2. , 3. , 6. , 5. ], ## [1.5, 4. , 9. , 0. ]]) Caso categórico: import pandas as pd from sklearn.impute import SimpleImputer # Datos de ejemplo con valores faltantes data = pd.DataFrame({ &#39;Columna1&#39;: [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, np.nan, &#39;B&#39;], &#39;Columna2&#39;: [&#39;X&#39;, np.nan, &#39;Y&#39;, &#39;Z&#39;, &#39;Z&#39;] }) # Definir las estrategias de imputación imputer_most_frequent = SimpleImputer(strategy=&#39;most_frequent&#39;) imputer_constant = SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;Unknown&#39;) # Crear un ColumnTransformer para aplicar diferentes estrategias a diferentes columnas column_transformer = ColumnTransformer(transformers=[ (&#39;most_frequent_imputer&#39;, imputer_most_frequent, [&#39;Columna1&#39;]), (&#39;constant_imputer&#39;, imputer_constant, [&#39;Columna2&#39;]) ]) # Aplicar el ColumnTransformer a los datos imputed_data = column_transformer.fit_transform(data) imputed_data ## array([[&#39;A&#39;, &#39;X&#39;], ## [&#39;A&#39;, &#39;Unknown&#39;], ## [&#39;A&#39;, &#39;Y&#39;], ## [&#39;A&#39;, &#39;Z&#39;], ## [&#39;B&#39;, &#39;Z&#39;]], dtype=object) 4.5.4 Transformaciones personalizadas Podría pensarse en una infinidad de transformaciones a aplicar a los datos. Transformaciones de escala, de suavizamiento, estadísticas e incluso personalizadas. Existen métodos simples y métodos avanzados para realizar este proceso. A continuación, se muestra el método simple: import pandas as pd from sklearn.preprocessing import FunctionTransformer, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline # Datos de ejemplo en forma de DataFrame data = pd.DataFrame({ &#39;Column1&#39;: [-1, -5, 0, 3, 7], &#39;Column2&#39;: [ 2, 6, 10, -4, -8], &#39;Column3&#39;: [-3, 7, -11, 5, -9] }) # Definir una función personalizada que suma dos columnas y divide por una tercera def custom_function(X, c1, c2): X[&quot;c1_c2&quot;] = X[c1]/ X[c2] return X # Crear el transformador de función personalizada custom_transformer = FunctionTransformer( custom_function, feature_names_out = &#39;one-to-one&#39;, kw_args={&#39;c1&#39;: &#39;Column1&#39;, &#39;c2&#39;: &#39;Column2&#39;} ) # Crear un ColumnTransformer para aplicar el transformador personalizado a las columnas column_transformer = ColumnTransformer( transformers=[ (&#39;custom&#39;, custom_transformer, [&quot;Column1&quot;, &quot;Column2&quot;])], remainder=&#39;passthrough&#39;, verbose_feature_names_out = False ) transformed = pd.DataFrame( data = column_transformer.fit_transform(data), columns = [&quot;Column1&quot;, &quot;Column2&quot;, &quot;C1_over_c2&quot;, &quot;Column3&quot;] ) transformed ## Column1 Column2 C1_over_c2 Column3 ## 0 -1.0 2.0 -0.500000 -3.0 ## 1 -5.0 6.0 -0.833333 7.0 ## 2 0.0 10.0 0.000000 -11.0 ## 3 3.0 -4.0 -0.750000 5.0 ## 4 7.0 -8.0 -0.875000 -9.0 test = pd.DataFrame({ &#39;Column1&#39;: [-1, -5, 9, -3, -7], &#39;Column2&#39;: [25, 6, -10, 4, -8], &#39;Column3&#39;: [31, -7, 11, -5, 9] }) # Aplicar el pipeline a los datos de prueba X_test_transformed = column_transformer.transform(test) X_test_transformed ## array([[ -1. , 25. , -0.04 , 31. ], ## [ -5. , 6. , -0.83333333, -7. ], ## [ 9. , -10. , -0.9 , 11. ], ## [ -3. , 4. , -0.75 , -5. ], ## [ -7. , -8. , 0.875 , 9. ]]) 4.5.5 Interacciones Entre las transformaciones más útiles que pueden implementarse en el proceso de ingeniería de datos se encuentran las interacciones. Estas se refieren a las relaciones combinadas entre dos o más variables que pueden afectar a la variable objetivo de un modelo. En otras palabras, una interacción ocurre cuando el efecto de una característica en la variable objetivo depende de los valores combinados de otra(s) característica(s). import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from plotnine import * from mizani.formatters import comma_format, dollar_format ( ggplot(ames, aes(x = &quot;Gr_Liv_Area&quot;, y = &quot;Sale_Price&quot;) ) + geom_point(alpha = .2) + facet_wrap(&quot;Bldg_Type&quot;) + geom_smooth(method = &quot;lm&quot;, se = False, color = &quot;red&quot;, alpha = 0.1) + scale_x_log10(labels = comma_format()) + scale_y_log10(labels = dollar_format(prefix=&#39;$&#39;, digits=0, big_mark=&#39;,&#39;)) + labs( title = &quot;Relación entre precio y tamaño con tipo de vivienda&quot;, x = &quot;Gross Living Area&quot;, y = &quot;Sale Price (USD)&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; Las interacciones son útiles porque muchas veces las relaciones entre características y la variable objetivo no son lineales ni independientes. La presencia de interacciones puede capturar patrones más complejos y proporcionar una mejor representación de los datos, lo que lleva a modelos más precisos y ajustados. Las interacciones pueden mejorar la calidad de las predicciones y el entendimiento de los factores que influyen en el resultado deseado. from sklearn.preprocessing import PolynomialFeatures # Ejemplo de datos data = pd.DataFrame({ &#39;C1&#39;: [1, 0, 1, 0, -1, 2, -2, 5], &#39;C2&#39;: [1, 5, -5, 3, -1, 0.5, 1, 10] }) # Crear interacciones polinómicas interaction_transformer = PolynomialFeatures( degree = 2, interaction_only = True, include_bias = False ) # ColumnTransformer para aplicar transformaciones preprocessor = ColumnTransformer( transformers=[ (&#39;interactions&#39;, interaction_transformer, [&#39;C1&#39;, &#39;C2&#39;]) ], remainder=&#39;passthrough&#39; # Mantener las columnas restantes sin cambios ) # Ajustar el pipeline y transformar a los datos preprocessor.fit_transform(data) ## array([[ 1. , 1. , 1. ], ## [ 0. , 5. , 0. ], ## [ 1. , -5. , -5. ], ## [ 0. , 3. , 0. ], ## [-1. , -1. , 1. ], ## [ 2. , 0.5, 1. ], ## [-2. , 1. , -2. ], ## [ 5. , 10. , 50. ]]) test = pd.DataFrame({ &#39;C1&#39;: [1, 2, 3, -4, 5], &#39;C2&#39;: [5, -4, 3, -2, 1] }) # aplicar en el conjunto de prueba preprocessor.transform(test) ## array([[ 1., 5., 5.], ## [ 2., -4., -8.], ## [ 3., 3., 9.], ## [-4., -2., 8.], ## [ 5., 1., 5.]]) 4.5.6 Renombramiento de nuevos datos Para concluir el proceso de transformación de datos en un dataframe luego de haber sido procesados por un transformador de columnas, es importante obtener uno nuevo con sus respectivos nombres de columnas. Para lograrlo, se realiza lo siguiente: data = pd.DataFrame({ &#39;edad&#39;: [20, 30, 40], &#39;ingreso&#39;: [50000, 60000, 70000], &#39;sexo&#39;: [&#39;M&#39;, &#39;F&#39;, &#39;M&#39;], &#39;educacion&#39;: [&#39;secundaria&#39;, &#39;universidad&#39;, &#39;preparatoria&#39;] }) # ColumnTransformer para características numéricas y categóricas ct = ColumnTransformer( transformers=[ (&#39;num&#39;, StandardScaler(), [&#39;edad&#39;, &#39;ingreso&#39;]), (&#39;cat&#39;, OneHotEncoder(drop=&#39;first&#39;), [&#39;sexo&#39;, &#39;educacion&#39;])], verbose_feature_names_out = False ) # Ajuste y transformación en data transformed_data = ct.fit_transform(data) # Abtener los nombres de las características de salida del transformador new_column_names = ct.get_feature_names_out() # Crear un DataFrame con los datos transformados y los nuevos nombres de las columnas transformed_df = pd.DataFrame(transformed_data, columns=new_column_names) # Imprimir el DataFrame resultante print(transformed_df) ## edad ingreso sexo_M educacion_secundaria educacion_universidad ## 0 -1.224745 -1.224745 1.0 1.0 0.0 ## 1 0.000000 0.000000 0.0 0.0 1.0 ## 2 1.224745 1.224745 1.0 0.0 0.0 Revisar la documentación de los transformadores predefinidos en sklearn: link. "],["regresión-lineal.html", "Capítulo 5 Regresión Lineal 5.1 Regresión lineal simple 5.2 Regresión lineal múltiple 5.3 Ajuste de modelo 5.4 Residuos del modelo 5.5 Implementación con Python 5.6 Métricas de desempeño 5.7 Validación cruzada 5.8 Métodos se selección de variables 5.9 Ejercicio", " Capítulo 5 Regresión Lineal En esta sección aprenderemos sobre regresión lineal simple y múltiple, como se ajusta un modelo de regresión en python, las métricas de desempeño para problemas de regresión y como podemos comparar modelos con estas métricas. Existen dos tipos de modelos de regresión lineal: 5.1 Regresión lineal simple En la regresión lineal simple se utiliza una variable independiente o explicativa “X” (numérica o categórica) para estimar una variable dependiente o de respuesta numérica “Y” mediante el ajuste de una recta permita conocer la relación existente entre ambas variables. Dicha relación entre variables se expresa como: \\[Y = \\beta_0 + \\beta_1X_1 + \\epsilon \\approx b + mx\\] Donde: \\(\\epsilon \\sim Norm(0,\\sigma^2)\\) (error aleatorio) \\(\\beta_0\\) = Coeficiente de regresión 0 (Ordenada al origen o intercepto) \\(\\beta_1\\) = Coeficiente de regresión 1 (Pendiente o regresor de variable \\(X_1\\)) \\(X_1\\) = Variable explicativa observada \\(Y\\) = Respuesta numérica Debido a que los valores reales de \\(\\beta_0\\) y \\(\\beta_1\\) son desconocidos, procedemos a estimarlos estadísticamente: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1\\] Con \\(\\hat{\\beta}_0\\) el estimado de la ordenada al origen y \\(\\hat{\\beta}_1\\) el estimado de la pendiente. 5.1.1 Interpretación Una de las bondades de los modelos lineales es la interpretabilidad de los elementos que lo componen. Los coeficientes de regresión. Los coeficientes de regresión representan el cambio medio en la variable de respuesta para una unidad de cambio en la variable predictora. Este control estadístico que ofrece la regresión es importante, porque aísla el rol de una variable del resto de las variables incluidas en el modelo. La clave para entender los coeficientes es pensar en ellos como pendientes, y con frecuencia se les llama coeficientes de pendiente. Ilustremos lo anterior con el siguiente ejemplo de Didi: \\[\\begin{align} \\hat{Y} &amp;= \\hat{\\beta}_0 \\quad + \\hat{\\beta}_1X_1 \\\\ &amp; = 108.6 + 156X_1 \\end{align}\\] \\(\\hat{\\beta}_0\\) = Es el valor esperado en la variable de respuesta cuando \\(\\beta_1\\) es cero. \\(\\hat{\\beta}_1\\) = Es el cambio esperado en la variable de respuesta por cada unidad de cambio en \\(X_1\\). ¡¡ IMPORTANTE !! Al analizar la interpretación de los coeficientes de regresión es importante tomar en cuenta que esta interpretación se realiza sobre la estructura predictiva de un modelo y no sobre el fenómeno en sí mismo. 5.2 Regresión lineal múltiple Cuando se utiliza más de una variable independiente, el proceso se denomina regresión lineal múltiple. En este escenario no es una recta sino un hiper-plano lo que se ajusta a partir de las covariables explicativas \\(\\{X_1, X_2, X_3, ...,X_n\\}\\) El objetivo de un modelo de regresión múltiple es tratar de explicar la relación que existe entre una variable dependiente (variable respuesta) \\(&quot;Y&quot;\\) un conjunto de variables independientes (variables explicativas) \\(\\{X1,..., Xm\\}\\), el modelo es de la forma: \\[Y = \\beta_0 + \\beta_1X_1 + \\cdot \\cdot \\cdot + \\beta_mX_m + \\epsilon\\] Donde: \\(Y\\) como variable respuesta. \\(X_1,X_2,...,X_m\\) como las variables explicativas, independientes o regresoras. \\(\\beta_1, \\beta_2,...,\\beta_m\\) Se conocen como coeficientes parciales de regresión. Cada una de ellas puede interpretarse como el efecto promedio que tiene el incremento de una unidad de la variable predictora \\(X_i\\) sobre la variable dependiente \\(Y\\), manteniéndose constantes el resto de variables. 5.2.1 Interpretación De la misma manera en que se interpretan los coeficientes de regresión en el caso simple, también se interpretan los coeficientes en el caso múltiple. En el ejemplo anterior, se tienen 3 variables explicativas, las cuales son: Horas diarias: Se refiere al número de horas trabajadas por día. Interpretación: Por cada hora adicional de trabajo al día, en promedio aumenta \\(\\$97\\) el ingreso, manteniendo el resto de variables constantes. Viajes por hr: Es el número de viajes terminados por hora (en promedio). Interpretación: Por cada viaje adicional en una hora, en promedio se reduce el ingreso en \\(\\$6.08\\), manteniendo el resto de variables constantes. Turno: Es el turno en que se trabaja (mañana, tarde, noche). Al ser una variable categórica, cada categoría produce un efecto distinto sobre la variable de respuesta. Mañana (4:00am - 12:00pm): Cuando se trabaja en la mañana, se obtiene en promedio \\(\\$325.5\\) más ingresos, en comparación con trabajar en la tarde. Noche (8:00pm - 4:00am): Cuando se trabaja en la noche, se obtiene en promedio \\(\\$457.4\\) más ingresos, en comparación con trabajar en la tarde. Intercepto: Es el ingreso promedio cuando todas las variables son cero, en promedio y se trabaja en la tarde \\(-\\$245.145\\). 5.3 Ajuste de modelo 5.3.1 Estimación de parámetros: Regresión lineal simple En la gran mayoría de casos, los valores \\(\\beta_0\\) y \\(\\beta_1\\) poblacionales son desconocidos, por lo que, a partir de una muestra, se obtienen sus estimaciones \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\). Estas estimaciones se conocen como coeficientes de regresión o least square coefficient estimates, ya que toman aquellos valores que minimizan la suma de cuadrados residuales, dando lugar a la recta que pasa más cerca de todos los puntos. En términos analíticos, la expresión matemática a optimizar y solución están dadas por: \\[min(\\epsilon) \\Rightarrow min(y-\\hat{y}) = min\\{y -(\\hat{\\beta}_0 + \\hat{\\beta}_1x)\\}\\] \\[\\begin{aligned} \\hat{\\beta}_0 &amp;= \\overline{y} - \\hat{\\beta}_1\\overline{x} \\\\ \\hat{\\beta}_1 &amp;= \\frac{\\sum^n_{i=1}(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum^n_{i=1}(x_i - \\overline{x})^2} =\\frac{S_{xy}}{S^2_x} \\end{aligned}\\] Donde: \\(S_{xy}\\) es la covarianza entre \\(x\\) y \\(y\\). \\(S_{x}^{2}\\) es la varianza de \\(x\\). \\(\\hat{\\beta}_0\\) es el valor esperado la variable \\(Y\\) cuando \\(X = 0\\), es decir, la intersección de la recta con el eje y. 5.3.2 Estimación de parámetros: Regresión lineal múltiple En el caso de múltiples parámetros, la notación se vuelve más sencilla al expresar el modelo mediante una combinación lineal dada por la multiplicación de matrices (álgebra lineal). \\[Y = X\\beta + \\epsilon\\] Donde: \\[Y = \\begin{pmatrix}y_1\\\\y_2\\\\.\\\\.\\\\.\\\\y_n\\end{pmatrix} \\quad \\beta = \\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\.\\\\.\\\\.\\\\\\beta_m\\end{pmatrix} \\quad \\epsilon = \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\.\\\\.\\\\.\\\\\\epsilon_n\\end{pmatrix} \\quad \\quad X = \\begin{pmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1m}\\\\1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2m}\\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{nm}\\end{pmatrix}\\\\\\] El estimador por mínimos cuadrados está dado por: \\[\\hat{\\beta} = (X^TX)^{-1}X^TY\\] 5.4 Residuos del modelo El residuo de una estimación se define como la diferencia entre el valor observado y el valor esperado acorde al modelo. \\[\\epsilon_i= y_i -\\hat{y}_i\\] A la hora de contemplar el conjunto de residuos hay dos posibilidades: La suma del valor absoluto de cada residuo. \\[RAS=\\sum_{i=1}^{n}{|e_i|}=\\sum_{i=1}^{n}{|y_i-\\hat{y}_i|}\\] La suma del cuadrado de cada residuo (RSS). Esta es la aproximación más empleada (mínimos cuadrados) ya que magnifica las desviaciones más extremas. \\[RSS=\\sum_{i=1}^{n}{e_i^2}=\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)^2}\\] Los residuos son muy importantes puesto que en ellos se basan las diferentes métricas de desempeño del modelo. Condiciones para el ajuste de una regresión lineal: Existen ciertas condiciones o supuestos que deben ser validados para el correcto ajuste de un modelo de regresión lineal, los cuales se enlistan a continuación: Linealidad: La relación entre ambas variables debe ser lineal. Distribución normal de los residuos: Los residuos se tiene que distribuir de forma normal, con media igual a 0. Varianza de residuos constante (homocedasticidad): La varianza de los residuos tiene que ser aproximadamente constante. Independencia: Las observaciones deben ser independientes unas de otras. Dado que las condiciones se verifican a partir de los residuos, primero se suele generar el modelo y después se valida. 5.5 Implementación con Python Se realizará el ajuste del modelo utilizando los conceptos estudiados anteriormente. Se llevará a cabo la implementación simple y también usando el esquema de partición de muestra mediante KFCV. 5.5.1 Carga y partición de datos from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from plydata.one_table_verbs import pull from sklearn.model_selection import train_test_split from mizani.formatters import comma_format, dollar_format from plotnine import * from siuba import * import pandas as pd ames = pd.read_csv(&quot;data/ames.csv&quot;) ames_y = ames &gt;&gt; pull(&quot;Sale_Price&quot;) # ames[[&quot;Sale_Price&quot;]] ames_x = select(ames, -_.Sale_Price) # ames.drop(&#39;Sale_Price&#39;, axis=1) ames_x_train, ames_x_test, ames_y_train, ames_y_test = train_test_split( ames_x, ames_y, test_size = 0.20, random_state = 195 ) 5.5.2 Pipeline de transformación de datos # pip install mlxtend==0.23.0 from mlxtend.feature_selection import ColumnSelector # Seleccionamos las variales numéricas de interés num_cols = [&quot;Full_Bath&quot;, &quot;Half_Bath&quot;] # Seleccionamos las variables categóricas de interés cat_cols = [&quot;Overall_Cond&quot;] # Juntamos todas las variables de interés columnas_seleccionadas = num_cols + cat_cols pipe = ColumnSelector(columnas_seleccionadas) ames_x_train_selected = pipe.fit_transform(ames_x_train) ames_train_selected = pd.DataFrame( ames_x_train_selected, columns = columnas_seleccionadas ) ames_train_selected.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2344 entries, 0 to 2343 ## Data columns (total 3 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Full_Bath 2344 non-null object ## 1 Half_Bath 2344 non-null object ## 2 Overall_Cond 2344 non-null object ## dtypes: object(3) ## memory usage: 55.1+ KB # ColumnTransformer para aplicar transformaciones preprocessor = ColumnTransformer( transformers = [ (&#39;scaler&#39;, StandardScaler(), num_cols), (&#39;onehotencoding&#39;, OneHotEncoder(drop=&#39;first&#39;), cat_cols) ], verbose_feature_names_out = False, remainder = &#39;passthrough&#39; # Mantener las columnas restantes sin cambios ) transformed_data = preprocessor.fit_transform(ames_train_selected) new_column_names = preprocessor.get_feature_names_out() transformed_df = pd.DataFrame( transformed_data.todense(), columns=new_column_names ) transformed_df ## Full_Bath Half_Bath Overall_Cond_Average ... Overall_Cond_Poor \\ ## 0 0.780752 1.239868 1.0 ... 0.0 ## 1 0.780752 -0.749352 1.0 ... 0.0 ## 2 0.780752 -0.749352 1.0 ... 0.0 ## 3 -1.013446 -0.749352 1.0 ... 0.0 ## 4 -1.013446 1.239868 0.0 ... 0.0 ## ... ... ... ... ... ... ## 2339 -1.013446 1.239868 0.0 ... 0.0 ## 2340 0.780752 -0.749352 1.0 ... 0.0 ## 2341 -1.013446 -0.749352 1.0 ... 0.0 ## 2342 0.780752 1.239868 1.0 ... 0.0 ## 2343 0.780752 -0.749352 1.0 ... 0.0 ## ## Overall_Cond_Very_Good Overall_Cond_Very_Poor ## 0 0.0 0.0 ## 1 0.0 0.0 ## 2 0.0 0.0 ## 3 0.0 0.0 ## 4 0.0 0.0 ## ... ... ... ## 2339 0.0 0.0 ## 2340 0.0 0.0 ## 2341 0.0 0.0 ## 2342 0.0 0.0 ## 2343 0.0 0.0 ## ## [2344 rows x 10 columns] transformed_df.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2344 entries, 0 to 2343 ## Data columns (total 10 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Full_Bath 2344 non-null float64 ## 1 Half_Bath 2344 non-null float64 ## 2 Overall_Cond_Average 2344 non-null float64 ## 3 Overall_Cond_Below_Average 2344 non-null float64 ## 4 Overall_Cond_Excellent 2344 non-null float64 ## 5 Overall_Cond_Fair 2344 non-null float64 ## 6 Overall_Cond_Good 2344 non-null float64 ## 7 Overall_Cond_Poor 2344 non-null float64 ## 8 Overall_Cond_Very_Good 2344 non-null float64 ## 9 Overall_Cond_Very_Poor 2344 non-null float64 ## dtypes: float64(10) ## memory usage: 183.2 KB 5.5.3 Creación y ajuste de modelo # Crear el pipeline con la regresión lineal pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, LinearRegression()) ]) # Entrenar el pipeline results = pipeline.fit(ames_train_selected, ames_y_train) 5.5.4 Predicción con nuevos datos y_pred = pipeline.predict(ames_x_test) ames_test = ( ames_x_test &gt;&gt; mutate(Sale_Price_Pred = y_pred, Sale_Price = ames_y_test) ) ames_test.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## Index: 586 entries, 390 to 714 ## Data columns (total 75 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 MS_SubClass 586 non-null object ## 1 MS_Zoning 586 non-null object ## 2 Lot_Frontage 586 non-null int64 ## 3 Lot_Area 586 non-null int64 ## 4 Street 586 non-null object ## 5 Alley 586 non-null object ## 6 Lot_Shape 586 non-null object ## 7 Land_Contour 586 non-null object ## 8 Utilities 586 non-null object ## 9 Lot_Config 586 non-null object ## 10 Land_Slope 586 non-null object ## 11 Neighborhood 586 non-null object ## 12 Condition_1 586 non-null object ## 13 Condition_2 586 non-null object ## 14 Bldg_Type 586 non-null object ## 15 House_Style 586 non-null object ## 16 Overall_Cond 586 non-null object ## 17 Year_Built 586 non-null int64 ## 18 Year_Remod_Add 586 non-null int64 ## 19 Roof_Style 586 non-null object ## 20 Roof_Matl 586 non-null object ## 21 Exterior_1st 586 non-null object ## 22 Exterior_2nd 586 non-null object ## 23 Mas_Vnr_Type 220 non-null object ## 24 Mas_Vnr_Area 586 non-null int64 ## 25 Exter_Cond 586 non-null object ## 26 Foundation 586 non-null object ## 27 Bsmt_Cond 586 non-null object ## 28 Bsmt_Exposure 586 non-null object ## 29 BsmtFin_Type_1 586 non-null object ## 30 BsmtFin_SF_1 586 non-null int64 ## 31 BsmtFin_Type_2 586 non-null object ## 32 BsmtFin_SF_2 586 non-null int64 ## 33 Bsmt_Unf_SF 586 non-null int64 ## 34 Total_Bsmt_SF 586 non-null int64 ## 35 Heating 586 non-null object ## 36 Heating_QC 586 non-null object ## 37 Central_Air 586 non-null object ## 38 Electrical 586 non-null object ## 39 First_Flr_SF 586 non-null int64 ## 40 Second_Flr_SF 586 non-null int64 ## 41 Gr_Liv_Area 586 non-null int64 ## 42 Bsmt_Full_Bath 586 non-null int64 ## 43 Bsmt_Half_Bath 586 non-null int64 ## 44 Full_Bath 586 non-null int64 ## 45 Half_Bath 586 non-null int64 ## 46 Bedroom_AbvGr 586 non-null int64 ## 47 Kitchen_AbvGr 586 non-null int64 ## 48 TotRms_AbvGrd 586 non-null int64 ## 49 Functional 586 non-null object ## 50 Fireplaces 586 non-null int64 ## 51 Garage_Type 586 non-null object ## 52 Garage_Finish 586 non-null object ## 53 Garage_Cars 586 non-null int64 ## 54 Garage_Area 586 non-null int64 ## 55 Garage_Cond 586 non-null object ## 56 Paved_Drive 586 non-null object ## 57 Wood_Deck_SF 586 non-null int64 ## 58 Open_Porch_SF 586 non-null int64 ## 59 Enclosed_Porch 586 non-null int64 ## 60 Three_season_porch 586 non-null int64 ## 61 Screen_Porch 586 non-null int64 ## 62 Pool_Area 586 non-null int64 ## 63 Pool_QC 586 non-null object ## 64 Fence 586 non-null object ## 65 Misc_Feature 27 non-null object ## 66 Misc_Val 586 non-null int64 ## 67 Mo_Sold 586 non-null int64 ## 68 Year_Sold 586 non-null int64 ## 69 Sale_Type 586 non-null object ## 70 Sale_Condition 586 non-null object ## 71 Longitude 586 non-null float64 ## 72 Latitude 586 non-null float64 ## 73 Sale_Price_Pred 586 non-null float64 ## 74 Sale_Price 586 non-null int64 ## dtypes: float64(3), int64(32), object(40) ## memory usage: 347.9+ KB ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) ) ## Sale_Price Sale_Price_Pred ## 390 165000 210460.576539 ## 1235 124000 171815.687476 ## 2288 75000 171815.687476 ## 107 206000 120369.520306 ## 1861 190000 266134.974133 ## ... ... ... ## 116 171000 176043.917900 ## 398 120500 120369.520306 ## 1253 146000 210460.576539 ## 78 125000 143978.488679 ## 714 110000 154831.608941 ## ## [586 rows x 2 columns] 5.5.5 Extracción de coeficientes import statsmodels.api as sm X_train_with_intercept = sm.add_constant(transformed_df) model = sm.OLS(ames_y_train, X_train_with_intercept).fit() model.summary() ## &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; ## &quot;&quot;&quot; ## OLS Regression Results ## ============================================================================== ## Dep. Variable: y R-squared: 0.389 ## Model: OLS Adj. R-squared: 0.386 ## Method: Least Squares F-statistic: 148.4 ## Date: Sat, 11 Nov 2023 Prob (F-statistic): 6.92e-241 ## Time: 02:23:00 Log-Likelihood: -29200. ## No. Observations: 2344 AIC: 5.842e+04 ## Df Residuals: 2333 BIC: 5.848e+04 ## Df Model: 10 ## Covariance Type: nonrobust ## ============================================================================================== ## coef std err t P&gt;|t| [0.025 0.975] ## ---------------------------------------------------------------------------------------------- ## const 1.684e+05 3072.078 54.819 0.000 1.62e+05 1.74e+05 ## Full_Bath 3.705e+04 1395.861 26.546 0.000 3.43e+04 3.98e+04 ## Half_Bath 1.399e+04 1318.277 10.615 0.000 1.14e+04 1.66e+04 ## Overall_Cond_Average 2.361e+04 3626.732 6.510 0.000 1.65e+04 3.07e+04 ## Overall_Cond_Below_Average -2.898e+04 7559.385 -3.833 0.000 -4.38e+04 -1.42e+04 ## Overall_Cond_Excellent 2.06e+04 1.11e+04 1.851 0.064 -1223.878 4.24e+04 ## Overall_Cond_Fair -5.408e+04 9776.596 -5.531 0.000 -7.32e+04 -3.49e+04 ## Overall_Cond_Good 6624.8898 4624.560 1.433 0.152 -2443.787 1.57e+04 ## Overall_Cond_Poor -7.097e+04 2.1e+04 -3.379 0.001 -1.12e+05 -2.98e+04 ## Overall_Cond_Very_Good 5766.3211 6581.572 0.876 0.381 -7140.019 1.87e+04 ## Overall_Cond_Very_Poor -9.785e+04 2.57e+04 -3.815 0.000 -1.48e+05 -4.76e+04 ## ============================================================================== ## Omnibus: 945.817 Durbin-Watson: 1.950 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 5981.229 ## Skew: 1.783 Prob(JB): 0.00 ## Kurtosis: 9.966 Cond. No. 23.9 ## ============================================================================== ## ## Notes: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ## &quot;&quot;&quot; IMPORTANTE: Es necesario entender que para cada uno de los coeficientes de regresión se realiza una prueba de hipótesis. Una vez calculado el valor estimado, se procede a determinar si este valor es significativamente distinto de cero, por lo que la hipótesis de cada coeficiente se plantea de la siguiente manera: \\[H_0:\\beta_i=0 \\quad Vs \\quad H_1:\\beta_i\\neq0\\] El software R nos devuelve el p-value asociado a cada coeficiente de regresión. Recordemos que valores pequeños de p sugieren que al rechazar \\(H_0\\), la probabilidad de equivocarnos es baja, por lo que procedemos a rechazar la hipótesis nula. 5.6 Métricas de desempeño Dado que nuestra variable a predecir es numérica, podemos medir qué tan cerca o lejos estuvimos del número esperado dada una predicción. Las métricas de desempeño asociadas a los problemas de regresión ocupan esa distancia cómo cuantificación del desempeño o de los errores cometidos por el modelo. Las métricas más utilizadas son: MEA: Mean Absolute Error MAPE: Mean Absolute Percentual Error \\(\\quad \\Rightarrow \\quad\\) más usada para reportar resultados RMSE: Root Mean Squared Error \\(\\quad \\quad \\quad \\Rightarrow \\quad\\) más usada para entrenar modelos \\(R^2\\) : R cuadrada \\(R^2\\) : \\(R^2\\) ajustada \\(\\quad \\quad \\quad \\quad \\quad \\quad \\Rightarrow \\quad\\) usada para conocer potencial de mejora MAE: Mean Absolute Error \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}{|y_{i}-\\hat{y}_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica suma los errores absolutos de cada predicción y los divide entre el número de observaciones, para obtener el promedio absoluto del error del modelo. Ventajas Vs Desventajas: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es muy sensible a valores atípicos, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando los errores importan lo mismo, es decir, importa lo mismo si se equivocó muy poco o se equivocó mucho. MAPE: Mean Absolute Percentage Error \\[MAPE = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{{|y_{i}-\\hat{y}_{i}|}}{|y_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es la métrica MAE expresada en porcentaje, por lo que mide el error del modelo en términos de porcentaje, al igual que con MAE, no hay errores negativos por el valor absoluto, y mientras más pequeño el error es mejor. Ventajas Vs Desventajas: Cuando existe un valor real de 0 esta métrica no se puede calcular, por otro lado, una de las ventajas sobre MAE es que no es sensible a valores atípicos. Se recomienda utilizar esta métrica cuando en tu problema no haya valores a predecir que puedan ser 0, por ejemplo, en ventas puedes llegar a tener 0 ventas, en este caso no podemos ocupar esta métrica. En general a las personas de negocio les gusta esta métrica pues es fácil de comprender. RMSE: Root Mean Squared Error \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}{(y_{i}-\\hat{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es muy parecida a MAE, solo que en lugar de sacar el valor absoluto de la diferencia entre el valor real y el valor predicho, para evitar valores negativos eleva esta diferencia al cuadrado, y saca el promedio de esa diferencia, al final, para dejar el valor en la escala inicial saca la raíz cuadrada. Esta es la métrica más utilizada en problemas de regresión, debido a que es más fácil de optimizar que el MAE. Ventajas Vs Desventaja: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es más sensible a valores atípicos que MAE pues eleva al cuadrado diferencias, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando en el problema que queremos resolver es muy costoso tener equivocaciones grandes, podemos tener varios errores pequeños, pero no grandes. \\(R^2\\): R cuadrada \\[R^{2} = \\frac{\\sum_{i=1}^{N}{(\\hat{y}_{i}-\\bar{y}_{i})^2}}{\\sum_{i=1}^{N}{(y_{i}-\\bar{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. \\(\\bar{y}_{i}:\\) Valor promedio de la variable y. El coeficiente de determinación es la proporción de la varianza total de la variable explicada por la regresión. El coeficiente de determinación, también llamado R cuadrado, refleja la bondad del ajuste de un modelo a la variable que pretender explicar. Es importante saber que el resultado del coeficiente de determinación oscila entre 0 y 1. Cuanto más cerca de 1 se sitúe su valor, mayor será el ajuste del modelo a la variable que estamos intentando explicar. De forma inversa, cuanto más cerca de cero, menos ajustado estará el modelo y, por tanto, menos fiable será. Ventajas Vs Desventaja: El problema del coeficiente de determinación, y razón por el cual surge el coeficiente de determinación ajustado, radica en que no penaliza la inclusión de variables explicativas no significativas, es decir, el valor de \\(R^2\\) siempre será más grande cuantas más variables sean incluidas en el modelo, aún cuando estas no sean significativas en la predicción. \\(\\bar{R}^2\\): \\(R^2\\) ajustada \\[\\bar{R}^2=1-\\frac{N-1}{N-k-1}[1-R^2]\\] Donde: \\(\\bar{R}²:\\) Es el valor de R² ajustado \\(R²:\\) Es el valor de R² original \\(N:\\) Es el total de observaciones en el ajuste \\(k:\\) Es el número de variables usadas en el modelo El coeficiente de determinación ajustado (R cuadrado ajustado) es la medida que define el porcentaje explicado por la varianza de la regresión en relación con la varianza de la variable explicada. Es decir, lo mismo que el R cuadrado, pero con una diferencia: El coeficiente de determinación ajustado penaliza la inclusión de variables. En la fórmula, N es el tamaño de la muestra y k el número de variables explicativas. 5.6.1 Implementación con python from siuba import * from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error from sklearn.metrics import mean_squared_error, r2_score pd.options.display.float_format = &#39;{:.2f}&#39;.format y_obs = ames_test[&quot;Sale_Price&quot;] y_pred = ames_test[&quot;Sale_Price_Pred&quot;] me = np.mean(y_obs - y_pred) mae = mean_absolute_error(y_obs, y_pred) mape = mean_absolute_percentage_error(y_obs, y_pred) mse = mean_squared_error(y_obs, y_pred) rmse = np.sqrt(mse) r2 = r2_score(y_obs, y_pred) n = len(y_obs) # Número de observaciones p = 9 # Número de predictores r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2) metrics_data = { &quot;Metric&quot;: [&quot;ME&quot;, &quot;MAE&quot;, &quot;MAPE&quot;, &quot;MSE&quot;, &quot;RMSE&quot;, &quot;R^2&quot;, &quot;R^2 Adj&quot;], &quot;Value&quot;: [me, mae, mape, mse, rmse, r2, r2_adj] } metrics_df = pd.DataFrame(metrics_data) metrics_df ## Metric Value ## 0 ME -935.44 ## 1 MAE 45776.78 ## 2 MAPE 0.26 ## 3 MSE 4365064775.42 ## 4 RMSE 66068.64 ## 5 R^2 0.34 ## 6 R^2 Adj 0.33 ( ames_test &gt;&gt; ggplot(aes(x = &quot;Sale_Price_Pred&quot;, y = &quot;Sale_Price&quot;)) + geom_point() + scale_y_continuous(labels = dollar_format(digits=0, big_mark=&#39;,&#39;), limits = [0, 600000] ) + scale_x_continuous(labels = dollar_format(digits=0, big_mark=&#39;,&#39;), limits = [0, 500000] ) + geom_abline(color = &quot;red&quot;) + coord_equal() + labs( title = &quot;Comparación entre predicción y observación&quot;, x = &quot;Predicción&quot;, y = &quot;Observación&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(x = &quot;error&quot;)) + geom_histogram(color = &quot;white&quot;, fill = &quot;black&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + scale_x_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + ylab(&quot;Conteos de clase&quot;) + xlab(&quot;Errores&quot;) + ggtitle(&quot;Distribución de error&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(sample = &quot;error&quot;)) + geom_qq(alpha = 0.3) + stat_qq_line(color = &quot;red&quot;) + scale_y_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits = 0)) + xlab(&quot;Distribución normal&quot;) + ylab(&quot;Distribución de errores&quot;) + ggtitle(&quot;QQ-Plot&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(x = &quot;Sale_Price&quot;)) + geom_linerange(aes(ymin = 0, ymax = &quot;error&quot;), colour = &quot;purple&quot;) + geom_point(aes(y = &quot;error&quot;), size = 0.05, alpha = 0.5) + geom_abline(intercept = 0, slope = 0) + scale_x_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + scale_y_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + xlab(&quot;Precio real&quot;) + ylab(&quot;Error de estimación&quot;) + ggtitle(&quot;Relación entre error y precio de venta&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; 5.7 Validación cruzada Un apaso fundamental al momento de crear modelos de ML es conocer la volatilidad en el desempeño. Queremos conocer cuánto suele variar el desepeño cuando el modelo presenta perturbaciones en los datos a lo largo del tiempo. El esquema de validación cruzada permite usar datos de prueba distintos en cada iteración, de tal manera que es posible conocer la variación en el desempeño. from sklearn.model_selection import KFold, cross_val_score from sklearn.metrics import make_scorer from sklearn.model_selection import cross_validate # Definir el objeto K-Fold Cross Validator kf = KFold(n_splits=10, shuffle=True, random_state=42) # Definir las métricas de desempeño que deseas calcular como funciones de puntuación scoring = { &#39;neg_mean_squared_error&#39;: make_scorer(mean_squared_error, greater_is_better=False), &#39;r2&#39;: make_scorer(r2_score), &#39;neg_mean_absolute_error&#39;: make_scorer(mean_absolute_error, greater_is_better=False), &#39;mape&#39;: make_scorer(mean_absolute_percentage_error, greater_is_better=False) } # Realizar la validación cruzada y calcular métricas de desempeño utilizando cross_val_score results = cross_validate( pipeline, ames_train_selected, ames_y_train, cv=kf, scoring=scoring ) # Calcular estadísticas resumidas (media y desviación estándar) de las métricas mean_rmse = np.mean(np.sqrt(-results[&#39;test_neg_mean_squared_error&#39;])) std_rmse = np.std(np.sqrt(-results[&#39;test_neg_mean_squared_error&#39;])) mean_r2 = np.mean(results[&#39;test_r2&#39;]) std_r2 = np.std(results[&#39;test_r2&#39;]) mean_mae = np.mean(-results[&#39;test_neg_mean_absolute_error&#39;]) std_mae = np.std(-results[&#39;test_neg_mean_absolute_error&#39;]) mean_mape = np.mean(-results[&#39;test_mape&#39;]) std_mape = np.std(-results[&#39;test_mape&#39;]) ## MAE: 42958.97843878972 +/- 2186.6691878738275 ## MAPE: 0.2533426638359547 +/- 0.01450262585260906 ## R^2: 0.3832313279985248 +/- 0.04081274336501242 ## RMSE: 62294.56380451259 +/- 4103.70714462239 5.8 Métodos se selección de variables Una de las preguntas clave a responder es: ¿Cómo selecciono las variables a usar en un modelo?. Existen muchas técnicas para ello. Incluso, existen modelos que se encargan de realizar esta tarea de modo automático. Analizaremos diferentes técnicas a lo largo del curso. 5.8.1 Forward selection (selección hacia adelante) Comienza sin predictores en el modelo, agrega iterativamente los predictores más contribuyentes y se detiene cuando la mejora del modelo ya no es estadísticamente significativa. 5.8.2 Backward selection (selección hacia atrás) Comienza con todos los predictores en el modelo (modelo completo), y elimina iterativamente los predictores menos contribuyentes y se detiene cuando tiene un modelo en el que todos los predictores son estadísticamente significativos. 5.9 Ejercicio Para reforzar el aprendizaje y desarrollar las habilidades de modelado, el alumno deberá: Proponer un conjunto de variables iniciales para construir un modelo lineal Construir un pipeline (extenso) para el feature engineering Crear modelo lineal Calcular métricas de desempeño Iterativamente mejorar el ajuste del modelo Interpretar resultados (coeficientes de regresión y resultados) Realizar gráficas de bondad de ajuste Agregar ejercicio final al reporte "],["regresión-logística.html", "Capítulo 6 Regresión Logística 6.1 Función sigmoide 6.2 Ajuste del modelo 6.3 Clasificación 6.4 Implementación en python 6.5 Métricas de desempeño 6.6 Estimación de probabilidades 6.7 Validación cruzada", " Capítulo 6 Regresión Logística El nombre de este modelo es: Regresión Bernoulli con liga logit, pero todo mundo la conoce solo por regresión logística. Es importante saber que la liga puede ser elegida dentro de un conjunto de ligas comunes, por lo que puede dejar de ser logit y seguir siendo regresión Bernoulli, pero ya no podría ser llamada “logística”. Al igual que en regresión lineal, existe la regresión simple y regresión múltiple. La regresión logística simple se utiliza una variable independiente, mientras que cuando se utiliza más de una variable independiente, el proceso se denomina regresión logística múltiple. Objetivo: Estimar la probabilidad de pertenecer a la categoría positiva de una variable de respuesta categórica. Posteriormente, se determina el umbral de probabilidad a partir del cual se clasifica a una observación como positiva o negativa. 6.1 Función sigmoide Si una variable cualitativa con dos categorías se codifica como 1 y 0, matemáticamente es posible ajustar un modelo de regresión lineal por mínimos cuadrados. El problema de esta aproximación es que, al tratarse de una recta, para valores extremos del predictor, se obtienen valores de \\(Y\\) menores que 0 o mayores que 1, lo que entra en contradicción con el hecho de que las probabilidades siempre están dentro del rango [0,1]. Para evitar estos problemas, la regresión logística transforma el valor devuelto por la regresión lineal empleando una función cuyo resultado está siempre comprendido entre 0 y 1. Existen varias funciones que cumplen esta descripción, una de las más utilizadas es la función logística (también conocida como función sigmoide): \\[\\sigma(Z)=\\frac{e^{Z}}{1+e^{Z}}\\] Función sigmoide: Para valores de \\(Z\\) muy grandes, el valor de \\(e^{Z}\\) tiende a infinito por lo que el valor de la función sigmoide es 1. Para valores de \\(Z\\) muy negativos, el valor \\(e^{Z}\\) tiende a cero, por lo que el valor de la función sigmoide es 0. Sustituyendo la \\(Z\\) de la función sigmoide por la función lineal \\(\\beta_0+\\beta_1X\\) se obtiene que: \\[\\pi=P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\\] donde \\(P(Y=k|X=x)\\) puede interpretarse como: la probabilidad de que la variable cualitativa \\(Y\\) adquiera el valor \\(k\\), dado que el predictor \\(X\\) tiene el valor \\(x\\). 6.2 Ajuste del modelo Esta función, puede ajustarse de forma sencilla con métodos de regresión lineal si se emplea su versión logarítmica: \\[logit(\\pi)= ln(\\frac{\\pi}{1-\\pi}) = ln(\\frac{p(Y=k|X=x)}{1−p(Y=k|X=x)})=\\beta_0+\\beta_1X\\] \\[P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}\\] La combinación óptima de coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) será aquella que tenga la máxima verosimilitud (maximum likelihood), es decir el valor de los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) con los que se maximiza la probabilidad de obtener los datos observados. El método de maximum likelihood está ampliamente extendido en la estadística aunque su implementación no siempre es trivial. Otra forma para ajustar un modelo de regresión logística es empleando descenso de gradiente. Si bien este no es el método de optimización más adecuado para resolver la regresión logística, está muy extendido en el ámbito de machine learning para ajustar otros modelos. 6.3 Clasificación Una de las principales aplicaciones de un modelo de regresión logística es clasificar la variable cualitativa en función de valor que tome el predictor. Para conseguir esta clasificación, es necesario establecer un threshold de probabilidad a partir de la cual se considera que la variable pertenece a uno de los niveles. Por ejemplo, se puede asignar una observación al grupo 1 si \\(p̂ (Y=1|X)&gt;0.3\\) y al grupo 0 si ocurre lo contrario. Es importante mencionar que el punto de corte no necesariamente tiene que ser 0.5, este puede ser seleccionado a conveniencia de la métrica a optimizar. 6.4 Implementación en python Ajustaremos un modelo de regresión logística usando la receta antes vista. El primer paso es cargar las librerías necesarias y los datos a explotar. Lectura de datos from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression import pandas as pd from siuba import * from plydata.one_table_verbs import pull pd.set_option(&#39;display.max_columns&#39;, 4) telco = pd.read_csv(&quot;data/Churn.csv&quot;) telco.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 7043 entries, 0 to 7042 ## Data columns (total 21 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 customerID 7043 non-null object ## 1 gender 7043 non-null object ## 2 SeniorCitizen 7043 non-null int64 ## 3 Partner 7043 non-null object ## 4 Dependents 7043 non-null object ## 5 tenure 7043 non-null int64 ## 6 PhoneService 7043 non-null object ## 7 MultipleLines 7043 non-null object ## 8 InternetService 7043 non-null object ## 9 OnlineSecurity 7043 non-null object ## 10 OnlineBackup 7043 non-null object ## 11 DeviceProtection 7043 non-null object ## 12 TechSupport 7043 non-null object ## 13 StreamingTV 7043 non-null object ## 14 StreamingMovies 7043 non-null object ## 15 Contract 7043 non-null object ## 16 PaperlessBilling 7043 non-null object ## 17 PaymentMethod 7043 non-null object ## 18 MonthlyCharges 7043 non-null float64 ## 19 TotalCharges 7043 non-null object ## 20 Churn 7043 non-null object ## dtypes: float64(1), int64(2), object(18) ## memory usage: 1.1+ MB Se separa el conjunto de datos completo, comenzando por la variable de respuesta y los explicativos. Posteriormente, se crean los conjuntos de training &amp; testing. Segmentación de datos telco_y = telco &gt;&gt; pull(&quot;Churn&quot;) # telco[[&quot;Churn&quot;]] telco_x = select(telco, -_.Churn, -_.customerID) # telco.drop(&#39;Churn&#39;, axis=1) telco_x_train, telco_x_test, telco_y_train, telco_y_test = train_test_split( telco_x, telco_y, train_size = 0.80, random_state = 195, stratify = telco_y ) 6.4.1 Pipeline de transformación de datos La selección de variables y la ingeniería de características se lleva a cabo mediante el pipeline de transformación. # pip install mlxtend==0.23.0 from mlxtend.feature_selection import ColumnSelector # Seleccionamos las variales numéricas de interés num_cols = [&quot;MonthlyCharges&quot;] # Seleccionamos las variables categóricas de interés cat_cols = [&quot;PaymentMethod&quot;, &quot;Dependents&quot;] # Juntamos todas las variables de interés columnas_seleccionadas = num_cols + cat_cols pipe = ColumnSelector(columnas_seleccionadas) telco_x_train_selected = pipe.fit_transform(telco_x_train) telco_train_selected = pd.DataFrame( telco_x_train_selected, columns = columnas_seleccionadas ) telco_train_selected.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 5634 entries, 0 to 5633 ## Data columns (total 3 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 MonthlyCharges 5634 non-null object ## 1 PaymentMethod 5634 non-null object ## 2 Dependents 5634 non-null object ## dtypes: object(3) ## memory usage: 132.2+ KB # ColumnTransformer para aplicar transformaciones preprocessor = ColumnTransformer( transformers = [ (&#39;scaler&#39;, StandardScaler(), num_cols), (&#39;onehotencoding&#39;, OneHotEncoder(drop=&#39;first&#39;, sparse_output=False), cat_cols) ], verbose_feature_names_out = False, remainder = &#39;passthrough&#39; # Mantener las columnas restantes sin cambios ) transformed_data = preprocessor.fit_transform(telco_train_selected) new_column_ntelco = preprocessor.get_feature_names_out() transformed_df = pd.DataFrame( transformed_data, columns=new_column_ntelco ) transformed_df ## MonthlyCharges PaymentMethod_Credit card (automatic) ... \\ ## 0 0.49 0.00 ... ## 1 -1.47 0.00 ... ## 2 -0.16 0.00 ... ## 3 0.44 1.00 ... ## 4 -0.03 1.00 ... ## ... ... ... ... ## 5629 -1.48 0.00 ... ## 5630 0.74 0.00 ... ## 5631 -1.00 0.00 ... ## 5632 0.56 0.00 ... ## 5633 -1.48 1.00 ... ## ## PaymentMethod_Mailed check Dependents_Yes ## 0 0.00 1.00 ## 1 1.00 1.00 ## 2 0.00 1.00 ## 3 0.00 1.00 ## 4 0.00 0.00 ## ... ... ... ## 5629 1.00 1.00 ## 5630 0.00 1.00 ## 5631 1.00 0.00 ## 5632 0.00 0.00 ## 5633 0.00 0.00 ## ## [5634 rows x 5 columns] transformed_df.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 5634 entries, 0 to 5633 ## Data columns (total 5 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 MonthlyCharges 5634 non-null float64 ## 1 PaymentMethod_Credit card (automatic) 5634 non-null float64 ## 2 PaymentMethod_Electronic check 5634 non-null float64 ## 3 PaymentMethod_Mailed check 5634 non-null float64 ## 4 Dependents_Yes 5634 non-null float64 ## dtypes: float64(5) ## memory usage: 220.2 KB 6.4.2 Creación y ajuste de modelo En este curso se comenzará con una regresión logística sin parametrización, tal como en el caso de la regresión lineal, sin embargo, posteriormente será posible agregar parámetros que permitirán crear mejores modelos a cambio de complejidad adicional. # Crear el pipeline con la regresión logit pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, LogisticRegression()) ]) # Entrenar el pipeline results = pipeline.fit(telco_train_selected, telco_y_train) 6.4.3 Predicción con nuevos datos Una vez creado el modelo, se procede a hacer predicciones. Estas predicciones deben hacerse sobre datos que el modelo no haya usado para ser creado, de lo contrario, se incurrirá en overfitting. y_pred = pipeline.predict(telco_x_test) telco_test = ( telco_x_test &gt;&gt; mutate(Churn_Pred = y_pred, Churn = telco_y_test) ) ( telco_test &gt;&gt; select(_.Churn, _.Churn_Pred) ) ## Churn Churn_Pred ## 5058 No No ## 2466 No No ## 1740 No No ## 2977 No No ## 1726 No No ## ... ... ... ## 4068 No No ## 4648 No No ## 3996 No No ## 4076 No No ## 2031 No No ## ## [1409 rows x 2 columns] 6.5 Métricas de desempeño Existen distintas métricas de desempeño para problemas de clasificación, debido a que contamos con la respuesta correcta podemos contar cuántos aciertos tuvimos y cuántos fallos tuvimos. Primero, por simplicidad ocuparemos un ejemplo de clasificación binaria, Cancelación (1) o No Cancelación (0). En este tipo de algoritmos definimos cuál de las categorías será nuestra etiqueta positiva y cuál será la negativa. La positiva será la categoría que queremos predecir -en nuestro ejemplo, Cancelación- y la negativa lo opuesto -en el caso binario- en nuestro ejemplo, no cancelación. Dadas estas definiciones tenemos 4 posibilidades: True positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que es fraude. False positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que no es fraude. True negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que no es fraude. False negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que es fraude. Matriz de confusión Esta métrica corresponde a una matriz en donde se plasma el conteo de los aciertos y los errores que haya hecho el modelo, esto es: los verdaderos positivos (TP), los verdaderos negativos (TN), los falsos positivos (FP) y los falsos negativos (FN). Normalmente los renglones representan las etiquetas predichas, ya sean positivas o negativas, y las columnas a las etiquetas reales, aunque esto puede cambiar en cualquier software. Accuracy Número de aciertos totales entre todas las predicciones. \\[accuracy = \\frac{TP + TN}{ TP+FP+TN+FN}\\] La métrica más utilizada, en conjuntos de datos no balanceados esta métrica no nos sirve, al contrario, nos engaña. Adicionalmente, cuando la identificación de una categoría es más importante que otra es mejor recurrir a otras métricas. Precision: Eficiencia De los que identificamos como clase positiva, cuántos identificamos correctamente. ¿Qué tan eficientes somos en la predicción? \\[precision = \\frac{TP}{TP + FP}\\] ¿Cuándo utilizar precisión? Esta es la métrica que ocuparás más, pues en un contexto de negocio, donde los recursos son finitos y tiene un costo asociado, ya sea monetario o de tiempo o de recursos, necesitarás que las predicciones de tu etiqueta positiva sean muy eficientes. Al utilizar esta métrica estaremos optimizando el modelo para minimizar el número de falsos positivos. Recall o Sensibilidad: Cobertura Del universo posible de nuestra clase positiva, cuántos identificamos correctamente. \\[recall = \\frac{TP}{TP + FN }\\] Esta métrica la ocuparás cuando en el contexto de negocio de tu problema sea más conveniente aumentar la cantidad de positivos o disminuir los falsos negativos. Esto se realiza debido al impacto que estos pueden tener en las personas en quienes se implementará la predicción. Especificidad Es el número de observaciones correctamente identificados como negativos fuera del total de negativos. \\[Specificity = \\frac{TN}{TN+FP}\\] F1-score Combina precision y recall para optimizar ambos. \\[F = 2 *\\frac{precision * recall}{precision + recall} \\] Se recomienda utilizar esta métrica de desempeño cuando quieres balancear tanto los falsos positivos como los falsos negativos. Aunque es una buena solución para tomar en cuenta ambos errores, pocas veces hay problemas reales que permiten ocuparla, esto es porque en más del 90% de los casos tenemos una restricción en recursos. Algo importante a tomar en cuenta a la hora de calcular las métricas de desempeño, es que estas deben hacerse sobre el conjunta de prueba. from sklearn.metrics import confusion_matrix matriz_confusion = confusion_matrix(telco_y_test, y_pred) matriz_confusion ## array([[932, 103], ## [276, 98]]) si queremos apreciar de una manera más legible y elegante la salida de una matriz de confusión, puede lograrse mediante el apoyo de la librería seaborn, la cual permite crear múltiples gráficos de manera simple. import seaborn as sns import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) # Crear un DataFrame a partir de la matriz de confusión confusion_df = pd.DataFrame( matriz_confusion, columns=[&#39;Predicción Negativa&#39;, &#39;Predicción Positiva&#39;], index=[&#39;Real Negativa&#39;, &#39;Real Positiva&#39;] ) # Crear una figura utilizando Seaborn sns.heatmap(confusion_df, annot=True, fmt=&#39;d&#39;, cmap=&#39;Blues&#39;, cbar=False); plt.title(&#39;Matriz de Confusión&#39;); plt.xlabel(&#39;Predicción&#39;); plt.ylabel(&#39;Realidad&#39;); plt.show(); Tómese en cuenta que esta matriz de confusión es creada a partir de un punto de corte arbirtrario en las probabilidades de predicción. Por default, python asigna un corte de porbabilidad en 0.5, sin embargo, es posible realizar otro corte, según la conveniencia para el estudio. Teniendo esto en mente podemos definir las siguientes métricas: AUC y ROC: Area Under the Curve y Receiver operator characteristic Una curva ROC es un gráfico que muestra el desempeño de un modelo de clasificación en todos los puntos de corte. AUC significa “Área bajo la curva ROC”. Es decir, AUC mide el área debajo de la curva ROC. Para poder determinar cual es el mejor punto de corte, es indispensable conocer el comportamiento y efecto de los diferentes puntos de corte. Veamos un ejemplo visual en nuestra aplicación de Shiny: ConfusionMatrixShiny 6.6 Estimación de probabilidades Para poder tomar la mejor decisión del punto de corte, deberemos hacer predicciones de probabilidades y posteriormente usar el threshold que mejor sirva a los propósitos del estudio. import numpy as np y_pred = pipeline.predict_proba(telco_x_test)[:,0] Churn_Pred = np.where(y_pred &gt;= 0.7, &quot;No&quot;, &quot;Yes&quot;) results = ( telco_x_test &gt;&gt; mutate( Churn_Prob = y_pred, Churn_Pred = Churn_Pred, Churn = telco_y_test) &gt;&gt; select(_.Churn_Prob, _.Churn_Pred, _.Churn) ) results ## Churn_Prob Churn_Pred Churn ## 5058 0.77 No No ## 2466 0.91 No No ## 1740 0.94 No No ## 2977 0.83 No No ## 1726 0.58 Yes No ## ... ... ... ... ## 4068 0.91 No No ## 4648 0.53 Yes No ## 3996 0.82 No No ## 4076 0.77 No No ## 2031 0.90 No No ## ## [1409 rows x 3 columns] ( results &gt;&gt; group_by(_.Churn_Pred) &gt;&gt; summarize(n = _.Churn_Pred.count() ) ) ## Churn_Pred n ## 0 No 961 ## 1 Yes 448 confusion_df = pd.DataFrame( confusion_matrix(telco_y_test, Churn_Pred), columns=[&#39;Predicción Negativa&#39;, &#39;Predicción Positiva&#39;], index=[&#39;Real Negativa&#39;, &#39;Real Positiva&#39;] ) # Crear una figura utilizando Seaborn plt.plot(); sns.heatmap(confusion_df, annot=True, fmt=&#39;d&#39;, cmap=&#39;Blues&#39;, cbar=False); plt.title(&#39;Matriz de Confusión&#39;); plt.xlabel(&#39;Predicción&#39;); plt.ylabel(&#39;Realidad&#39;); plt.show(); from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score from plotnine import * fpr, tpr, thresholds = roc_curve( y_true = np.where(telco_y_test == &quot;Yes&quot;, 0, 1), y_score = y_pred ) roc_thresholds = pd.DataFrame({ &#39;thresholds&#39;: thresholds, &#39;tpr&#39;: tpr, &#39;fpr&#39;: fpr} ) ( roc_thresholds &gt;&gt; ggplot(aes(x = fpr, y = tpr)) + geom_path(size = 1.2) + geom_abline(colour = &quot;gray&quot;) + xlab(&quot;Tasa de falsos positivos&quot;) + ylab(&quot;Sensibilidad&quot;) + ggtitle(&quot;Curva ROC&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; roc_auc_score(np.where(telco_y_test == &quot;Yes&quot;, 0, 1), y_pred) ## 0.7081944767366762 precision, recall, thresholds = precision_recall_curve( y_true = np.where(telco_y_test == &quot;Yes&quot;, 0, 1), probas_pred = y_pred ) pr_thresholds = pd.DataFrame({ &#39;thresholds&#39;: np.append(0, thresholds), &#39;precision&#39;: precision, &#39;recall&#39;: recall} ) ( pr_thresholds &gt;&gt; ggplot(aes(x = recall, y =precision)) + geom_path(size = 1.2) + geom_abline(colour = &quot;gray&quot;, intercept = 1, slope = -1) + xlim(0, 1) + ylim(0, 1) + xlab(&quot;Recall&quot;) + ylab(&quot;Precision&quot;) + ggtitle(&quot;Curva PR&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; average_precision_score(np.where(telco_y_test == &quot;Yes&quot;, 0, 1), y_pred) ## 0.8662177360427568 6.7 Validación cruzada Un apaso fundamental al momento de crear modelos de ML es conocer la variación en el desempeño de un modelo. Queremos conocer cuánto suele fluctuar el desepeño cuando el modelo presenta perturbaciones en los datos a lo largo del tiempo. El esquema de validación cruzada permite usar datos de prueba distintos en cada iteración, de tal manera que es posible estimar la variación en el desempeño. from sklearn.model_selection import KFold, cross_val_score from sklearn.metrics import make_scorer, roc_auc_score, average_precision_score from sklearn.model_selection import cross_validate from sklearn.preprocessing import LabelEncoder # Definir el objeto K-Fold Cross Validator kf = KFold(n_splits=10, shuffle=True, random_state=42) # Definir las métricas de desempeño que deseas calcular como funciones de puntuación scoring = { &#39;roc_auc&#39;: make_scorer(roc_auc_score, greater_is_better=True), &#39;average_precision&#39;: make_scorer(average_precision_score, greater_is_better=True) } label_encoder = LabelEncoder() y = label_encoder.fit_transform(telco_y_train) # Realizar la validación cruzada y calcular métricas de desempeño utilizando cross_val_score results = cross_validate( pipeline, telco_train_selected, 1-y, cv=kf, scoring=scoring ) auc_roc_scores = results[&#39;test_roc_auc&#39;] auc_roc_scores ## array([0.64418544, 0.59368071, 0.58210621, 0.61630695, 0.5852977 , ## 0.57726774, 0.61213223, 0.60262944, 0.57838112, 0.5928815 ]) auc_pr_scores = results[&#39;test_average_precision&#39;] auc_pr_scores ## array([0.81737309, 0.76634381, 0.73688596, 0.78742837, 0.74429755, ## 0.78635671, 0.80363038, 0.77942919, 0.73790323, 0.79105159]) # Calcular estadísticas resumidas (media y desviación estándar) de las métricas mean_roc = np.mean(auc_roc_scores) std_roc = np.std(auc_pr_scores) mean_pr = np.mean(auc_pr_scores) std_pr = np.std(auc_pr_scores) ## ROC AUC: 0.5984869040092843 +/- 0.026499546449472944 ## PR AUC: 0.7750699890025018 +/- 0.026499546449472944 "],["k-nearest-neighbor.html", "Capítulo 7 K-Nearest-Neighbor 7.1 Clasificación 7.2 Regresión 7.3 Ajuste del modelo 7.4 Implementación en Python", " Capítulo 7 K-Nearest-Neighbor KNN es un algoritmo de aprendizaje supervisado que podemos usar tanto para regresión como clasificación. Es un algoritmo fácil de interpretar y que permite ser flexible en el balance entre sesgo y varianza (dependiendo de los hiper-parámetros seleccionados). El algoritmo de K vecinos más cercanos realiza comparaciones entre un nuevo elemento y las observaciones anteriores que ya cuentan con etiqueta. La esencia de este algoritmo está en etiquetar a un nuevo elemento de manera similar a como están etiquetados aquellos K elementos que más se le parecen. Veremos este proceso para cada uno de los posibles casos: 7.1 Clasificación La idea detrás del algoritmo es sencilla, etiqueta una nueva observación en la categoría que tenga mas elementos de las k observaciones más cercanas, es decir: Seleccionamos el hiper-parámetro K como el número elegido de vecinos. Se calculará la similitud (distancia) de esta nueva observación a cada observación existente. Ordenaremos estas distancias de menor a mayor. Tomamos las K primeras entradas de la lista ordenada. La nueva observación será asignada al grupo que tenga mayor número de observaciones en estas k primeras distancias (asignación por moda) A continuación se ejemplifica este proceso: Ejemplo: Otro método que permite tener mayor control sobre las clasificaciones es asignar la probabilidad de pertenencia a cada clase de acuerdo con la proporción existente de cada una de las mismas. A partir de dichas probabilidades, el usuario puede determinar el punto de corte que sea más conveniente para el problema a resolver. 7.2 Regresión En el caso de regresión, la etiqueta de una nueva observación se realiza a través del promedio del valor en las k observaciones más cercanas, es decir: Seleccionamos el hiper-parámetro K como el número elegido de vecinos. Se calculará la similitud (distancia) de esta nueva observación a cada observación existente Ordenaremos estas distancias de menor a mayor Tomamos las K primeras entradas de la lista ordenada. La nueva observación será etiquetada mediante el promedio del valor de las observaciones en estas k primeras distancias. Considerando un modelo de 3 vecinos más cercanos, las siguientes imágenes muestran el proceso de ajuste y predicción de nuevas observaciones. Ejemplo de balance de sesgo y varianza 7.3 Ajuste del modelo En contraste con otros algoritmos de aprendizaje supervisado, K-NN no genera un modelo del aprendizaje con datos de entrenamiento, sino que el aprendizaje sucede en el mismo momento en el que se prueban los datos de prueba. A este tipo de algoritmos se les llama lazy learning methods porque no aprende del conjunto de entrenamiento inmediatamente, sino que almacena el conjunto de datos y, en el momento de la clasificación, realiza una acción en el conjunto de datos. El algoritmo KNN en la fase de entrenamiento simplemente almacena el conjunto de datos y cuando obtiene nuevos datos, clasifica esos datos en una categoría que es muy similar a los nuevos datos. 7.3.1 Selección de Hiper-parámetro K Al configurar un modelo KNN, sólo hay algunos parámetros que deben elegirse/ajustarse para mejorar el rendimiento, uno de estos parámetros es el valor de la K. No existe una forma particular de determinar el mejor valor para “K”, por lo que debemos probar algunos valores para encontrar “el mejor” de ellos. Para los modelos de clasificación, especialmente si solo hay dos clases, generalmente se elige un número impar para k. Esto es para que el algoritmo nunca llegue a un “empate” Una opción para seleccionar la K adecuada es ejecutar el algoritmo KNN varias veces con diferentes valores de K y elegimos la K que reduce la cantidad de errores mientras se mantiene la capacidad del algoritmo para hacer predicciones con precisión. Observemos lo siguiente: Estas gráficas se conoce como “gráfica de codo” y generalmente se usan para determinar el valor K. A medida que disminuimos el valor de K a 1, nuestras predicciones se vuelven menos estables. Imaginemos que tomamos K = 1 y tenemos un punto de consulta rodeado por varios rojos y uno verde, pero el verde es el vecino más cercano. Razonablemente, pensaríamos que el punto de consulta es probablemente rojo, pero como K = 1, KNN predice incorrectamente que el punto de consulta es verde. Inversamente, a medida que aumentamos el valor de K, nuestras predicciones se vuelven más estables debido a que tenemos más observaciones con quienes comparar, por lo tanto, es más probable que hagan predicciones más precisas. Eventualmente, comenzamos a presenciar un número creciente de errores, es en este punto que sabemos que hemos llevado el valor de K demasiado lejos. 7.3.2 Métodos de cálculo de la distancia entre observaciones Otro parámetro que podemos ajustar para el modelo es la distancia usada, existen diferentes formas de medir qué tan “cerca” están dos puntos entre sí, y las diferencias entre estos métodos pueden volverse significativas en dimensiones superiores. La más utilizada es la distancia euclidiana, el tipo estándar de distancia. \\[d(X,Y) = \\sqrt{\\sum_{i=1}^{n} (x_i-y_i)^2}\\] Otra métrica es la llamada distancia de Manhattan, que mide la distancia tomada en cada dirección cardinal, en lugar de a lo largo de la diagonal. \\[d(X,Y) = \\sum_{i=1}^{n} |x_i - y_i|\\] De manera más general, las anteriores son casos particulares de la distancia de Minkowski, cuya fórmula es: \\[d(X,Y) = (\\sum_{i=1}^{n} |x_i-y_i|^p)^{\\frac{1}{p}}\\] La distancia de coseno es ampliamente en análisis de texto, sistemas de recomendación \\[d(X,Y)= 1 - \\frac{\\sum_{i=1}^{n}{X_iY_i}}{\\sqrt{\\sum_{i=1}^{n}{X_i^2}}\\sqrt{\\sum_{i=1}^{n}{Y_i^2}}}\\] Un link interesante Otro link interesante 7.4 Implementación en Python Usaremos los pipelines antes implementados para ajustar tanto el modelo de regresión como el de clasificación. Una ventaja que se explorará al usar KFCV es el conjunto de hiperparámetros para elegir el mejor modelo posible. Los pasos a seguir, son los siguientes: Carga de librerías Carga y separación inicial de datos ( test, train ). Pre-procesamiento e ingeniería de variables. Selección de tipo de modelo con hiperparámetros iniciales. Cálculo de métricas de desempeño. Creación de grid search y métricas de desempeño. Entrenamiento de modelos con hiperparámetros definidos. Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario). Selección de modelo a usar. Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario). Validar poder predictivo con datos de prueba. 7.4.1 Regresión Paso 0: Carga de librerías from mlxtend.feature_selection import ColumnSelector from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.neighbors import KNeighborsRegressor from sklearn.pipeline import Pipeline from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error from sklearn.metrics import mean_squared_error, r2_score, make_scorer from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate from sklearn.model_selection import GridSearchCV from sklearn.utils import shuffle from plydata.one_table_verbs import pull from mizani.formatters import comma_format, dollar_format from plotnine import * from siuba import * import pandas as pd import numpy as np Paso 1: Carga y separación inicial de datos ( test, train ) Comenzamos por cargar los datos completos e identificar la variable de respuesta para separarla de las explicativas #### CARGA DE DATOS #### ames = pd.read_csv(&quot;data/ames.csv&quot;) ames_y = ames &gt;&gt; pull(&quot;Sale_Price&quot;) # ames[[&quot;Sale_Price&quot;]] ames_x = select(ames, -_.Sale_Price) # ames.drop(&#39;Sale_Price&#39;, axis=1) #### DIVISIÓN DE DATOS #### ames_x_train, ames_x_test, ames_y_train, ames_y_test = train_test_split( ames_x, ames_y, test_size = 0.20, random_state = 195 ) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables En este paso se pone a prueba la imaginación y conocimiento de transformaciones estadísticas que permitan ofrecer un mejor ajuste a la relación entre datos dependientes y de respuesta. EL primer pipeline es para facilitar las transformaciones sobre las columnas sugeridas inicialmente: ## SELECCIÓN DE VARIABLES # Seleccionamos las variales numéricas de interés num_cols = [&quot;Full_Bath&quot;, &quot;Half_Bath&quot;] # Seleccionamos las variables categóricas de interés cat_cols = [&quot;Overall_Cond&quot;] # Juntamos todas las variables de interés columnas_seleccionadas = num_cols + cat_cols pipe = ColumnSelector(columnas_seleccionadas) ames_x_train_selected = pipe.fit_transform(ames_x_train) ames_train_selected = pd.DataFrame( ames_x_train_selected, columns = columnas_seleccionadas ) ames_train_selected.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2344 entries, 0 to 2343 ## Data columns (total 3 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Full_Bath 2344 non-null object ## 1 Half_Bath 2344 non-null object ## 2 Overall_Cond 2344 non-null object ## dtypes: object(3) ## memory usage: 55.1+ KB Y una vez que se han sugerido algunas columnas, se procede a realizar el pipeline de transformación: ## TRANSFORMACIÓN DE COLUMNAS # ColumnTransformer para aplicar transformaciones preprocessor = ColumnTransformer( transformers = [ (&#39;scaler&#39;, StandardScaler(), num_cols), (&#39;onehotencoding&#39;, OneHotEncoder(drop=&#39;first&#39;, sparse_output=False), cat_cols) ], verbose_feature_names_out = False, remainder = &#39;passthrough&#39; # Mantener las columnas restantes sin cambios ) transformed_data = preprocessor.fit_transform(ames_train_selected) new_column_names = preprocessor.get_feature_names_out() transformed_df = pd.DataFrame( transformed_data, columns=new_column_names ) transformed_df ## Full_Bath Half_Bath ... Overall_Cond_Very_Good \\ ## 0 0.78 1.24 ... 0.00 ## 1 0.78 -0.75 ... 0.00 ## 2 0.78 -0.75 ... 0.00 ## 3 -1.01 -0.75 ... 0.00 ## 4 -1.01 1.24 ... 0.00 ## ... ... ... ... ... ## 2339 -1.01 1.24 ... 0.00 ## 2340 0.78 -0.75 ... 0.00 ## 2341 -1.01 -0.75 ... 0.00 ## 2342 0.78 1.24 ... 0.00 ## 2343 0.78 -0.75 ... 0.00 ## ## Overall_Cond_Very_Poor ## 0 0.00 ## 1 0.00 ## 2 0.00 ## 3 0.00 ## 4 0.00 ## ... ... ## 2339 0.00 ## 2340 0.00 ## 2341 0.00 ## 2342 0.00 ## 2343 0.00 ## ## [2344 rows x 10 columns] transformed_df.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2344 entries, 0 to 2343 ## Data columns (total 10 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Full_Bath 2344 non-null float64 ## 1 Half_Bath 2344 non-null float64 ## 2 Overall_Cond_Average 2344 non-null float64 ## 3 Overall_Cond_Below_Average 2344 non-null float64 ## 4 Overall_Cond_Excellent 2344 non-null float64 ## 5 Overall_Cond_Fair 2344 non-null float64 ## 6 Overall_Cond_Good 2344 non-null float64 ## 7 Overall_Cond_Poor 2344 non-null float64 ## 8 Overall_Cond_Very_Good 2344 non-null float64 ## 9 Overall_Cond_Very_Poor 2344 non-null float64 ## dtypes: float64(10) ## memory usage: 183.2 KB Recordemos que la función ColumnTransformes() solo son los pasos a seguir, necesitamos usar el método fit() que nos devuelve una receta actualizada con las estimaciones y la función transform() que nos devuelve la matriz transformada. Estos pasos pueden resumirse con el método: fit_transform() Una vez que la receta de transformación de datos está lista, procedemos a implementar el pipeline del modelo de interés. Paso 3: Selección de tipo de modelo con hiperparámetros iniciales Ahora se procede a unir en un mismo flujo el proceso de ingeniería de datos y un modelo inicial sugerido. En este primer ejemplo, se muestra un modelo de 5 vecinos más cercanos. # Crear el pipeline con la regresión por KNN pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, KNeighborsRegressor(n_neighbors=5)) ]) # Entrenar el pipeline results = pipeline.fit(ames_train_selected, ames_y_train) ## PREDICCIONES y_pred = pipeline.predict(ames_x_test) ames_test = ( ames_x_test &gt;&gt; mutate(Sale_Price_Pred = y_pred, Sale_Price = ames_y_test) ) ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) ) ## Sale_Price Sale_Price_Pred ## 390 165000 192410.00 ## 1235 124000 131950.00 ## 2288 75000 131950.00 ## 107 206000 130440.00 ## 1861 190000 159600.00 ## ... ... ... ## 116 171000 181390.00 ## 398 120500 130440.00 ## 1253 146000 192410.00 ## 78 125000 167706.00 ## 714 110000 160180.00 ## ## [586 rows x 2 columns] Estas son las predicciones logradas con el modelo inicial. 7.4.1.1 Métricas de desempeño Se procede en el siguiente paso a cuantificar los errores producidos por la predicción. Paso 4: Cálculo de métricas de desempeño pd.options.display.float_format = &#39;{:.2f}&#39;.format y_obs = ames_test[&quot;Sale_Price&quot;] y_pred = ames_test[&quot;Sale_Price_Pred&quot;] me = np.mean(y_obs - y_pred) mae = mean_absolute_error(y_obs, y_pred) mape = mean_absolute_percentage_error(y_obs, y_pred) mse = mean_squared_error(y_obs, y_pred) rmse = np.sqrt(mse) r2 = r2_score(y_obs, y_pred) n = len(y_obs) # Número de observaciones p = 9 # Número de predictores r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2) metrics_data = { &quot;Metric&quot;: [&quot;ME&quot;, &quot;MAE&quot;, &quot;MAPE&quot;, &quot;MSE&quot;, &quot;RMSE&quot;, &quot;R^2&quot;, &quot;R^2 Adj&quot;], &quot;Value&quot;: [me, mae, mape, mse, rmse, r2, r2_adj] } metrics_df = pd.DataFrame(metrics_data) metrics_df ## Metric Value ## 0 ME 3944.20 ## 1 MAE 47350.96 ## 2 MAPE 0.27 ## 3 MSE 4911787604.86 ## 4 RMSE 70084.15 ## 5 R^2 0.25 ## 6 R^2 Adj 0.24 Una manera amigable de dimensionar los errores y el buen desempeño es mediante gráficos #### Gráficos de desempeño de modelo ( ames_test &gt;&gt; ggplot(aes(x = &quot;Sale_Price&quot;, y = &quot;Sale_Price_Pred&quot;)) + geom_point() + scale_y_continuous(labels = dollar_format(digits=0, big_mark=&#39;,&#39;), limits = [0, 600000] ) + scale_x_continuous(labels = dollar_format(digits=0, big_mark=&#39;,&#39;), limits = [0, 500000] ) + geom_abline(color = &quot;red&quot;) + coord_equal() + labs( title = &quot;Comparación entre predicción y observación&quot;, y = &quot;Predicción&quot;, x = &quot;Observación&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(x = &quot;error&quot;)) + geom_histogram(color = &quot;white&quot;, fill = &quot;black&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + scale_x_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + ylab(&quot;Conteos de clase&quot;) + xlab(&quot;Errores&quot;) + ggtitle(&quot;Distribución de error&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(sample = &quot;error&quot;)) + geom_qq(alpha = 0.3) + stat_qq_line(color = &quot;red&quot;) + scale_y_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits = 0)) + xlab(&quot;Distribución normal&quot;) + ylab(&quot;Distribución de errores&quot;) + ggtitle(&quot;QQ-Plot&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(x = &quot;Sale_Price&quot;)) + geom_linerange(aes(ymin = 0, ymax = &quot;error&quot;), colour = &quot;purple&quot;) + geom_point(aes(y = &quot;error&quot;), size = 0.05, alpha = 0.5) + geom_abline(intercept = 0, slope = 0) + scale_x_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + scale_y_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + xlab(&quot;Precio real&quot;) + ylab(&quot;Error de estimación&quot;) + ggtitle(&quot;Relación entre error y precio de venta&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; 7.4.1.2 Validación cruzada Para determinar cuáles son los hiper-parámetros que funcionan mejor, es necesario realizar experimentos mediante ensayo-error hasta determinar la mejor solución. En cada partición del método de muestreo KFCV se implementan las distintas configuraciones y se calculan predicciones. Con las predicciones hechas en cada fold, se obtienen intervalos de confianza para conocer la variación asociada al modelo a través de los hiper-parámetros implementados. Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo, sin embargo, para realizar este proceso de forma ágil, se inicializará un flujo de trabajo que se encargue de realizar todos los experimentos deseados y elegir el modelo adecuado. Paso 5: Creación de grid search y métricas de desempeño # Definir el objeto K-Fold Cross Validator k = 10 kf = KFold(n_splits=k, shuffle=True, random_state=42) param_grid = { &#39;n_neighbors&#39;: range(2, 21), &#39;weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;], &#39;metric&#39;: [&#39;euclidean&#39;, &#39;manhattan&#39;], &#39;p&#39;: [1, 2] } Algunas otras posibles distancias son: euclidean manhattan chebyshev minkowski hamming jaccard cosine Una vez definidos los posibles hiperparámetros, procedemos a definir las métricas que serán usadas para cuantificar la bondad del ajuste. # Definir las métricas de desempeño que deseas calcular como funciones de puntuación def adjusted_r2_score(y_true, y_pred, n, p): r2 = r2_score(y_true, y_pred) adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) return adjusted_r2 scoring = { &#39;neg_mean_squared_error&#39;: make_scorer(mean_squared_error, greater_is_better=False), &#39;r2&#39;: make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_train_selected)), p=len(ames_train_selected.columns)), &#39;neg_mean_absolute_error&#39;: make_scorer(mean_absolute_error, greater_is_better=False), &#39;mape&#39;: make_scorer(mean_absolute_percentage_error, greater_is_better=False) } Paso 6: Entrenamiento de modelos con hiperparámetros definidos Teniendo todos los elementos anteriores listos, se procede con el ajuste de todas las posibles configuraciones del modelo a través de la validación cruzada. Esto permitirá contar con medidas de tendencia central para los resultados de cada uno de las configuraciones y evaluar si estadísticamente hay diferencia entre los mejores resultados. pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, GridSearchCV( KNeighborsRegressor(), param_grid, cv=kf, scoring=scoring, refit=&#39;neg_mean_squared_error&#39;, verbose=3, n_jobs=-1) ) ]) pipeline.fit(ames_train_selected, ames_y_train) ## Fitting 10 folds for each of 152 candidates, totalling 1520 fits ## Pipeline(steps=[(&#39;preprocessor&#39;, ## ColumnTransformer(remainder=&#39;passthrough&#39;, ## transformers=[(&#39;scaler&#39;, StandardScaler(), ## [&#39;Full_Bath&#39;, &#39;Half_Bath&#39;]), ## (&#39;onehotencoding&#39;, ## OneHotEncoder(drop=&#39;first&#39;, ## sparse_output=False), ## [&#39;Overall_Cond&#39;])], ## verbose_feature_names_out=False)), ## (&#39;regressor&#39;, ## GridSearchCV(cv=KFold(n_splits=10, random_state=42, shuffle=True), ## estimator=KN... ## &#39;weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;]}, ## refit=&#39;neg_mean_squared_error&#39;, ## scoring={&#39;mape&#39;: make_scorer(mean_absolute_percentage_error, greater_is_better=False), ## &#39;neg_mean_absolute_error&#39;: make_scorer(mean_absolute_error, greater_is_better=False), ## &#39;neg_mean_squared_error&#39;: make_scorer(mean_squared_error, greater_is_better=False), ## &#39;r2&#39;: make_scorer(adjusted_r2_score, n=2344.0, p=3)}, ## verbose=3))]) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) Podemos obtener las métricas de los resultados de cada fold: results_cv = pipeline.named_steps[&#39;regressor&#39;].cv_results_ # Convierte los resultados en un DataFrame pd.set_option(&#39;display.max_columns&#39;, 500) results_df = pd.DataFrame(results_cv) results_df.columns ## Index([&#39;mean_fit_time&#39;, &#39;std_fit_time&#39;, &#39;mean_score_time&#39;, &#39;std_score_time&#39;, ## &#39;param_metric&#39;, &#39;param_n_neighbors&#39;, &#39;param_p&#39;, &#39;param_weights&#39;, ## &#39;params&#39;, &#39;split0_test_neg_mean_squared_error&#39;, ## &#39;split1_test_neg_mean_squared_error&#39;, ## &#39;split2_test_neg_mean_squared_error&#39;, ## &#39;split3_test_neg_mean_squared_error&#39;, ## &#39;split4_test_neg_mean_squared_error&#39;, ## &#39;split5_test_neg_mean_squared_error&#39;, ## &#39;split6_test_neg_mean_squared_error&#39;, ## &#39;split7_test_neg_mean_squared_error&#39;, ## &#39;split8_test_neg_mean_squared_error&#39;, ## &#39;split9_test_neg_mean_squared_error&#39;, ## &#39;mean_test_neg_mean_squared_error&#39;, &#39;std_test_neg_mean_squared_error&#39;, ## &#39;rank_test_neg_mean_squared_error&#39;, &#39;split0_test_r2&#39;, &#39;split1_test_r2&#39;, ## &#39;split2_test_r2&#39;, &#39;split3_test_r2&#39;, &#39;split4_test_r2&#39;, &#39;split5_test_r2&#39;, ## &#39;split6_test_r2&#39;, &#39;split7_test_r2&#39;, &#39;split8_test_r2&#39;, &#39;split9_test_r2&#39;, ## &#39;mean_test_r2&#39;, &#39;std_test_r2&#39;, &#39;rank_test_r2&#39;, ## &#39;split0_test_neg_mean_absolute_error&#39;, ## &#39;split1_test_neg_mean_absolute_error&#39;, ## &#39;split2_test_neg_mean_absolute_error&#39;, ## &#39;split3_test_neg_mean_absolute_error&#39;, ## &#39;split4_test_neg_mean_absolute_error&#39;, ## &#39;split5_test_neg_mean_absolute_error&#39;, ## &#39;split6_test_neg_mean_absolute_error&#39;, ## &#39;split7_test_neg_mean_absolute_error&#39;, ## &#39;split8_test_neg_mean_absolute_error&#39;, ## &#39;split9_test_neg_mean_absolute_error&#39;, ## &#39;mean_test_neg_mean_absolute_error&#39;, &#39;std_test_neg_mean_absolute_error&#39;, ## &#39;rank_test_neg_mean_absolute_error&#39;, &#39;split0_test_mape&#39;, ## &#39;split1_test_mape&#39;, &#39;split2_test_mape&#39;, &#39;split3_test_mape&#39;, ## &#39;split4_test_mape&#39;, &#39;split5_test_mape&#39;, &#39;split6_test_mape&#39;, ## &#39;split7_test_mape&#39;, &#39;split8_test_mape&#39;, &#39;split9_test_mape&#39;, ## &#39;mean_test_mape&#39;, &#39;std_test_mape&#39;, &#39;rank_test_mape&#39;], ## dtype=&#39;object&#39;) # Puedes seleccionar las columnas de interés, por ejemplo: summary_df = ( results_df &gt;&gt; select(-_.contains(&quot;split&quot;), -_.contains(&quot;time&quot;), -_.params) ) summary_df ## param_metric param_n_neighbors param_p param_weights \\ ## 0 euclidean 2 1 uniform ## 1 euclidean 2 1 distance ## 2 euclidean 2 2 uniform ## 3 euclidean 2 2 distance ## 4 euclidean 3 1 uniform ## .. ... ... ... ... ## 147 manhattan 19 2 distance ## 148 manhattan 20 1 uniform ## 149 manhattan 20 1 distance ## 150 manhattan 20 2 uniform ## 151 manhattan 20 2 distance ## ## mean_test_neg_mean_squared_error std_test_neg_mean_squared_error \\ ## 0 -5511130178.67 779535172.99 ## 1 -5644748510.42 867751490.59 ## 2 -5511130178.67 779535172.99 ## 3 -5644748510.42 867751490.59 ## 4 -4728796013.81 503312671.96 ## .. ... ... ## 147 -3975017717.36 819048816.69 ## 148 -3753148524.86 499208852.34 ## 149 -3960403910.23 825919565.56 ## 150 -3753148524.86 499208852.34 ## 151 -3960403910.23 825919565.56 ## ## rank_test_neg_mean_squared_error mean_test_r2 std_test_r2 \\ ## 0 145 0.12 0.10 ## 1 149 0.10 0.10 ## 2 145 0.12 0.10 ## 3 149 0.10 0.10 ## 4 139 0.25 0.07 ## .. ... ... ... ## 147 59 0.37 0.09 ## 148 1 0.40 0.05 ## 149 55 0.37 0.09 ## 150 1 0.40 0.05 ## 151 55 0.37 0.09 ## ## rank_test_r2 mean_test_neg_mean_absolute_error \\ ## 0 145 -51009.74 ## 1 149 -51232.98 ## 2 145 -51009.74 ## 3 149 -51232.98 ## 4 139 -47533.27 ## .. ... ... ## 147 61 -42546.03 ## 148 1 -42399.59 ## 149 55 -42392.34 ## 150 1 -42399.59 ## 151 55 -42392.34 ## ## std_test_neg_mean_absolute_error rank_test_neg_mean_absolute_error \\ ## 0 3971.24 145 ## 1 3914.42 149 ## 2 3971.24 145 ## 3 3914.42 149 ## 4 2605.61 139 ## .. ... ... ## 147 2542.37 53 ## 148 2134.62 17 ## 149 2507.64 15 ## 150 2134.62 17 ## 151 2507.64 15 ## ## mean_test_mape std_test_mape rank_test_mape ## 0 -0.31 0.04 147 ## 1 -0.31 0.04 151 ## 2 -0.31 0.04 147 ## 3 -0.31 0.04 151 ## 4 -0.29 0.03 139 ## .. ... ... ... ## 147 -0.25 0.02 45 ## 148 -0.26 0.02 71 ## 149 -0.25 0.02 19 ## 150 -0.26 0.02 71 ## 151 -0.25 0.02 19 ## ## [152 rows x 16 columns] En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: ( summary_df &gt;&gt; ggplot(aes(x = &quot;param_n_neighbors&quot;, y = &quot;mean_test_r2&quot;, size = &quot;param_p&quot;, color = &quot;param_metric&quot;, shape = &quot;param_weights&quot;)) + geom_point(alpha = 0.65) + ggtitle(&quot;Parametrización de KNN vs R^2&quot;) + xlab(&quot;Parámetro: Número de vecinos cercanos&quot;) + ylab(&quot;R^2 promedio&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; Seleccionando la mejor parametrización, se puede definir la siguiente gráfica para comparar la diferencia entre hiperparámetros a un nivel más granular. ( summary_df &gt;&gt; filter( _.param_weights == &quot;uniform&quot;, _.param_p == 1, _.param_metric == &quot;manhattan&quot;) &gt;&gt; mutate( ymin = np.maximum(0, _.mean_test_r2 - _.std_test_r2), ymax = np.minimum(1, _.mean_test_r2 + _.std_test_r2)) &gt;&gt; ggplot(aes(x = &quot;param_n_neighbors&quot;, y = &quot;mean_test_r2&quot;)) + geom_errorbar(aes(ymin=&#39;ymin&#39;, ymax=&#39;ymax&#39;), width=0.3, position=position_dodge(0.9)) + geom_point(alpha = 0.65) + ggtitle(&quot;Parametrización de KNN vs R^2&quot;) + xlab(&quot;Parámetro: Número de vecinos cercanos&quot;) + ylab(&quot;R^2 promedio&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; En la anterior gráfica observamos la R^2 con distintos números de vecinos. Paso 8: Selección de modelo a usar Habiendo realizado un análisis de los hiperparámetros, se procede a elegir el mejor modelo. Esto a veces a muy evidente y otras no. En cualquier caso, puede automatizarse la extracción del mejor modelo: best_params = pipeline.named_steps[&#39;regressor&#39;].best_params_ best_params ## {&#39;metric&#39;: &#39;manhattan&#39;, &#39;n_neighbors&#39;: 20, &#39;p&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;} Sabiendo cuáles son los mejores hiperparámetros, se procede a extraer el modelo que DEBEREMOS AJUSTAR A TODOS LOS DATOS DE ENTRENAMIENTO. best_estimator = pipeline.named_steps[&#39;regressor&#39;].best_estimator_ best_estimator ## KNeighborsRegressor(metric=&#39;manhattan&#39;, n_neighbors=20, p=1) Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) Ahora obtendremos el modelo que mejor desempeño tiene y haremos las predicciones del conjunto de prueba con este modelo. Es importante volver a hacer el ajuste con el modelo elegido. final_knn_pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, best_estimator) ]) # Entrenar el pipeline final_knn_pipeline.fit(ames_train_selected, ames_y_train) ## Pipeline(steps=[(&#39;preprocessor&#39;, ## ColumnTransformer(remainder=&#39;passthrough&#39;, ## transformers=[(&#39;scaler&#39;, StandardScaler(), ## [&#39;Full_Bath&#39;, &#39;Half_Bath&#39;]), ## (&#39;onehotencoding&#39;, ## OneHotEncoder(drop=&#39;first&#39;, ## sparse_output=False), ## [&#39;Overall_Cond&#39;])], ## verbose_feature_names_out=False)), ## (&#39;regressor&#39;, ## KNeighborsRegressor(metric=&#39;manhattan&#39;, n_neighbors=20, p=1))]) Este último objeto creado es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción, sólo se necesita de los nuevos datos y de este último elemento para poder realizar nuevas predicciones. Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: ## Predicciones finales y_pred_knn = final_knn_pipeline.predict(ames_x_test) results_reg = ( ames_x_test &gt;&gt; mutate(final_knn_pred = y_pred_knn, Sale_Price = ames_y_test) &gt;&gt; select(_.Sale_Price, _.final_knn_pred) ) results_reg ## Sale_Price final_knn_pred ## 390 165000 211575.75 ## 1235 124000 186793.90 ## 2288 75000 186793.90 ## 107 206000 129435.00 ## 1861 190000 221465.45 ## ... ... ... ## 116 171000 146492.50 ## 398 120500 129435.00 ## 1253 146000 211575.75 ## 78 125000 132821.50 ## 714 110000 172145.00 ## ## [586 rows x 2 columns] Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paquetería MLmetrics. Es posible definir nuestro propio conjunto de métricas que deseamos reportar creando el objeto metric_set: me = np.mean(y_obs - y_pred_knn) mae = mean_absolute_error(y_obs, y_pred_knn) mape = mean_absolute_percentage_error(y_obs, y_pred_knn) mse = mean_squared_error(y_obs, y_pred_knn) rmse = np.sqrt(mse) r2 = r2_score(y_obs, y_pred_knn) r2_adj = adjusted_r2_score(y_true = y_obs, y_pred = y_pred_knn, n=np.ceil(len(ames_train_selected)), p=len(ames_train_selected.columns)) metrics_data = { &quot;Metric&quot;: [&quot;ME&quot;, &quot;MAE&quot;, &quot;MAPE&quot;, &quot;MSE&quot;, &quot;RMSE&quot;, &quot;R^2&quot;, &quot;R^2 Adj&quot;], &quot;Value&quot;: [me, mae, mape, mse, rmse, r2, r2_adj] } metrics_df = pd.DataFrame(metrics_data) metrics_df ## Metric Value ## 0 ME 82.60 ## 1 MAE 45123.19 ## 2 MAPE 0.26 ## 3 MSE 4275876001.50 ## 4 RMSE 65390.18 ## 5 R^2 0.35 ## 6 R^2 Adj 0.35 ( results_reg &gt;&gt; ggplot(aes(x = &quot;final_knn_pred&quot;, y = &quot;Sale_Price&quot;)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; 7.4.1.3 Importancia de variables Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia de cada variable en el modelo. importance = np.zeros(ames_x_test[columnas_seleccionadas].shape[1]) # Realiza el procedimiento de permutación for i in range(ames_x_test[columnas_seleccionadas].shape[1]): ames_x_test_permuted = ames_x_test[columnas_seleccionadas].copy() ames_x_test_permuted.iloc[:, i] = shuffle(ames_x_test_permuted.iloc[:, i], random_state=42) # Permuta una característica y_pred_permuted = final_knn_pipeline.predict(ames_x_test_permuted) mse_permuted = mean_squared_error(ames_y_test, y_pred_permuted) importance[i] = mse - mse_permuted # Calcula la importancia relativa importance = importance / importance.sum() importance ## array([0.70636906, 0.13520653, 0.15842441]) importance_df = pd.DataFrame({ &#39;Variable&#39;: columnas_seleccionadas, &#39;Importance&#39;: importance }) # Crea la gráfica de barras ( importance_df &gt;&gt; ggplot(aes(x= &#39;reorder(Variable, Importance)&#39;, y=&#39;Importance&#39;)) + geom_bar(stat=&#39;identity&#39;, fill=&#39;blue&#39;, color = &quot;black&quot;) + labs(title=&#39;Importancia de las Variables&#39;, x=&#39;Variable&#39;, y=&#39;Importancia&#39;) + coord_flip() ) ## &lt;Figure Size: (1280 x 960)&gt; La gráfica anterior muestra la importancia de una variable cuando se lleva a cabo una permutación. Dado que este resultado fue aleatorio, resulta vital contar con un conjunto de permutaciones que permitan conocer la caída promedio y desviación esperada en el desempeño cuando se elimina una variable o esta se vuelve inservible. n_permutations = 50 performance_losses = [] for i in range(ames_x_test[columnas_seleccionadas].shape[1]): loss = [] for j in range(n_permutations): ames_x_test_permuted = ames_x_test[columnas_seleccionadas].copy() ames_x_test_permuted.iloc[:, i] = np.random.permutation(ames_x_test_permuted.iloc[:, i]) y_pred_permuted = final_knn_pipeline.predict(ames_x_test_permuted) mse_permuted = mean_squared_error(ames_y_test, y_pred_permuted) loss.append(mse_permuted) performance_losses.append(loss) performance_losses = performance_losses/np.sum(performance_losses, axis=0) mean_losses = np.mean(performance_losses, axis=1) std_losses = np.std(performance_losses, axis=1) importance_df = pd.DataFrame({ &#39;Variable&#39;: columnas_seleccionadas, &#39;Mean_Loss&#39;: mean_losses, &#39;Std_Loss&#39;: std_losses }) ( importance_df &gt;&gt; mutate( ymin = _.Mean_Loss - _.Std_Loss, ymax = _.Mean_Loss + _.Std_Loss) &gt;&gt; ggplot(aes(x = &#39;reorder(Variable, Mean_Loss)&#39;, y = &quot;Mean_Loss&quot;)) + geom_errorbar(aes(ymin=&#39;ymin&#39;, ymax=&#39;ymax&#39;), width=0.2, position=position_dodge(0.9)) + geom_point(alpha = 0.65) + labs(title=&#39;Importancia de las Variables&#39;, x=&#39;Variable&#39;, y=&#39;Importancia&#39;) + coord_flip() ) ## &lt;Figure Size: (1280 x 960)&gt; Si en la gráfica anterior notamos algo raro en cuanto a la(s) variable(s) más importante(s) y la factibilidad de conseguirla(s) o usarla(s)… ¡¡HAY QUE EMPEZAR DESDE CERO SIN CONSIDERAR ESA VARIABLE!! "],["árboles-de-decisión.html", "Capítulo 8 Árboles de decisión 8.1 Ajuste del modelo 8.2 Regularización de árboles 8.3 Aprendizaje conjunto 8.4 Bagging 8.5 Random Forest 8.6 Implementación en Python", " Capítulo 8 Árboles de decisión Un árbol de decisiones es un algoritmo del aprendizaje supervisado que se puede utilizar tanto para problemas de clasificación como de regresión. Es un clasificador estructurado en árbol, donde los nodos internos representan las características de un conjunto de datos, las ramas representan las reglas de decisión y cada nodo hoja representa el resultado. La idea básica de los árboles es buscar puntos de cortes en las variables de entrada para hacer predicciones, ir dividiendo la muestra, y encontrar cortes sucesivos para refinar las predicciones. En un árbol de decisión, hay dos tipos nodos, el nodo de decisión o nodos internos (Decision Node) y el nodo hoja o nodo terminal (Leaf node). Los nodos de decisión se utilizan para tomar cualquier decisión y tienen múltiples ramas, mientras que los nodos hoja son el resultado de esas decisiones y no contienen más ramas. Regresión: En el caso de la regresión de árboles de decisión, en los nodos finales se calcula el promedio de la variable de respuesta. El promedio será la estimación del modelo. Clasificación: Por otro lado, en los árboles de clasificación se calcula la proporción de elementos de cada categoría en los nodos finales. De esta manera se calcula la probabilidad de pertenencia a la categoría. 8.1 Ajuste del modelo En un árbol de decisión, para predecir la clase del conjunto de datos, el algoritmo comienza desde el nodo raíz del árbol. Este algoritmo compara los valores de la variable raíz con la variable de registro y, según la comparación, sigue una rama y salta al siguiente nodo. Para el siguiente nodo, el algoritmo vuelve a comparar el valor de la siguiente variable con los otros sub-nodos y avanza. Continúa el proceso hasta que se llega a un nodo hoja. El proceso completo se puede comprender mejor con los siguientes pasos: Comenzamos el árbol con el nodo raíz (llamado S), que contiene el conjunto de entrenamiento completo. Encuentre la mejor variable en el conjunto de datos usando Attribute Selective Measure (ASM). Divida la S en subconjuntos que contengan valores posibles para la mejor variable. Genere el nodo del árbol de decisión, que contiene la mejor variable. Cree de forma recursiva nuevos árboles de decisión utilizando los subconjuntos del conjunto de datos creado en el paso 3. Continúe este proceso hasta que se alcance una etapa en la que no pueda particionar más los nodos y este nodo final sera un nodo hoja. Para clasificación nos quedaremos la moda de la variable respuesta del nodo hoja y para regresión usaremos la media de la variable respuesta. 8.1.1 Attribute Selective Measure (ASM) Al implementar un árbol de decisión, surge el problema principal de cómo seleccionar la mejor variable para el nodo raíz y para los sub-nodos. Para resolver este problemas existe una técnica que se llama medida de selección de atributos o ASM. Mediante esta medición, podemos seleccionar fácilmente la mejor variable para los nodos del árbol. Una de las técnicas más populares para ASM es: Índice de Gini La medida del grado de probabilidad de que una variable en particular se clasifique incorrectamente cuando se elige al azar se llama índice de Gini o impureza de Gini. Los datos se distribuyen por igual según el índice de Gini. \\[Gini = \\sum_{i=1}^{n}\\hat{p_i}(1-\\hat{p}_i)\\] Con \\(p_i\\) como la probabilidad de que un objeto se clasifique en una clase particular. Esta métrica puede analizarse como una métrica de impureza. Cuando todos o la mayoría de elementos dentro de un nodo pertenecen a una misma clase, el índice de Gini toma valores cercanos a cero. Cuando se utiliza el índice de Gini como criterio seleccionar la variable para el nodo raíz, seleccionaremos la variable con el índice de Gini menor. 8.2 Regularización de árboles Para asegurarse de que no exista sobre-ajuste en el modelo, es importante considerar algunas regularizaciones a los hiper-parámetros implementados. Posteriormente, se determinará cuál de las posibles combinaciones produce mejores resultados. 8.2.1 Nivel de profundidad de árbol Podríamos preguntarnos cuándo dejar de crecer un árbol. Pueden existir problemas que tengan un gran conjunto de variables y esto da como resultado una gran cantidad de divisiones, lo que a su vez genera un árbol de decisión muy grande. Estos árboles son complejos y pueden provocar un sobre-ajuste. Entonces, necesitamos saber cuándo parar. Una forma de hacer esto, es establecer un número mínimo de entradas de entrenamiento para dividir un nodo (min_n). Otra forma, es establecer la profundidad máxima del modelo. La profundidad máxima se refiere a la longitud del camino más largo desde el nodo raíz hasta un nodo hoja (max_depth). 8.3 Aprendizaje conjunto El aprendizaje conjunto da crédito a la idea de la “sabiduría de las multitudes”, lo que sugiere que la toma de decisiones de un grupo más grande de individuos (modelos) suele ser mejor que la de un individuo. El aprendizaje en conjunto es un grupo (o conjunto) de individuos o modelos, que trabajan colectivamente para lograr una mejor predicción final. Un solo modelo, también conocido como aprendiz básico puede no funcionar bien individualmente debido a una gran variación o un alto sesgo, sin embargo, cuando se agregan individuos débiles, pueden formar un individuo fuerte, ya que su combinación reduce el sesgo o la varianza, lo que produce un mejor rendimiento del modelo. Los métodos de conjunto se ilustran con frecuencia utilizando árboles de decisión, ya que este algoritmo puede ser propenso a sobre ajustar (alta varianza y bajo sesgo) y también puede prestarse a desajuste (baja varianza y alto sesgo) cuando es muy pequeño, como un árbol de decisión con un nivel. Nota: Cuando un algoritmo se adapta o no se adapta a su conjunto de entrenamiento, no se puede generalizar bien a nuevos conjuntos de datos, por lo que se utilizan métodos de conjunto para contrarrestar este comportamiento y permitir la generalización del modelo a nuevos conjuntos de datos. 8.4 Bagging Primero tenemos que definir qué es la Agregación de Bootstrap o Bagging. Este es un algoritmo de aprendizaje automático diseñado para mejorar la estabilidad y precisión de algoritmos de ML usados en clasificación estadística y regresión. Además reduce la varianza y ayuda a evitar el sobre-ajuste. Aunque es usualmente aplicado a métodos de árboles de decisión, puede ser usado con cualquier tipo de método. Bagging es un caso especial del promediado de modelos. Los métodos de bagging son métodos donde los algoritmos simples son usados en paralelo. El principal objetivo de los métodos en paralelo es el de aprovecharse de la independencia que hay entre los algoritmos simples, ya que el error se puede reducir bastante al promediar las salidas de los modelos simples. Es como si, queriendo resolver un problema entre varias personas independientes unas de otras, damos por bueno lo que eligiese la mayoría de las personas. Para obtener la agregación de las salidas de cada modelo simple e independiente, bagging puede usar la votación para los métodos de clasificación y el promedio para los métodos de regresión. El bagging o agregación bootstrap, es un método de aprendizaje por conjuntos que se usa comúnmente para reducir la varianza dentro de un conjunto de datos ruidoso. 8.5 Random Forest Un bosque aleatorio es un algoritmo de aprendizaje automático supervisado que se construye a partir de algoritmos de árbol de decisión. Este algoritmo se aplica en diversas industrias, como la banca y el comercio electrónico, para predecir el comportamiento y los resultados. En esta clase se dará una descripción general del algoritmo de bosque aleatorio, cómo funciona y las características del algoritmo. También se señalan las ventajas y desventajas de este algoritmo. 8.5.1 ¿Qué es? Un bosque aleatorio es una técnica de aprendizaje automático que se utiliza para resolver problemas de regresión y clasificación. Utiliza el aprendizaje por conjuntos, que es una técnica que combina muchos clasificadores para proporcionar soluciones a problemas complejos. Este algoritmo consta de muchos árboles de decisión. El “bosque” generado se entrena mediante agregación de bootstrap (bagging), el cual es es un meta-algoritmo de conjunto que mejora la precisión de los algoritmos de aprendizaje automático. El algoritmo establece el resultado en función de las predicciones de los árboles de decisión. Predice tomando el promedio o la media de la salida de varios árboles. El aumento del número de árboles aumenta la precisión del resultado. Un bosque aleatorio erradica las limitaciones de un algoritmo de árbol de decisión. Reduce el sobre-ajuste de conjuntos de datos y aumenta la precisión. Genera predicciones sin requerir muchas configuraciones. 8.5.2 Características de los bosques aleatorios Es más preciso que el algoritmo árbol de decisiones. Proporciona una forma eficaz de gestionar los datos faltantes. Puede producir una predicción razonable sin ajuste de hiperparámetros. Resuelve el problema del sobre-ajuste en los árboles de decisión. En cada árbol forestal aleatorio, se selecciona aleatoriamente un subconjunto de características en el punto de división del nodo. 8.5.3 Aplicar árboles de decisión en un bosque aleatorio La principal diferencia entre el algoritmo de árbol de decisión y el algoritmo de bosque aleatorio es que el establecimiento de nodos raíz y la desagregación de nodos se realiza de forma aleatoria en este último. El bosque aleatorio emplea el método de bagging para generar la predicción requerida. El método bagging implica el uso de diferentes muestras de datos (datos de entrenamiento) en lugar de una sola muestra. Los árboles de decisión producen diferentes resultados, dependiendo de los datos de entrenamiento alimentados al algoritmo de bosque aleatorio. Nuestro primer ejemplo todavía se puede utilizar para explicar cómo funcionan los bosques aleatorios. Supongamos que solo tenemos cuatro árboles de decisión. En este caso, los datos de entrenamiento que comprenden las observaciones y características de estudio se dividirán en cuatro nodos raíz. Supongamos que queremos modelar si un cliente compra o no compra un teléfono. Los nodos raíz podrían representar cuatro características que podrían influir en la elección de un cliente (precio, almacenamiento interno, cámara y RAM). El bosque aleatorio dividirá los nodos seleccionando características al azar. La predicción final se seleccionará en función del resultado de los cuatro árboles. El resultado elegido por la mayoría de los árboles de decisión será la elección final. Si tres árboles predicen la compra y un árbol predice que no comprará, entonces la predicción final será la compra. En este caso, se prevé que el cliente comprará. El siguiente diagrama muestra un clasificador de bosque aleatorio simple. 8.5.4 Ventajas y desventjas de bosques aleatorios Ventajas Puede realizar tareas de regresión y clasificación. Un bosque aleatorio produce buenas predicciones que se pueden entender fácilmente. Puede manejar grandes conjuntos de datos de manera eficiente. Proporciona un mayor nivel de precisión en la predicción de resultados sobre el algoritmo del árbol de decisión. Desventajas Cuando se usa un bosque aleatorio, se requieren bastantes recursos para el cálculo. Consume más tiempo en comparación con un algoritmo de árbol de decisiones. No producen buenos resultados cuando los datos son muy escasos. En este caso, el subconjunto de características y la muestra de arranque producirán un espacio invariante. Esto conducirá a divisiones improductivas, que afectarán el resultado. 8.6 Implementación en Python Usaremos los pipelines antes implementados para ajustar tanto el modelo de regresión como el de clasificación. Una ventaja que se explorará al usar KFCV es el conjunto de hiperparámetros para elegir el mejor modelo posible. Los pasos a seguir, son los siguientes: Carga de librerías Carga y separación inicial de datos ( test, train ). Pre-procesamiento e ingeniería de variables. Selección de tipo de modelo con hiperparámetros iniciales. Cálculo de métricas de desempeño. Creación de grid search y métricas de desempeño. Entrenamiento de modelos con hiperparámetros definidos. Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario). Selección de modelo a usar. Ajuste de modelo final con todos los datos. Validar poder predictivo con datos de prueba. 8.6.1 Regresión Paso 0: Carga de librerías from mlxtend.feature_selection import ColumnSelector from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestRegressor from sklearn.pipeline import Pipeline from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error from sklearn.metrics import mean_squared_error, r2_score, make_scorer from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate from sklearn.model_selection import GridSearchCV from sklearn.utils import shuffle from sklearn import set_config from plydata.one_table_verbs import pull from plydata.tidy import pivot_longer from mizani.formatters import comma_format, dollar_format from plotnine import * from siuba import * import pandas as pd import numpy as np import pickle Paso 1: Carga y separación inicial de datos ( test, train ) Comenzamos por cargar los datos completos e identificar la variable de respuesta para separarla de las explicativas #### CARGA DE DATOS #### ames = pd.read_csv(&quot;data/ames.csv&quot;) ames = (ames &gt;&gt; mutate( Second_Flr_SF = _.Second_Flr_SF.astype(float), Full_Bath = _.Full_Bath.astype(int), Half_Bath = _.Half_Bath.astype(int)) ) ames_y = ames &gt;&gt; pull(&quot;Sale_Price&quot;) # ames[[&quot;Sale_Price&quot;]] ames_x = select(ames, -_.Sale_Price) # ames.drop(&#39;Sale_Price&#39;, axis=1) #### DIVISIÓN DE DATOS #### ames_x_train, ames_x_test, ames_y_train, ames_y_test = train_test_split( ames_x, ames_y, train_size = 0.80, random_state = 195 ) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables En este paso se pone a prueba la imaginación y conocimiento de transformaciones estadísticas que permitan ofrecer un mejor ajuste a la relación entre datos dependientes y de respuesta. EL primer pipeline es para facilitar las transformaciones sobre las columnas sugeridas inicialmente: ## SELECCIÓN DE VARIABLES # Seleccionamos las variales numéricas de interés num_cols = [&quot;Full_Bath&quot;, &quot;Half_Bath&quot;, &quot;Second_Flr_SF&quot;] # Seleccionamos las variables categóricas de interés cat_cols = [&quot;Overall_Cond&quot;] # Juntamos todas las variables de interés columnas_seleccionadas = num_cols + cat_cols pipe = ColumnSelector(columnas_seleccionadas) ames_x_train_selected = pipe.fit_transform(ames_x_train) ames_train_selected = (pd.DataFrame( ames_x_train_selected, columns = columnas_seleccionadas) &gt;&gt; mutate( Second_Flr_SF = _.Second_Flr_SF.astype(float), Full_Bath = _.Full_Bath.astype(int), Half_Bath = _.Half_Bath.astype(int)) ) ames_train_selected.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2344 entries, 0 to 2343 ## Data columns (total 4 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Full_Bath 2344 non-null int64 ## 1 Half_Bath 2344 non-null int64 ## 2 Second_Flr_SF 2344 non-null float64 ## 3 Overall_Cond 2344 non-null object ## dtypes: float64(1), int64(2), object(1) ## memory usage: 73.4+ KB Y una vez que se han sugerido algunas columnas, se procede a realizar el pipeline de transformación: ## TRANSFORMACIÓN DE COLUMNAS # ColumnTransformer para aplicar transformaciones preprocessor = ColumnTransformer( transformers = [ (&#39;scaler&#39;, StandardScaler(), num_cols), (&#39;onehotencoding&#39;, OneHotEncoder(drop=&#39;first&#39;, sparse_output=False), cat_cols) ], verbose_feature_names_out = False, remainder = &#39;passthrough&#39; # Mantener las columnas restantes sin cambios ) transformed_data = preprocessor.fit_transform(ames_train_selected) new_column_names = preprocessor.get_feature_names_out() transformed_df = pd.DataFrame( transformed_data, columns=new_column_names ) transformed_df ## Full_Bath Half_Bath Second_Flr_SF Overall_Cond_Average \\ ## 0 0.78 1.24 0.68 1.00 ## 1 0.78 -0.75 -0.78 1.00 ## 2 0.78 -0.75 0.73 1.00 ## 3 -1.01 -0.75 -0.78 1.00 ## 4 -1.01 1.24 -0.78 0.00 ## ... ... ... ... ... ## 2339 -1.01 1.24 0.50 0.00 ## 2340 0.78 -0.75 -0.78 1.00 ## 2341 -1.01 -0.75 -0.78 1.00 ## 2342 0.78 1.24 1.38 1.00 ## 2343 0.78 -0.75 1.36 1.00 ## ## Overall_Cond_Below_Average Overall_Cond_Excellent Overall_Cond_Fair \\ ## 0 0.00 0.00 0.00 ## 1 0.00 0.00 0.00 ## 2 0.00 0.00 0.00 ## 3 0.00 0.00 0.00 ## 4 0.00 0.00 0.00 ## ... ... ... ... ## 2339 1.00 0.00 0.00 ## 2340 0.00 0.00 0.00 ## 2341 0.00 0.00 0.00 ## 2342 0.00 0.00 0.00 ## 2343 0.00 0.00 0.00 ## ## Overall_Cond_Good Overall_Cond_Poor Overall_Cond_Very_Good \\ ## 0 0.00 0.00 0.00 ## 1 0.00 0.00 0.00 ## 2 0.00 0.00 0.00 ## 3 0.00 0.00 0.00 ## 4 1.00 0.00 0.00 ## ... ... ... ... ## 2339 0.00 0.00 0.00 ## 2340 0.00 0.00 0.00 ## 2341 0.00 0.00 0.00 ## 2342 0.00 0.00 0.00 ## 2343 0.00 0.00 0.00 ## ## Overall_Cond_Very_Poor ## 0 0.00 ## 1 0.00 ## 2 0.00 ## 3 0.00 ## 4 0.00 ## ... ... ## 2339 0.00 ## 2340 0.00 ## 2341 0.00 ## 2342 0.00 ## 2343 0.00 ## ## [2344 rows x 11 columns] transformed_df.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 2344 entries, 0 to 2343 ## Data columns (total 11 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Full_Bath 2344 non-null float64 ## 1 Half_Bath 2344 non-null float64 ## 2 Second_Flr_SF 2344 non-null float64 ## 3 Overall_Cond_Average 2344 non-null float64 ## 4 Overall_Cond_Below_Average 2344 non-null float64 ## 5 Overall_Cond_Excellent 2344 non-null float64 ## 6 Overall_Cond_Fair 2344 non-null float64 ## 7 Overall_Cond_Good 2344 non-null float64 ## 8 Overall_Cond_Poor 2344 non-null float64 ## 9 Overall_Cond_Very_Good 2344 non-null float64 ## 10 Overall_Cond_Very_Poor 2344 non-null float64 ## dtypes: float64(11) ## memory usage: 201.6 KB Recordemos que la función ColumnTransformes() solo son los pasos a seguir, necesitamos usar el método fit() que nos devuelve una receta actualizada con las estimaciones y la función transform() que nos devuelve la matriz transformada. Estos pasos pueden resumirse con el método: fit_transform() Una vez que la receta de transformación de datos está lista, procedemos a implementar el pipeline del modelo de interés. Paso 3: Selección de tipo de modelo con hiperparámetros iniciales Ahora se procede a unir en un mismo flujo el proceso de ingeniería de datos y un modelo inicial sugerido. En este primer ejemplo, se muestra un modelo de 5 vecinos más cercanos. # Crear el pipeline con la regresión por Random Forest pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, RandomForestRegressor( n_estimators=10, min_samples_split=2, min_samples_leaf=2, random_state=12345)) ]) # Entrenar el pipeline results = pipeline.fit(ames_train_selected, ames_y_train) ## PREDICCIONES y_pred = pipeline.predict(ames_x_test) ames_test = ( ames_x_test &gt;&gt; mutate(Sale_Price_Pred = y_pred, Sale_Price = ames_y_test) ) ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) ) ## Sale_Price Sale_Price_Pred ## 390 165000 224063.10 ## 1235 124000 195821.07 ## 2288 75000 91954.24 ## 107 206000 132702.51 ## 1861 190000 192461.90 ## ... ... ... ## 116 171000 154051.67 ## 398 120500 132702.51 ## 1253 146000 224063.10 ## 78 125000 130667.37 ## 714 110000 125676.00 ## ## [586 rows x 2 columns] Estas son las predicciones logradas con el modelo inicial. 8.6.1.1 Métricas de desempeño Se procede en el siguiente paso a cuantificar los errores producidos por la predicción. Paso 4: Cálculo de métricas de desempeño pd.options.display.float_format = &#39;{:.2f}&#39;.format y_obs = ames_test[&quot;Sale_Price&quot;] y_pred = ames_test[&quot;Sale_Price_Pred&quot;] me = np.mean(y_obs - y_pred) mae = mean_absolute_error(y_obs, y_pred) mape = mean_absolute_percentage_error(y_obs, y_pred) mse = mean_squared_error(y_obs, y_pred) rmse = np.sqrt(mse) r2 = r2_score(y_obs, y_pred) n = len(y_obs) # Número de observaciones p = 11 # Número de predictores r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2) metrics_data = { &quot;Metric&quot;: [&quot;ME&quot;, &quot;MAE&quot;, &quot;MAPE&quot;, &quot;MSE&quot;, &quot;RMSE&quot;, &quot;R^2&quot;, &quot;R^2 Adj&quot;], &quot;Value&quot;: [me, mae, mape, mse, rmse, r2, r2_adj] } metrics_df = pd.DataFrame(metrics_data) metrics_df ## Metric Value ## 0 ME -326.52 ## 1 MAE 38207.14 ## 2 MAPE 0.22 ## 3 MSE 3298090728.47 ## 4 RMSE 57429.01 ## 5 R^2 0.50 ## 6 R^2 Adj 0.49 Una manera amigable de dimensionar los errores y el buen desempeño es mediante gráficos #### Gráficos de desempeño de modelo ( ames_test &gt;&gt; ggplot(aes(x = &quot;Sale_Price&quot;, y = &quot;Sale_Price_Pred&quot;)) + geom_point() + scale_y_continuous(labels = dollar_format(digits=0, big_mark=&#39;,&#39;), limits = [0, 600000] ) + scale_x_continuous(labels = dollar_format(digits=0, big_mark=&#39;,&#39;), limits = [0, 500000] ) + geom_abline(color = &quot;red&quot;) + coord_equal() + labs( title = &quot;Comparación entre predicción y observación&quot;, y = &quot;Predicción&quot;, x = &quot;Observación&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(x = &quot;error&quot;)) + geom_histogram(color = &quot;white&quot;, fill = &quot;black&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + scale_x_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + ylab(&quot;Conteos de clase&quot;) + xlab(&quot;Errores&quot;) + ggtitle(&quot;Distribución de error&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(sample = &quot;error&quot;)) + geom_qq(alpha = 0.3) + stat_qq_line(color = &quot;red&quot;) + scale_y_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits = 0)) + xlab(&quot;Distribución normal&quot;) + ylab(&quot;Distribución de errores&quot;) + ggtitle(&quot;QQ-Plot&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; ( ames_test &gt;&gt; select(_.Sale_Price, _.Sale_Price_Pred) &gt;&gt; mutate(error = _.Sale_Price - _.Sale_Price_Pred) &gt;&gt; ggplot(aes(x = &quot;Sale_Price&quot;)) + geom_linerange(aes(ymin = 0, ymax = &quot;error&quot;), colour = &quot;purple&quot;) + geom_point(aes(y = &quot;error&quot;), size = 0.05, alpha = 0.5) + geom_abline(intercept = 0, slope = 0) + scale_x_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + scale_y_continuous(labels=dollar_format(big_mark=&#39;,&#39;, digits=0)) + xlab(&quot;Precio real&quot;) + ylab(&quot;Error de estimación&quot;) + ggtitle(&quot;Relación entre error y precio de venta&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; 8.6.1.2 Validación cruzada Para determinar cuáles son los hiper-parámetros que funcionan mejor, es necesario realizar experimentos mediante ensayo-error hasta determinar la mejor solución. En cada partición del método de muestreo KFCV se implementan las distintas configuraciones y se calculan predicciones. Con las predicciones hechas en cada fold, se obtienen intervalos de confianza para conocer la variación asociada al modelo a través de los hiper-parámetros implementados. Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo, sin embargo, para realizar este proceso de forma ágil, se inicializará un flujo de trabajo que se encargue de realizar todos los experimentos deseados y elegir el modelo adecuado. Paso 5: Creación de grid search y métricas de desempeño # Definir el objeto K-Fold Cross Validator k = 5 kf = KFold(n_splits=k, shuffle=True, random_state=42) param_grid = { &#39;max_depth&#39;: range(2, 5), &#39;min_samples_split&#39;: range(2, 8), &#39;min_samples_leaf&#39;: range(2, 8), &#39;max_features&#39;: range(1, 5) } Una vez definidos los posibles hiperparámetros, procedemos a definir las métricas que serán usadas para cuantificar la bondad del ajuste. # Definir las métricas de desempeño que deseas calcular como funciones de puntuación def adjusted_r2_score(y_true, y_pred, n, p): r2 = r2_score(y_true, y_pred) adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) return adjusted_r2 scoring = { &#39;neg_mean_squared_error&#39;: make_scorer(mean_squared_error, greater_is_better=False), &#39;r2&#39;: make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_train_selected)), p=len(ames_train_selected.columns)), &#39;neg_mean_absolute_error&#39;: make_scorer(mean_absolute_error, greater_is_better=False), &#39;mape&#39;: make_scorer(mean_absolute_percentage_error, greater_is_better=False) } Paso 6: Entrenamiento de modelos con hiperparámetros definidos Teniendo todos los elementos anteriores listos, se procede con el ajuste de todas las posibles configuraciones del modelo a través de la validación cruzada. Esto permitirá contar con medidas de tendencia central para los resultados de cada uno de las configuraciones y evaluar si estadísticamente hay diferencia entre los mejores resultados. pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, GridSearchCV( RandomForestRegressor(), param_grid = param_grid, cv=kf, scoring=scoring, refit=&#39;neg_mean_squared_error&#39;, verbose=2, n_jobs=7, error_score=&#39;raise&#39;) ) ]) pipeline.fit(ames_train_selected, ames_y_train) pickle.dump(pipeline, open(&#39;models/grid_search_random_forest.pkl&#39;, &#39;wb&#39;)) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) Podemos obtener las métricas de los resultados de cada fold: pipeline = pickle.load(open(&#39;models/grid_search_random_forest.pkl&#39;, &#39;rb&#39;)) results_cv = pipeline.named_steps[&#39;regressor&#39;].cv_results_ # Convierte los resultados en un DataFrame pd.set_option(&#39;display.max_columns&#39;, 500) results_df = pd.DataFrame(results_cv) results_df.columns ## Index([&#39;mean_fit_time&#39;, &#39;std_fit_time&#39;, &#39;mean_score_time&#39;, &#39;std_score_time&#39;, ## &#39;param_max_depth&#39;, &#39;param_max_features&#39;, &#39;param_min_samples_leaf&#39;, ## &#39;param_min_samples_split&#39;, &#39;params&#39;, ## &#39;split0_test_neg_mean_squared_error&#39;, ## &#39;split1_test_neg_mean_squared_error&#39;, ## &#39;split2_test_neg_mean_squared_error&#39;, ## &#39;split3_test_neg_mean_squared_error&#39;, ## &#39;split4_test_neg_mean_squared_error&#39;, ## &#39;mean_test_neg_mean_squared_error&#39;, &#39;std_test_neg_mean_squared_error&#39;, ## &#39;rank_test_neg_mean_squared_error&#39;, &#39;split0_test_r2&#39;, &#39;split1_test_r2&#39;, ## &#39;split2_test_r2&#39;, &#39;split3_test_r2&#39;, &#39;split4_test_r2&#39;, &#39;mean_test_r2&#39;, ## &#39;std_test_r2&#39;, &#39;rank_test_r2&#39;, &#39;split0_test_neg_mean_absolute_error&#39;, ## &#39;split1_test_neg_mean_absolute_error&#39;, ## &#39;split2_test_neg_mean_absolute_error&#39;, ## &#39;split3_test_neg_mean_absolute_error&#39;, ## &#39;split4_test_neg_mean_absolute_error&#39;, ## &#39;mean_test_neg_mean_absolute_error&#39;, &#39;std_test_neg_mean_absolute_error&#39;, ## &#39;rank_test_neg_mean_absolute_error&#39;, &#39;split0_test_mape&#39;, ## &#39;split1_test_mape&#39;, &#39;split2_test_mape&#39;, &#39;split3_test_mape&#39;, ## &#39;split4_test_mape&#39;, &#39;mean_test_mape&#39;, &#39;std_test_mape&#39;, ## &#39;rank_test_mape&#39;], ## dtype=&#39;object&#39;) # Puedes seleccionar las columnas de interés, por ejemplo: summary_df = ( results_df &gt;&gt; select(-_.contains(&quot;split._&quot;), -_.contains(&quot;time&quot;), -_.params) ) summary_df ## param_max_depth param_max_features param_min_samples_leaf \\ ## 0 2 1 2 ## 1 2 1 2 ## 2 2 1 2 ## 3 2 1 2 ## 4 2 1 2 ## .. ... ... ... ## 427 4 4 7 ## 428 4 4 7 ## 429 4 4 7 ## 430 4 4 7 ## 431 4 4 7 ## ## param_min_samples_split mean_test_neg_mean_squared_error \\ ## 0 2 -4970280637.82 ## 1 3 -4974587424.48 ## 2 4 -5005446361.21 ## 3 5 -5019964053.35 ## 4 6 -5034494921.64 ## .. ... ... ## 427 3 -3576638787.58 ## 428 4 -3563145452.88 ## 429 5 -3584718778.74 ## 430 6 -3599654298.71 ## 431 7 -3583862726.28 ## ## std_test_neg_mean_squared_error rank_test_neg_mean_squared_error \\ ## 0 502974264.93 398 ## 1 520187648.69 399 ## 2 596477224.42 402 ## 3 475974437.54 408 ## 4 462785558.52 409 ## .. ... ... ## 427 416707863.11 30 ## 428 410466613.96 19 ## 429 415571196.41 37 ## 430 437276292.66 42 ## 431 416165232.48 35 ## ## mean_test_r2 std_test_r2 rank_test_r2 \\ ## 0 0.21 0.02 398 ## 1 0.21 0.02 399 ## 2 0.21 0.03 402 ## 3 0.21 0.02 408 ## 4 0.20 0.01 410 ## .. ... ... ... ## 427 0.44 0.03 27 ## 428 0.44 0.03 19 ## 429 0.43 0.03 37 ## 430 0.43 0.03 42 ## 431 0.43 0.03 35 ## ## mean_test_neg_mean_absolute_error std_test_neg_mean_absolute_error \\ ## 0 -49246.68 1474.96 ## 1 -49247.84 993.79 ## 2 -49481.49 1732.09 ## 3 -49624.64 814.17 ## 4 -49623.45 724.66 ## .. ... ... ## 427 -39875.82 1197.55 ## 428 -39734.62 1124.42 ## 429 -39856.54 1130.38 ## 430 -39917.63 1252.06 ## 431 -39806.33 1282.96 ## ## rank_test_neg_mean_absolute_error mean_test_mape std_test_mape \\ ## 0 398 -0.31 0.02 ## 1 399 -0.31 0.01 ## 2 404 -0.32 0.01 ## 3 409 -0.32 0.01 ## 4 408 -0.32 0.01 ## .. ... ... ... ## 427 29 -0.24 0.01 ## 428 6 -0.24 0.01 ## 429 26 -0.24 0.01 ## 430 33 -0.24 0.01 ## 431 14 -0.24 0.01 ## ## rank_test_mape ## 0 399 ## 1 398 ## 2 402 ## 3 406 ## 4 409 ## .. ... ## 427 27 ## 428 6 ## 429 28 ## 430 36 ## 431 20 ## ## [432 rows x 16 columns] En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: ( summary_df &gt;&gt; ggplot(aes(x = &quot;param_max_features&quot;, y = &quot;mean_test_r2&quot;, fill = &quot;param_max_depth&quot;)) + geom_point() + ggtitle(&quot;Parametrización de Random Forest vs R^2&quot;) + xlab(&quot;Parámetro: Número de features por árbol&quot;) + ylab(&quot;R^2 promedio&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; Seleccionando la mejor parametrización, se puede definir la siguiente gráfica para comparar la diferencia entre hiperparámetros a un nivel más granular. ( summary_df &gt;&gt; select(_.param_max_depth, _.param_max_features, _.param_min_samples_leaf, _.param_min_samples_split, _.mean_test_r2) &gt;&gt; pivot_longer( cols = [&quot;param_max_depth&quot;, &quot;param_max_features&quot;, &quot;param_min_samples_leaf&quot;, &quot;param_min_samples_split&quot;], names_to=&quot;parameter&quot;, values_to=&quot;value&quot;) &gt;&gt; ggplot(aes(x = &quot;value&quot;, y = &quot;mean_test_r2&quot;)) + geom_point(size = 1, ) + facet_wrap(&quot;~parameter&quot;, scales = &quot;free_x&quot;) + xlab(&quot;Parameter value&quot;) + ylab(&quot;R^2 promedio&quot;) + ggtitle(&quot;Parametrización de Random Forest vs R^2&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; Paso 8: Selección de modelo a usar Habiendo realizado un análisis de los hiperparámetros, se procede a elegir el mejor modelo. Esto a veces a muy evidente y otras no. En cualquier caso, puede automatizarse la extracción del mejor modelo: best_params = pipeline.named_steps[&#39;regressor&#39;].best_params_ best_params ## {&#39;max_depth&#39;: 4, &#39;max_features&#39;: 4, &#39;min_samples_leaf&#39;: 2, &#39;min_samples_split&#39;: 5} Sabiendo cuáles son los mejores hiperparámetros, se procede a extraer el modelo que DEBEREMOS AJUSTAR A TODOS LOS DATOS DE ENTRENAMIENTO. best_estimator = pipeline.named_steps[&#39;regressor&#39;].best_estimator_ best_estimator ## RandomForestRegressor(max_depth=4, max_features=4, min_samples_leaf=2, ## min_samples_split=5) Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) Ahora obtendremos el modelo que mejor desempeño tiene y haremos las predicciones del conjunto de prueba con este modelo. Es importante volver a hacer el ajuste con el modelo elegido. final_rf_pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, best_estimator) ]) # Entrenar el pipeline final_rf_pipeline.fit(ames_train_selected, ames_y_train) ## Pipeline(steps=[(&#39;preprocessor&#39;, ## ColumnTransformer(remainder=&#39;passthrough&#39;, ## transformers=[(&#39;scaler&#39;, StandardScaler(), ## [&#39;Full_Bath&#39;, &#39;Half_Bath&#39;, ## &#39;Second_Flr_SF&#39;]), ## (&#39;onehotencoding&#39;, ## OneHotEncoder(drop=&#39;first&#39;, ## sparse_output=False), ## [&#39;Overall_Cond&#39;])], ## verbose_feature_names_out=False)), ## (&#39;regressor&#39;, ## RandomForestRegressor(max_depth=4, max_features=4, ## min_samples_leaf=2, ## min_samples_split=5))]) Este último objeto creado es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción, sólo se necesita de los nuevos datos y de este último elemento para poder realizar nuevas predicciones. Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: ## Predicciones finales y_pred_rf = final_rf_pipeline.predict(ames_x_test) results_reg = ( ames_x_test &gt;&gt; mutate(final_rf_pred = y_pred_rf, Sale_Price = ames_y_test) &gt;&gt; select(_.Sale_Price, _.final_rf_pred) ) results_reg ## Sale_Price final_rf_pred ## 390 165000 216723.87 ## 1235 124000 166863.81 ## 2288 75000 157479.80 ## 107 206000 131379.70 ## 1861 190000 254600.78 ## ... ... ... ## 116 171000 154932.83 ## 398 120500 131379.70 ## 1253 146000 216723.87 ## 78 125000 135138.56 ## 714 110000 143666.50 ## ## [586 rows x 2 columns] Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paquetería MLmetrics. Es posible definir nuestro propio conjunto de métricas que deseamos reportar creando el objeto metric_set: me = np.mean(y_obs - y_pred_rf) mae = mean_absolute_error(y_obs, y_pred_rf) mape = mean_absolute_percentage_error(y_obs, y_pred_rf) mse = mean_squared_error(y_obs, y_pred_rf) rmse = np.sqrt(mse) r2 = r2_score(y_obs, y_pred_rf) r2_adj = adjusted_r2_score(y_true = y_obs, y_pred = y_pred_rf, n=np.ceil(len(ames_train_selected)), p=len(ames_train_selected.columns)) metrics_data = { &quot;Metric&quot;: [&quot;ME&quot;, &quot;MAE&quot;, &quot;MAPE&quot;, &quot;MSE&quot;, &quot;RMSE&quot;, &quot;R^2&quot;, &quot;R^2 Adj&quot;], &quot;Value&quot;: [me, mae, mape, mse, rmse, r2, r2_adj] } metrics_df = pd.DataFrame(metrics_data) metrics_df ## Metric Value ## 0 ME -709.41 ## 1 MAE 41978.16 ## 2 MAPE 0.25 ## 3 MSE 3819043820.82 ## 4 RMSE 61798.41 ## 5 R^2 0.42 ## 6 R^2 Adj 0.42 ( results_reg &gt;&gt; ggplot(aes(x = &quot;final_rf_pred&quot;, y = &quot;Sale_Price&quot;)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) ) ## &lt;Figure Size: (1280 x 960)&gt; 8.6.1.3 Importancia de variables Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia de cada variable en el modelo. importance = np.zeros(ames_x_test[columnas_seleccionadas].shape[1]) # Realiza el procedimiento de permutación for i in range(ames_x_test[columnas_seleccionadas].shape[1]): ames_x_test_permuted = ames_x_test[columnas_seleccionadas].copy() ames_x_test_permuted.iloc[:, i] = shuffle(ames_x_test_permuted.iloc[:, i], random_state=42) # Permuta una característica y_pred_permuted = final_rf_pipeline.predict(ames_x_test_permuted) mse_permuted = mean_squared_error(ames_y_test, y_pred_permuted) importance[i] = mse - mse_permuted # Calcula la importancia relativa importance = importance / importance.sum() importance ## array([0.58905059, 0.1001707 , 0.20209608, 0.10868263]) importance_df = pd.DataFrame({ &#39;Variable&#39;: columnas_seleccionadas, &#39;Importance&#39;: importance }) # Crea la gráfica de barras ( importance_df &gt;&gt; ggplot(aes(x= &#39;reorder(Variable, Importance)&#39;, y=&#39;Importance&#39;)) + geom_bar(stat=&#39;identity&#39;, fill=&#39;blue&#39;, color = &quot;black&quot;) + labs(title=&#39;Importancia de las Variables&#39;, x=&#39;Variable&#39;, y=&#39;Importancia&#39;) + coord_flip() ) ## &lt;Figure Size: (1280 x 960)&gt; La gráfica anterior muestra la importancia de una variable cuando se lleva a cabo una permutación. Dado que este resultado fue aleatorio, resulta vital contar con un conjunto de permutaciones que permitan conocer la caída promedio y desviación esperada en el desempeño cuando se elimina una variable o esta se vuelve inservible. n_permutations = 50 performance_losses = [] for i in range(ames_x_test[columnas_seleccionadas].shape[1]): loss = [] for j in range(n_permutations): ames_x_test_permuted = ames_x_test[columnas_seleccionadas].copy() ames_x_test_permuted.iloc[:, i] = np.random.permutation(ames_x_test_permuted.iloc[:, i]) y_pred_permuted = final_rf_pipeline.predict(ames_x_test_permuted) mse_permuted = mean_squared_error(ames_y_test, y_pred_permuted) loss.append(mse_permuted) performance_losses.append(loss) performance_losses = performance_losses/np.sum(performance_losses, axis=0) mean_losses = np.mean(performance_losses, axis=1) std_losses = np.std(performance_losses, axis=1) importance_df = pd.DataFrame({ &#39;Variable&#39;: columnas_seleccionadas, &#39;Mean_Loss&#39;: mean_losses, &#39;Std_Loss&#39;: std_losses }) ( importance_df &gt;&gt; mutate( ymin = _.Mean_Loss - _.Std_Loss, ymax = _.Mean_Loss + _.Std_Loss) &gt;&gt; ggplot(aes(x = &#39;reorder(Variable, Mean_Loss)&#39;, y = &quot;Mean_Loss&quot;)) + geom_errorbar(aes(ymin=&#39;ymin&#39;, ymax=&#39;ymax&#39;), width=0.2, position=position_dodge(0.9)) + geom_point(alpha = 0.65) + labs(title=&#39;Importancia de las Variables&#39;, x=&#39;Variable&#39;, y=&#39;Importancia&#39;) + coord_flip() ) ## &lt;Figure Size: (1280 x 960)&gt; importances = final_rf_pipeline.named_steps[&#39;regressor&#39;].feature_importances_ columns = final_rf_pipeline[&quot;preprocessor&quot;].get_feature_names_out() ( pd.DataFrame({&#39;feature&#39;: columns, &#39;Importance&#39;: importances}) &gt;&gt; ggplot(aes(x= &#39;reorder(feature, Importance)&#39;, y=&#39;Importance&#39;)) + geom_bar(stat=&#39;identity&#39;, fill=&#39;blue&#39;, color = &quot;black&quot;) + labs(title=&#39;Importancia de las Variables&#39;, x=&#39;Variable&#39;, y=&#39;Importancia&#39;) + coord_flip() ) ## &lt;Figure Size: (1280 x 960)&gt; Si en la gráfica anterior notamos algo raro en cuanto a la(s) variable(s) más importante(s) y la factibilidad de conseguirla(s) o usarla(s)… ¡¡HAY QUE EMPEZAR DESDE CERO SIN CONSIDERAR ESA VARIABLE!! "],["clustering-no-jerárquico.html", "Capítulo 9 Clustering No Jerárquico 9.1 Cálculo de distancia 9.2 Tendencia de factibilidad 9.3 K - means", " Capítulo 9 Clustering No Jerárquico 9.1 Cálculo de distancia Otro parámetro que podemos ajustar para el modelo es la distancia usada, existen diferentes formas de medir qué tan “cerca” están dos puntos entre sí, y las diferencias entre estos métodos pueden volverse significativas en dimensiones superiores. La más utilizada es la distancia euclidiana, el tipo estándar de distancia. \\[d(X,Y) = \\sqrt{\\sum_{i=1}^{n} (x_i-y_i)^2}\\] Otra métrica es la llamada distancia de Manhattan, que mide la distancia tomada en cada dirección cardinal, en lugar de a lo largo de la diagonal. \\[d(X,Y) = \\sum_{i=1}^{n} |x_i - y_i|\\] De manera más general, las anteriores son casos particulares de la distancia de Minkowski, cuya fórmula es: \\[d(X,Y) = (\\sum_{i=1}^{n} |x_i-y_i|^p)^{\\frac{1}{p}}\\] La distancia de coseno es ampliamente en análisis de texto, sistemas de recomendación. \\[d(X,Y)= 1 - \\frac{\\sum_{i=1}^{n}{X_iY_i}}{\\sqrt{\\sum_{i=1}^{n}{X_i^2}}\\sqrt{\\sum_{i=1}^{n}{Y_i^2}}}\\] La distancia de Jaccard es ampliamente usada para medir similitud cuando se trata de variables categóricas. Es usado en análisis de texto y sistemas de recomendación. \\[d(X, Y) = \\frac{X \\cap Y}{X \\cup Y}\\] La distancia de Gower´s mide la similitud entre variables de forma distinta dependiendo del tipo de dato (numérica, nominal, ordinal). \\[D_{Gower}(X_1, X_2) = 1 - \\frac{1}{p} \\sum_{j=1}^{p}{s_j(X_1, X_2)} ; \\quad \\quad s_j(X_1, X_2)=1-\\frac{|y_{1j}-y_{2j}|}{R_j} \\] Para mayor detalle sobre el funcionamiento de la métrica, revisar el siguiente link Un link interesante Otro link interesante 9.1.1 Distancias homogéneas Las distancias basadas en la correlación son ampliamente usadas en múltiples análisis. Esta medida puede calcularse mediante pearson, spearman o kendall. from sklearn.preprocessing import scale from sklearn.metrics import pairwise_distances from sklearn.datasets import load_iris from pyclustertend import vat, ivat, hopkins # from github git@github.com:lachhebo/pyclustertend.git import pandas as pd import numpy as np import matplotlib.pyplot as plt USArrests = pd.read_csv(&quot;data/USArrests.csv&quot;, index_col=0) # Escalar los datos USArrests_scaled = scale(USArrests) dist_cor = pairwise_distances(USArrests_scaled, metric=&#39;correlation&#39;) # Imprimir la matriz de distancias print(np.round(dist_cor[0:7, 0:7], 1)) ## [[0. 0.7 1.4 0.1 1.9 1.7 1.7] ## [0.7 0. 0.8 0.4 0.8 0.5 1.9] ## [1.4 0.8 0. 1.2 0.3 0.6 0.8] ## [0.1 0.4 1.2 0. 1.6 1.4 1.9] ## [1.9 0.8 0.3 1.6 0. 0.1 0.7] ## [1.7 0.5 0.6 1.4 0.1 0. 1. ] ## [1.7 1.9 0.8 1.9 0.7 1. 0. ]] 9.1.2 Visualización de distancias En cualquier análisis, es de gran valor contar con un gráfico que permita conocer de manera práctica y simple el resumen de distancias. Un mapa de calor es una solución bastante útil, el cual representará de en una escala de color a los elementos cerca y lejos. plt.clf() plt.figure(figsize=(4, 4)) ## &lt;Figure size 800x800 with 0 Axes&gt; vat(dist_cor, figure_size = (4, 4)) plt.title(&#39;Gráfico VAT (Visual Assessment of Cluster Tendency)&#39;) ## Text(0.5, 1.0, &#39;Gráfico VAT (Visual Assessment of Cluster Tendency)&#39;) plt.show() plt.clf() ivat(dist_cor, figure_size = (4, 4)) plt.title(&#39;Gráfico VAT mejorado&#39;) ## Text(0.5, 1.0, &#39;Gráfico VAT mejorado&#39;) plt.show() La intensidad del color es proporcional al valor de similaridad entre observaciones. Cuando la distancia es cero, el color es negro y cuando la distancia es amplia, el color es blanco. Los elementos que pertenecen a un mismo cluster se muestran en orden consecutivo. 9.2 Tendencia de factibilidad Este análisis es considerado como la evaluación de factibilidad de implementar análisis de clustering. Antes de aplicar cualquier técnica de clustering vale la pena evaluar si el conjunto de datos contiene clusters naturales significativos (i.e. estructuras no aleatorias) o no. En caso de que sí existan estructuras conglomeradas naturales, se deberá proceder a identificar el número de clusters a extraer. A diferencia de otros tipos de análisis, una desventaja que tiene el análisis de clustering es que en todo momento regresarán clusters incluso cuando los datos no contengan tal estructura, por lo que si ciegamente se implementa un método de clustering, este dividirá los datos en clusters debido a que es lo esperado a realizar. iris = pd.DataFrame(load_iris().data, columns=load_iris().feature_names) # Generar un DataFrame aleatorio con las mismas dimensiones que iris random_df = pd.DataFrame( np.random.uniform( iris.min(axis=0), iris.max(axis=0), size=iris.shape), columns=load_iris().feature_names ) df = iris.copy() Se comienza con una evaluación visual sobre los datos para evaluar la significancia de los clusters. Debido a que los más probable es que los datos tengan más de dos dimensiones, se aprovecha el análisis de componentes principales para representar los datos en un espacio de dimensión menor. import patchworklib as pw from plotnine import * from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def plot_pca(data, title): # Escala los datos utilizando StandardScaler scaler = StandardScaler() data_scaled = scaler.fit_transform(data) # Realiza el análisis de componentes principales (PCA) pca = PCA(n_components=2) principal_components = pca.fit_transform(data_scaled) df_pca = pd.DataFrame(data=principal_components, columns=[&#39;PC1&#39;, &#39;PC2&#39;]) return( ggplot(df_pca, aes(x=&#39;PC1&#39;, y=&#39;PC2&#39;)) + geom_point() + labs(title=title) + theme_classic() ) iris_plot = plot_pca(df, &quot;PCA - Iris data&quot;) random_plot = plot_pca(random_df, &quot;PCA - Random data&quot;) iris_plot2 = pw.load_ggplot(iris_plot, figsize=(4,4)) random_plot2 = pw.load_ggplot(random_plot, figsize=(4,4)) iris_plot_vs_random_plot = (iris_plot2|random_plot2) iris_plot_vs_random_plot.savefig(&quot;img/09-not-hclus/iris_plot_vs_random_plot2.png&quot;) Puede observarse en el primer gráfico que, al menos existen 2 clusters significativos, con posibilidad de que sean 3. A diferencia del gráfico de la derecha que no muestra una tendencia en la estructura conglomerativa. Es sumamente importante realizar esta evaluación porque from sklearn.cluster import KMeans from scipy.cluster.hierarchy import dendrogram, linkage import seaborn as sns import matplotlib.pyplot as plt scaler = StandardScaler() random_df_scaled = pd.DataFrame( data = scaler.fit_transform(random_df), columns = random_df.columns).iloc[:,:2] # K-means clustering kmeans = KMeans(n_clusters=3, random_state=12345) km_res2 = kmeans.fit_predict(random_df_scaled) # Crear un DataFrame con los resultados del clustering random_df_scaled[&#39;cluster&#39;] = km_res2 # Elegir una paleta de colores con tres colores contrastantes palette = sns.color_palette(&quot;husl&quot;, 3) # Gráfico de puntos coloreados por clúster utilizando seaborn plt.clf() # Scatter plot con tres colores contrastantes plt.subplot(1, 2, 1) ## &lt;Axes: &gt; sns.scatterplot( data=random_df_scaled, x=&#39;sepal length (cm)&#39;, y=&#39;sepal width (cm)&#39;, hue=&#39;cluster&#39;, palette=palette ) ## &lt;Axes: xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt; plt.title(&quot;K-Means Clustering&quot;) ## Text(0.5, 1.0, &#39;K-Means Clustering&#39;) # Dendrograma jerárquico utilizando scipy plt.subplot(1, 2, 2) ## &lt;Axes: &gt; linkage_matrix = linkage(random_df_scaled, method=&#39;ward&#39;) dendro = dendrogram(linkage_matrix, no_labels=True) plt.title(&quot;Dendrograma&quot;) ## Text(0.5, 1.0, &#39;Dendrograma&#39;) # Ajustar el diseño y mostrar la figura plt.tight_layout() plt.show() Puede observarse que ambos métodos imponen una segmentación a los datos que son uniformemente aleatorios y que no contienen ninguna segmentación natural. Por esta razón, siempre deberá realizarse este análisis previamente y elegir si se desea proceder con el análisis. El método anterior fue totalmente gráfico. Se procede a continuación a mostrar una metodología estadística para determinar la factibilidad de implementar análisis de clustering. El estadístico Hopkins es usado para evaluar la tendencia de clustering en un conjunto de datos. Mide la probabilidad de que un conjunto de datos dado sea generado por una distribución uniforme. En otras palabras, prueba la aleatoriedad espacial de los datos. Por ejemplo, Sea D un conjunto de datos real, el estadístico de Hopkins puede ser calculado de la siguiente forma: Proceso: Muestrear aleatoriamente n puntos \\((p_1, p_2, p_3, ..., p_n)\\) de D Para cada punto \\(p_i \\in D\\), encontrar su vecino más cercano \\(p_j\\); luego calcular la distancia entre \\(p_i\\) y \\(p_j\\) y denotarla como \\(x_i = dist(p_i, p_j)\\) Generar un conjunto de datos simulado \\((random_D)\\) tomado de una distribución uniformemente aleatoria con n puntos \\((q_1, q_2, q_3, ..., q_n)\\) y de la misma variación que la original del conjunto D. Para cada punto \\(q_i \\in random_D\\), encontrar su vecino más cercano \\(q_j\\) en D; posteriormente, calcular la distancia entre \\(q_i\\) y \\(q_j\\) y denotarla como \\(y_i=dist(q_i, q_j)\\). Calcular el estadístico de Hopkins como la distancia media más cercana de vecinos en los datos aleatorios y dividirlos por la suma de las distancias medias de vecinos más cercanos de los datos reales y aleatorios: \\[H=\\frac{\\sum_{i=1}^{n}{y_i}}{\\sum_{i=1}^{n}{x_i} + \\sum_{i=1}^{n}{y_i}}\\] Un valor cercano a 0.5 significa que \\(\\sum_{i=1}^{n}{y_i}\\) y \\(\\sum_{i=1}^{n}{x_i}\\) son similares uno del otro y por lo tanto, D es distribuida aleatoriamente. Por lo tanto, las hipótesis nula y alternativa son definidas como sigue: Hipótesis Nula: El conjunto de datos D es uniformemente distribuido (sin clusters significativos). Hipótesis Alternativa: El conjunto de datos D no es distribuido uniformemente (contiene clusters significativos). Cuando el estadístico de Hopkins tiene valores cercanos a cero, entonces puede rechazarse la hipótesis nula y concluir que el conjunto de datos D tiene datos conglomerables significativos. df_scaled = scaler.fit_transform(df) hopkins(df_scaled, df_scaled.shape[0]) ## 0.1824699352503313 Este valor sugiere rechazar la hipótesis nula en favor de la alternativa. Por último, se compararán otros 2 gráficos con la disimilitud de los dos conjuntos de datos. La metodología de este gráfico lleva por nombre “Visual Assessment of Cluster Tendency” (VAT). Este método consiste de 3 pasos: Calcula la matriz de disimilaridad (DM) entre objetos usando la distancia Euclidiana. Re-ordena la DM de forma que los elementos similares estén cercanos unos de otros. Este proceso crea una Matriz de Di-similaridad Ordenada (ODM). La ODM es mostrada como una imagen de disimilaridad ordenada, la cual es la salida visual de VAT. plt.figure(figsize=(4, 4)) ## &lt;Figure size 800x800 with 0 Axes&gt; # Original data plt.clf() ivat(pairwise_distances(df, metric=&#39;correlation&#39;), figure_size = (4, 4)) plt.title(&#39;Gráfico VAT - Datos reales&#39;) ## Text(0.5, 1.0, &#39;Gráfico VAT - Datos reales&#39;) plt.show() # Random data plt.clf() ivat(pairwise_distances(random_df, metric=&#39;correlation&#39;), figure_size = (4, 4)) plt.title(&#39;Gráfico VAT - Datos aleatorios&#39;) ## Text(0.5, 1.0, &#39;Gráfico VAT - Datos aleatorios&#39;) plt.show() Donde: Oscuro: Alta similaridad Claro: Baja similaridad La matriz de disimilaridad anterior confirma que existe una estructura de cluster en el conjunto de datos Iris, pero no en el aleatorio. La técnica VAT detecta la tendencia de clustering de forma visual al contar el número de bloques cuadradas sobre la diagonal en la imagen VAT. 9.3 K - means La agrupación en grupos con K-means es uno de los algoritmos de aprendizaje de máquina no supervisados más simples y populares. K-medias es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. Un cluster se refiere a una colección de puntos de datos agregados a a un grupo debido a ciertas similitudes. 9.3.1 Ajuste de modelo: ¿Cómo funciona el algortimo? Paso 1: Seleccionar el número de clusters K El primer paso en k-means es elegir el número de conglomerados, K. Como estamos en un problema de análisis no supervisado, no hay K correcto, existen métodos para seleccionar algún K pero no hay respuesta correcta. Paso 2: Seleccionar K puntos aleatorios de los datos como centroides. A continuación, seleccionamos aleatoriamente el centroide para cada grupo. Supongamos que queremos tener 2 grupos, por lo que K es igual a 2, seleccionamos aleatoriamente los centroides: Paso 3: Asignamos todos los puntos al centroide del cluster más cercano. Una vez que hemos inicializado los centroides, asignamos cada punto al centroide del cluster más cercano: Paso 4: Volvemos a calcular los centroides de los clusters recién formados. Ahora, una vez que hayamos asignado todos los puntos a cualquiera de los grupos, el siguiente paso es calcular los centroides de los grupos recién formados: Paso 5: Repetir los pasos 3 y 4. Criterios de paro: Existen tres criterios de paro para detener el algoritmo: Los centroides de los grupos recién formados no cambian: Podemos detener el algoritmo si los centroides no cambian. Incluso después de múltiples iteraciones, si obtenemos los mismos centroides para todos los clusters, podemos decir que el algoritmo no está aprendiendo ningún patrón nuevo y es una señal para detener el entrenamiento. Los puntos permanecen en el mismo grupo: Otra señal clara de que debemos detener el proceso de entrenamiento si los puntos permanecen en el mismo cluster incluso después de entrenar el algoritmo para múltiples iteraciones. Se alcanza el número máximo de iteraciones: Finalmente, podemos detener el entrenamiento si se alcanza el número máximo de iteraciones. Supongamos que hemos establecido el número de iteraciones en 100. El proceso se repetirá durante 100 iteraciones antes de detenerse. 9.3.2 Calidad de ajuste 9.3.2.1 Inercia La idea detrás de la agrupación de k-medias consiste en definir agrupaciones de modo que se minimice la variación total dentro de la agrupación (conocida como within cluster variation o inertia). Existen distintos algoritmos de k-medias, el algoritmo estándar es el algoritmo de Hartigan-Wong, que define within cluster variation como la suma de las distancias euclidianas entre los elementos y el centroide correspondiente al cuadrado: \\[W(C_k)=\\sum_{x_i \\in C_k}(x_i-\\mu_k)²\\] donde \\(x_i\\) es una observación que pertenece al cluster \\(C_k\\) y \\(\\mu_k\\) es la media del cluster \\(C_k\\) Cada observación \\(x_i\\) se asigna a un grupo de modo que la suma de cuadrados de la distancia de la observación a sus centroide del grupo asignado \\(\\mu_k\\) es mínima. Definimos la total within cluster variation total de la siguiente manera: \\[total \\quad within = \\sum_{k=1}^{K}W(C_k) = \\sum_{k=1}^{K}\\sum_{x_i \\in C_k}(x_i-\\mu_k)²\\] 9.3.3 ¿Cómo seleccionamos K? Una de las dudas más comunes que se tienen al trabajar con K-Means es seleccionar el número correcto de clusters. El número máximo posible de conglomerados será igual al número de observaciones en el conjunto de datos. Pero entonces, ¿cómo podemos decidir el número óptimo de agrupaciones? Una cosa que podemos hacer es trazar un gráfico, también conocido como gráfica de codo, donde el eje x representará el número de conglomerados y el eje y será una métrica de evaluación, en este caso usaremos inertia. Comenzaremos con un valor de K pequeño, digamos 2. Entrenaremos el modelo usando 2 grupos, calculamos la inercia para ese modelo y, finalmente, agregamos el punto en el gráfico mencionado. Digamos que tenemos un valor de inercia de alrededor de 1000: Ahora, aumentaremos el número de conglomerados, entrenaremos el modelo nuevamente y agregaremos el valor de inercia en la gráfica con distintos números de K: Cuando cambiamos el valor de K de 2 a 4, el valor de inercia se redujo de forma muy pronunciada. Esta disminución en el valor de inercia se reduce y eventualmente se vuelve constante a medida que aumentamos más el número de grupos. Entonces, el valor de K donde esta disminución en el valor de inercia se vuelve constante se puede elegir como el valor de grupo correcto para nuestros datos. Aquí, podemos elegir cualquier número de conglomerados entre 6 y 10. Podemos tener 7, 8 o incluso 9 conglomerados. También debe tener en cuenta el costo de cálculo al decidir la cantidad de clusters. Si aumentamos el número de clusters, el costo de cálculo también aumentará. Entonces, si no tiene recursos computacionales altos, deberíamos un número menor de clusters. 9.3.4 Implementación en Python Usaremos los datos USArrests, que contiene estadísticas, en arrestos por cada 100,000 residentes por asalto, asesinato y violación en cada uno de los 50 estados de EE. UU. En 1973. También se da el porcentaje de la población que vive en áreas urbanas. from sklearn.cluster import KMeans USArrests = pd.read_csv(&quot;data/USArrests.csv&quot;, index_col=0) USArrests.head() ## Murder Assault UrbanPop Rape ## Alabama 13.20 236 58 21.20 ## Alaska 10.00 263 48 44.50 ## Arizona 8.10 294 80 31.00 ## Arkansas 8.80 190 50 19.50 ## California 9.00 276 91 40.60 # Escalar los datos USArrests_scaled = pd.DataFrame(data = scale(USArrests), columns = USArrests.columns) USArrests_scaled.head() ## Murder Assault UrbanPop Rape ## 0 1.26 0.79 -0.53 -0.00 ## 1 0.51 1.12 -1.22 2.51 ## 2 0.07 1.49 1.01 1.05 ## 3 0.23 0.23 -1.08 -0.19 ## 4 0.28 1.28 1.78 2.09 Usaremos la función KMeans(), los siguientes parámetros son los más usados: n_clusters: Número de clústeres que se deben formar. Este es un parámetro crítico y debes ajustarlo según el conocimiento del dominio o mediante técnicas como el codo (elbow method) para encontrar el número óptimo de clústeres. init: Método de inicialización de centroides. ‘k-means++’ es una opción común y efectiva. n_init: Número de veces que se ejecuta el algoritmo con diferentes centroides iniciales. El resultado final será el mejor de estas ejecuciones. max_iter: Número máximo de iteraciones del algoritmo en una sola ejecución. Es importante establecer un límite para evitar bucles infinitos. tol: Tolerancia para declarar la convergencia. Cuando el cambio en los centroides es menor que tol, se considera que el algoritmo ha convergido. random_state: Semilla para la reproducibilidad de los resultados. fit: Método que ajusta el modelo a los datos. En el siguiente ejemplo se agruparán los datos en seis grupos (n_centers = 3). Como se había mencionado, la función KMeans también tiene una opción n_init que intenta múltiples configuraciones iniciales y regresa la mejor, agregar n_init = 25 generará 25 configuraciones iniciales. kmeans = KMeans( n_clusters=2, init=&#39;k-means++&#39;, n_init=25, max_iter=300, tol=1e-4, random_state=42 ) # Ajustar el modelo a los datos kmeans.fit(USArrests_scaled) ## KMeans(n_clusters=2, n_init=25, random_state=42) # Obtener las etiquetas de los clústeres asignadas a cada punto de datos labels = kmeans.labels_ labels ## array([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, ## 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, ## 0, 0, 0, 0, 0, 0], dtype=int32) # Obtener las coordenadas de los centroides finales centroids = kmeans.cluster_centers_ centroids ## array([[-0.67675778, -0.68274685, -0.13306084, -0.57037591], ## [ 1.01513667, 1.02412028, 0.19959126, 0.85556386]]) # Obtener la inercia (suma de las distancias cuadradas de cada punto al centroide de su clúster) inertia = kmeans.inertia_ inertia ## 104.96163315756871 La salida de kmeans contiene la siguiente información: labels_: Atributo que contiene las etiquetas de los clústeres asignadas a cada punto de datos. cluster_centers_: Atributo que contiene las coordenadas de los centroides finales de los clústeres. inertia_: Atributo que contiene la inercia, que es la suma de las distancias cuadradas de cada punto al centroide de su clúster. # Realiza el análisis de componentes principales (PCA) pca = PCA(n_components=2) principal_components = pca.fit_transform(USArrests_scaled) df_pca = pd.DataFrame(data=principal_components, columns=[&#39;PC1&#39;, &#39;PC2&#39;]) df_pca[&quot;cluster2&quot;] = kmeans.labels_+1 plt.clf() sns.scatterplot(x=&quot;PC1&quot;, y=&quot;PC2&quot;, hue=&quot;cluster2&quot;, data=df_pca, palette=&quot;rainbow&quot;) ## &lt;Axes: xlabel=&#39;PC1&#39;, ylabel=&#39;PC2&#39;&gt; plt.title(&quot;Visualización de los resultados de k-means&quot;) ## Text(0.5, 1.0, &#39;Visualización de los resultados de k-means&#39;) plt.show() Debido a que el número de conglomerados (K) debe establecerse antes de iniciar el algoritmo, a menudo es recomendado utilizar varios valores diferentes de K y examinar las diferencias en los resultados. Podemos ejecutar el mismo proceso para 3, 4 y 5 clusters, y los resultados se muestran en la siguiente figura: k3 = KMeans(n_clusters=3, n_init = 25, random_state=333) k3.fit(USArrests_scaled) ## KMeans(n_clusters=3, n_init=25, random_state=333) df_pca[&quot;cluster3&quot;] = k3.labels_ + 1 k4 = KMeans(n_clusters=4, n_init = 25, random_state=444) k4.fit(USArrests_scaled) ## KMeans(n_clusters=4, n_init=25, random_state=444) df_pca[&quot;cluster4&quot;] = k4.labels_ + 1 k5 = KMeans(n_clusters=5, n_init = 25, random_state=555) k5.fit(USArrests_scaled) ## KMeans(n_clusters=5, n_init=25, random_state=555) df_pca[&quot;cluster5&quot;] = k5.labels_ + 1 df_pca.head() ## PC1 PC2 cluster2 cluster3 cluster4 cluster5 ## 0 0.99 1.13 2 3 3 3 ## 1 1.95 1.07 2 3 4 4 ## 2 1.76 -0.75 2 3 4 4 ## 3 -0.14 1.12 1 1 3 5 ## 4 2.52 -1.54 2 3 4 4 from plydata.tidy import pivot_longer from plydata.one_table_verbs import select from plotnine import * colores_por_valor = {1: &#39;red&#39;, 2: &#39;green&#39;, 3: &#39;blue&#39;, 4: &#39;purple&#39;, 5: &#39;yellow&#39;} ( df_pca &gt;&gt; pivot_longer( cols=select(startswith=&#39;cluster&#39;), names_to = &quot;n_cluster&quot;, values_to=&#39;value&#39; ) &gt;&gt; ggplot(aes(x = &quot;PC1&quot;, y = &quot;PC2&quot;, fill = &quot;factor(value)&quot;, color = &quot;factor(value)&quot;)) + geom_point() + scale_color_manual(values=colores_por_valor) + facet_wrap(&quot;~n_cluster&quot;, ncol=2) ) ## &lt;Figure Size: (1280 x 960)&gt; Recordemos que podemos usar la gráfica de codo para obtener el número óptimo de K, usaremos la función fviz_nbclust() para esto. from yellowbrick.cluster import KElbowVisualizer # Configurar el modelo KMeans model = KMeans(n_init = 25) # Crear visualizador de codo para determinar el número óptimo de clústeres (K) plt.clf() visualizer = KElbowVisualizer(model, k=(1, 10), metric=&#39;distortion&#39;) # Ajustar el modelo y visualizar el codo visualizer.fit(USArrests_scaled) ## KElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=9, n_init=25), ## k=(1, 10)) visualizer.show() ## &lt;Axes: title={&#39;center&#39;: &#39;Distortion Score Elbow for KMeans Clustering&#39;}, xlabel=&#39;k&#39;, ylabel=&#39;distortion score&#39;&gt; Ejercicio Comparar el grado de marginación original de CONAPO con el método de kmeans y comentar las diferencias. "],["despedida.html", "Capítulo 10 DESPEDIDA", " Capítulo 10 DESPEDIDA "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
